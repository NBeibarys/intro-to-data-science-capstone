{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049da969",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "**Beibarys Nyussupov, Joseph Tadros, Luke Ducker**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3baa7",
   "metadata": {},
   "source": [
    "This project assesses university professors using a large, publicly available dataset scraped from RateMyProfessor.com. The objective is to integrate core concepts from the course into a single applied analysis and to extract actionable insights about teaching quality and student perceptions. The dataset contains aggregated student ratings and related attributes for a broad sample of professors, with low individual response rates but substantial overall scale. Prior research reports a correlation of approximately 0.7 between RateMyProfessor ratings and official end-of-course teaching evaluations, which supports the analytical value of this source despite known response bias. All data collection and basic structuring steps were completed in advance, while data science-relevant preprocessing, including the identification and handling of missing data, was performed in this project. To address the research questions, appropriate statistical methods were selected based on underlying assumptions, with explicit justification provided throughout the report. Visualizations support interpretation and highlight key patterns in the data. All hypothesis testing uses α = 0.005 to reduce false positive findings (Habibzadeh, 2025). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e37acb",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209c0af",
   "metadata": {},
   "source": [
    "### First dataset: `rmpCapstoneNum.csv`\n",
    "Dataset contains 89893 records. Each of these\n",
    "records (rows) corresponds to information about one professor. \n",
    "\n",
    "Columns:\n",
    "\n",
    "`1: Average Rating (the arithmetic mean of all individual quality ratings of this professor)`\n",
    "\n",
    "`2: Average Difficulty (the arithmetic mean of all individual difficulty ratings of this professor)`\n",
    "\n",
    "`3: Number of ratings (simply the total number of ratings these averages are based on)`\n",
    "\n",
    "`4: Received a “pepper”? (Boolean - was this professor judged as “hot” by the students?)`\n",
    "\n",
    "`5: The proportion of students that said they would take the class again`\n",
    "\n",
    "`6: The number of ratings coming from online classes`\n",
    "\n",
    "`7: Male gender (Boolean – 1: determined with high confidence that professor is male)`\n",
    "\n",
    "`8: Female (Boolean – 1: determined with high confidence that professor is female`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b444e87",
   "metadata": {},
   "source": [
    "### Second dataset: ` rmpCapstoneQual.csv`\n",
    "Dataset contains same 89893 records. \n",
    "\n",
    "Columns: \n",
    "\n",
    "`1: Major/Field`\n",
    "\n",
    "`2: University`\n",
    "\n",
    "`3: US State (2 letter abbreviation)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f4217",
   "metadata": {},
   "source": [
    "### Third dataset: `rmpCapstoneTags.csv`\n",
    "Dataset contains same 89893 records. \n",
    "\n",
    "The numbers in these columns correspond to the raw number of “tags” a professor has received. A student can award up to 3 such tags, but doesn’t have to award any. These tags are supposed to characterize the teaching style of the professor qualitatively, beyond ratings. \n",
    "\n",
    "Columns: \n",
    "\n",
    "`1: “Tough grader”`\n",
    "\n",
    "`2: “Good feedback”`\n",
    "\n",
    "`3: “Respected”`\n",
    "\n",
    "`4: “Lots to read”`\n",
    "\n",
    "`5: “Participation matters”`\n",
    "\n",
    "`6: “Don’t skip class or you will not pass”`\n",
    "\n",
    "`7: “Lots of homework”`\n",
    "\n",
    "`8: “Inspirational”`\n",
    "\n",
    "`9: “Pop quizzes!”`\n",
    "\n",
    "`10: “Accessible”`\n",
    "\n",
    "`11: “So many papers”`\n",
    "\n",
    "`12: “Clear grading”`\n",
    "\n",
    "`13: “Hilarious”`\n",
    "\n",
    "`14: “Test heavy”`\n",
    "\n",
    "`15: “Graded by few things”`\n",
    "\n",
    "`16: “Amazing lectures”`\n",
    "\n",
    "`17: “Caring”`\n",
    "\n",
    "`18: “Extra credit”`\n",
    "\n",
    "`19: “Group projects”`\n",
    "\n",
    "`20: “Lecture heavy\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff85d92",
   "metadata": {},
   "source": [
    "**How to Run This Notebook:**\n",
    "\n",
    "**1. Run all cells before Q1.**\n",
    "- This initializes imports, helper functions, data loading, preprocessing, and global configuration.\n",
    "\n",
    "**2. After that, each question section is independent.**\n",
    "\n",
    "**3. For any question QX:**\n",
    "- First run the cell that defines qX_df.\n",
    "- Then run the remaining cells in that section in order.\n",
    "- Skipping the qX_df cell will cause errors in downstream cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f26d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from cliffs_delta import cliffs_delta as cd\n",
    "\n",
    "# machine learning \n",
    "# train test split\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import (KFold, StratifiedKFold, \n",
    "                                     train_test_split,\n",
    "                                     cross_val_score, cross_validate, \n",
    "                                     GridSearchCV, RandomizedSearchCV)\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import (plot_optimization_history, \n",
    "                                  plot_param_importances, \n",
    "                                  plot_slice, \n",
    "                                  plot_parallel_coordinate)\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import (r2_score, root_mean_squared_error, \n",
    "                             mean_squared_error, recall_score, precision_score, \n",
    "                             f1_score, accuracy_score, roc_auc_score, confusion_matrix, \n",
    "                             ConfusionMatrixDisplay, classification_report, roc_curve, auc)\n",
    "import shap\n",
    "\n",
    "# models \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn import svm\n",
    "\n",
    "# cloning model \n",
    "from sklearn.base import clone\n",
    "\n",
    "# reproducibility \n",
    "import random\n",
    "\n",
    "# specify seed \n",
    "n_number = 12250697\n",
    "random.seed(n_number)\n",
    "np.random.seed(n_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b1998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "# simple significance test interpretation\n",
    "def significance(alpha, p_value):\n",
    "    \"\"\"\n",
    "    Decide based on alpha. \n",
    "    Note: p is the probability, under H0, of observing a statistic at least this extreme.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"p-value: {p_value:.6g}\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"p = {p_value:.6g} < α = {alpha} = Reject H0 (statistically significant).\")\n",
    "    else:\n",
    "        print(f\"p = {p_value:.6g} ≥ α = {alpha} = Fail to reject H0 (not statistically significant).\")\n",
    "\n",
    "# short-cut for assessing regression model\n",
    "def score_model(model, x_train, x_test, y_train, y_test):\n",
    "\n",
    "    # fit the model \n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_train_hat = model.predict(x_train)\n",
    "    y_test_hat = model.predict(x_test)\n",
    "\n",
    "    #r2_score\n",
    "    train_r2 = r2_score(y_train, y_train_hat)\n",
    "    test_r2 = r2_score(y_test, y_test_hat)\n",
    "\n",
    "    #mse\n",
    "    train_rmse = root_mean_squared_error(y_train, y_train_hat)\n",
    "    test_rmse = root_mean_squared_error(y_test, y_test_hat)\n",
    "\n",
    "    out = {\n",
    "        \"Training RMSE\": train_rmse,\n",
    "        \"Training R2\": train_r2,\n",
    "        \"Testing RMSE\": test_rmse,\n",
    "        \"Testing R2\": test_r2\n",
    "    }\n",
    "\n",
    "    return out\n",
    "\n",
    "# fine-tuning function for models (optuna)\n",
    "def start_study_optuna(objective, n_trials, sampler_seed, direction=\"minimize\"):\n",
    "    # sampler for reproducibility\n",
    "    sampler = TPESampler(seed=sampler_seed)\n",
    "\n",
    "    # start the study for fine-tuning\n",
    "    study = optuna.create_study(direction=direction, sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # print best parameters \n",
    "    print(f\"Best Params: {study.best_params}\")\n",
    "    return study\n",
    "\n",
    "# short-cut for assessing classification model\n",
    "def score_classifier(model, x_train, x_test, y_train, y_test, threshold=0.5):\n",
    "\n",
    "    # fit model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # predicted probabilities\n",
    "    y_train_proba = model.predict_proba(x_train)[:, 1]\n",
    "    y_test_proba = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # predicted classes\n",
    "    y_train_hat = (y_train_proba >= threshold).astype(int)\n",
    "    y_test_hat = (y_test_proba >= threshold).astype(int)\n",
    "\n",
    "    # confusion matrix on test\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_hat).ravel()\n",
    "\n",
    "    # specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    out = {\n",
    "        # probability-based metrics\n",
    "        \"Training ROC-AUC\": roc_auc_score(y_train, y_train_proba),\n",
    "        \"Testing ROC-AUC\": roc_auc_score(y_test, y_test_proba),\n",
    "\n",
    "        # threshold-based metrics\n",
    "        \"Training Accuracy\": accuracy_score(y_train, y_train_hat),\n",
    "        \"Testing Accuracy\": accuracy_score(y_test, y_test_hat),\n",
    "\n",
    "        \"Training Precision\": precision_score(y_train, y_train_hat),\n",
    "        \"Testing Precision\": precision_score(y_test, y_test_hat),\n",
    "\n",
    "        \"Training Recall\": recall_score(y_train, y_train_hat),  # sensitivity\n",
    "        \"Testing Recall\": recall_score(y_test, y_test_hat),\n",
    "\n",
    "        \"Testing Specificity\": specificity,\n",
    "\n",
    "        \"Training F1\": f1_score(y_train, y_train_hat),\n",
    "        \"Testing F1\": f1_score(y_test, y_test_hat),\n",
    "\n",
    "        # diagnostics\n",
    "        \"Confusion Matrix (test)\": [[tn, fp], [fn, tp]]\n",
    "    }\n",
    "\n",
    "    return out\n",
    "\n",
    "# function for ROC-AUC plot \n",
    "def add_roc(ax, y_true, y_prob, label, lw=2):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, linewidth=lw, label=f\"{label} (AUC={roc_auc:.4f})\")\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369ecf9",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdc87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and write locations\n",
    "data_folder = os.path.join(\"..\", \"data\")\n",
    "raw_folder = os.path.join(data_folder, \"raw\")\n",
    "\n",
    "# file names\n",
    "rmp_num_filename = \"rmpCapstoneNum.csv\"\n",
    "rmp_qual_filename = \"rmpCapstoneQual.csv\"\n",
    "rmp_tags_filename = \"rmpCapstoneTags.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8db064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data \n",
    "df_num = pd.read_csv(os.path.join(raw_folder, rmp_num_filename), header=None)\n",
    "df_qual = pd.read_csv(os.path.join(raw_folder, rmp_qual_filename), header=None)\n",
    "df_tags = pd.read_csv(os.path.join(raw_folder, rmp_tags_filename), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32923425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers for first dataset \n",
    "df_num_column_names = [\n",
    "     \"average_rating\",\n",
    "     \"average_difficulty\",\n",
    "     \"number_of_ratings\",\n",
    "     \"received_a_pepper\",\n",
    "     \"would_take_again\",\n",
    "     \"number_of_ratings_online\",\n",
    "     \"male_gender\",\n",
    "     \"female_gender\",\n",
    "]\n",
    "\n",
    "# headers for second dataset\n",
    "df_qual_column_names = [\n",
    "     \"major\",\n",
    "     \"university\",\n",
    "     \"state\",\n",
    "]\n",
    "\n",
    "# headers for third dataset \n",
    "df_tags_column_names = [\n",
    "    \"tough_grader\",\n",
    "    \"good_feedback\",\n",
    "    \"respected\",\n",
    "    \"lots_to_read\",\n",
    "    \"participation_matters\",\n",
    "    \"dont_skip_class_or_you_will_not_pass\",\n",
    "    \"lots_of_homework\",\n",
    "    \"inspirational\",\n",
    "    \"pop_quizzes\",\n",
    "    \"accessible\",\n",
    "    \"so_many_papers\",\n",
    "    \"clear_grading\",\n",
    "    \"hilarious\",\n",
    "    \"test_heavy\",\n",
    "    \"graded_by_few_things\",\n",
    "    \"amazing_lectures\",\n",
    "    \"caring\",\n",
    "    \"extra_credit\",\n",
    "    \"group_projects\",\n",
    "    \"lecture_heavy\"\n",
    "]\n",
    "\n",
    "# assign headers to dataframes using .columns attribute \n",
    "df_num.columns = df_num_column_names\n",
    "df_qual.columns = df_qual_column_names\n",
    "df_tags.columns = df_tags_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the data for the easier manipulation \n",
    "df = df_num.join(df_qual).join(df_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data set (first five rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51072835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the dataset \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be0133",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11921b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values of the dataset by each column. \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128d0d3",
   "metadata": {},
   "source": [
    "A large block of missing data appears in the dataset. Exactly 19,889 records share missing values across all core numerical, categorical, and qualitative variables, with the exception of the “would take again” column. These missing values occur in the same row positions, which indicates a systematic issue rather than random missingness. In addition, all tag variables equal zero for these same records, which implies an absence of meaningful rating information rather than true zero-valued responses. Together, these patterns suggest these rows do not represent valid professor evaluations. Removing all 19,889 records is therefore justified, as retaining them would introduce noise and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d16580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values from the dataset \n",
    "df = df.dropna(subset=\"average_rating\")\n",
    "# check the results\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check information about the dataset after removal \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66cbbd",
   "metadata": {},
   "source": [
    "After removing the invalid records, the dataset contains 70,004 observations. The only remaining missing data is in the would_take_again variable, where a large proportion of values is missing. This variable is a bounded proportion, and the extent of missingness is too high for null values removal or imputation to be appropriate. Imputing values would impose artificial structure on a substantial share of the data and bias both estimates and inference. For this reason, we do not modify this column unless explicitly required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c9e53",
   "metadata": {},
   "source": [
    "### Number of ratings threshold \n",
    "This section addresses the reliability of average professor ratings. An average rating based on very few student evaluations is unstable and can produce extreme values that do not reflect typical teaching quality. Professors with only one or two ratings are therefore not comparable to those evaluated by many students. To reduce noise and improve interpretability, an optimal number of ratings threshold is chosen and applied so that average ratings are computed and analyzed only when they are supported by a sufficient volume of observations. This choice reduces the influence of outliers driven by sparse data and ensures that comparisons across professors rely on more reliable estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of number of ratings \n",
    "df[\"number_of_ratings\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for distribution of number of ratings \n",
    "plt.figure(figsize = (12, 10))\n",
    "\n",
    "# histogram \n",
    "plt.hist(df[\"number_of_ratings\"], \n",
    "         bins = 100, \n",
    "         alpha = 0.7)\n",
    "\n",
    "# show median and mean value on the plot \n",
    "# median\n",
    "plt.axvline(df[\"number_of_ratings\"].median(), \n",
    "           color = \"green\", \n",
    "           linestyle = \"dashed\", \n",
    "           linewidth = 2, \n",
    "           label = f\"median: {df['number_of_ratings'].median():.1f}\")\n",
    "\n",
    "# mean\n",
    "plt.axvline(df[\"number_of_ratings\"].mean(), \n",
    "           color = \"red\", \n",
    "           linestyle = \"dashed\", \n",
    "           linewidth = 2, \n",
    "           label= f\"mean: {df['number_of_ratings'].mean():.1f}\")\n",
    "\n",
    "# aesthetics \n",
    "# title\n",
    "plt.title(\"Distribution of number of ratings\", \n",
    "          fontweight = \"bold\", \n",
    "          fontsize = 18)\n",
    "\n",
    "# x axis \n",
    "plt.xlabel(\"Number of ratings\", \n",
    "           fontsize = 16)\n",
    "# limit\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "# y axis\n",
    "plt.ylabel(\"Frequency\", \n",
    "           fontsize = 16)\n",
    "# size for both x and y ticks \n",
    "plt.tick_params(axis = \"both\", \n",
    "                labelsize = 14)\n",
    "\n",
    "# legend \n",
    "plt.legend(fontsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c43706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the professors with less than 5 ratings (mean)\n",
    "df_filtered = df[df[\"number_of_ratings\"] >= 5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12726add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution and information about the data after filter \n",
    "df_filtered[\"number_of_ratings\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f2be3",
   "metadata": {},
   "source": [
    "The distribution of the number of ratings is highly right skewed, with a median of 3 and a mean of 5.4. Most professors receive only a small number of ratings, which leads to unstable and potentially extreme average values. To address this, we set a minimum threshold of five ratings and excluded professors with fewer than five evaluations. This cutoff is close to the mean and removes cases where average ratings are driven by very sparse data, while preserving the majority of the sample and improving the reliability of comparisons across professors. After applying this filter, 25,368 professors remain in the analysis dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about the dataset \n",
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f4c60",
   "metadata": {},
   "source": [
    "### Categorical variables \n",
    "In this section, we address the handling of categorical variables. Teaching style tags were normalized by dividing each tag count by the total number of tags received by a professor, converting raw counts into proportions. This normalization controls for differences in the total number of ratings and ensures that tag values are comparable across professors. Cases with no assigned tags resulted in undefined values after normalization and were set to zero, reflecting the absence of tag information rather than meaningful signal.\n",
    "\n",
    "In addition, we examine the gender indicators provided in the dataset. The male and female variables are treated as categorical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeaebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of all tags before normalization \n",
    "# starting from 11th variable \n",
    "# .describe \n",
    "df_filtered.iloc[:, 11:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total # of tags for each professor\n",
    "total_tag_counts = df_filtered[df_tags_column_names].sum(axis = 1)\n",
    "\n",
    "# normalize tag columns to be between 0 and 1 by dividing tag counts by total tag counts\n",
    "for col in df_tags_column_names:\n",
    "    df_filtered[col] = df_filtered[col] / total_tag_counts\n",
    "\n",
    "# fill na values with 0 that resulted from divide by zero issues\n",
    "df_filtered[df_tags_column_names] = df_filtered[df_tags_column_names].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution of tags after the normalization \n",
    "df_filtered.iloc[:, 11:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd6714",
   "metadata": {},
   "source": [
    "Now, we are looking at gender variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42dbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check male and female gender error, where both are 1, or both 0\n",
    "both_11 = df_filtered[(df_filtered[\"male_gender\"] == 1) & (df_filtered[\"female_gender\"] == 1)].shape[0]\n",
    "both_00 = df_filtered[(df_filtered[\"male_gender\"] == 0) & (df_filtered[\"female_gender\"] == 0)].shape[0]\n",
    "\n",
    "# print the results \n",
    "print(f\"Number of professors with both male and female gender as 1: {both_11}\")\n",
    "print(f\"Number of professors with both male and female gender as 0: {both_00}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81204590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25a1ca",
   "metadata": {},
   "source": [
    "This is a clear data error and should be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df570806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where both male and female gender are 1 or 0\n",
    "df_filtered_final = df_filtered[~((df_filtered[\"male_gender\"] == 1) & (df_filtered[\"female_gender\"] == 1)) & \n",
    "                          (~((df_filtered[\"male_gender\"] == 0) & (df_filtered[\"female_gender\"] == 0)))].copy()\n",
    "\n",
    "# check the results\n",
    "df_filtered_final.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb8714",
   "metadata": {},
   "source": [
    "After data pre-processing, we are left with ~18k rows. Now we will proceed answering research questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb448987",
   "metadata": {},
   "source": [
    "## Q1. Activists have asserted that there is a strong gender bias in student evaluations of professors, with male professors enjoying a boost in rating from this bias...We would like you to answer the question whether there is evidence of a pro-male gender bias in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ece44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the average ratings into two groups based on gender\n",
    "# Male = 1, Male = 0\n",
    "df1_male = df_filtered_final[df_filtered_final[\"male_gender\"] == 1].copy()\n",
    "df1_female = df_filtered_final[df_filtered_final[\"male_gender\"] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of professors in each group\n",
    "print(f\"Number of male professors: {df1_male.shape[0]}\")\n",
    "print(f\"Number of female professors: {df1_female.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ce627",
   "metadata": {},
   "source": [
    "A difference is noticeable, but not very large. We can proceed with the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ee5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare distributions of average rating between male and female professors\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# box plot for male and female professors \n",
    "sns.boxplot(data=df_filtered_final, \n",
    "            x='average_rating', \n",
    "            y='male_gender', \n",
    "            palette = \"Set2\", \n",
    "            showfliers = False, \n",
    "            medianprops = {\"color\": \"red\", \"linewidth\": 2}, \n",
    "            orient= \"horizontal\")\n",
    "\n",
    "# aesthetics \n",
    "plt.title('Average Rating Distribution by Gender', \n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "\n",
    "# xlabel\n",
    "plt.xlabel('Average Rating', \n",
    "           fontsize = 16)\n",
    "# ylabel\n",
    "plt.ylabel('Gender (1 = Male, 0 = Female)', \n",
    "           fontsize = 16)\n",
    "\n",
    "# legend\n",
    "plt.legend(['0 - Female', \n",
    "            '1 - Male'], \n",
    "            fontsize = 16,\n",
    "           loc = \"upper left\")\n",
    "# ticks \n",
    "plt.tick_params(axis = \"both\", \n",
    "                labelsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e89b80",
   "metadata": {},
   "source": [
    "The boxplot suggests a small difference in average ratings by gender. The distributions for male and female professors overlap substantially, but the male distribution appears slightly shifted to the right, with a higher median average rating. The spread of ratings is similar across genders, indicating comparable variability. To further test the difference, we have to apply a significance test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13faee2f",
   "metadata": {},
   "source": [
    "### Mann-Whitney U Test (one-tailed) \n",
    "\n",
    "`Rationale for the test:` For this question, we test whether male professors receive higher average ratings than female professors. Average ratings are bounded, derived from ordinal Likert-scale responses, and do not reliably satisfy normality assumptions at the individual professor level. Population parameters such as the true mean and variance are unknown.\n",
    "\n",
    "We therefore use a one-sided Mann-Whitney U test, which is appropriate for comparing central tendency between two independent groups without assuming normality. This test evaluates whether ratings for one group tend to be systematically higher than those for the other, consistent with the directional hypothesis of pro-male bias.\n",
    "\n",
    "`Null hypothesis (H0):` The median average rating for male professors is equal to the median average rating for female professors.\n",
    "\n",
    "`Alternative hypothesis (Ha):` The median average rating for male professors is higher than the median average rating for female professors.\n",
    "\n",
    "`Significance level:` 0.005\n",
    "\n",
    "`Interpretation:` The p-value represents the probability of observing a difference in average ratings at least as extreme as the one observed, assuming the null hypothesis is true.\n",
    "- If the p-value is smaller than 0.005, we reject H0 and conclude that there is statistically significant evidence of a pro-male bias in average ratings.\n",
    "- If the p-value is larger than 0.005, we fail to reject H0 and conclude that the observed difference is consistent with random variation and does not provide sufficient evidence of gender bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct mann-whitney u test for average rating using our function\n",
    "u_stat, p_value = stats.mannwhitneyu(df1_male['average_rating'], \n",
    "                                     df1_female['average_rating'], \n",
    "                                     alternative='greater')\n",
    "# interpretation \n",
    "significance(0.005, p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326abdf",
   "metadata": {},
   "source": [
    "Our p-value (0.00024523) is smaller than the significance threshold of 0.005, so we reject the null hypothesis and conclude that the median average rating for male professors is higher than that for female professors.\n",
    "\n",
    "This result does not establish causal evidence of pro-male rating bias, as the data are observational, not randomized, and we do not control for all potential confounders such as course type, institution, teaching experience, or selection effects in who chooses to rate professors.\n",
    "\n",
    "However, we do find statistical evidence of a systematic difference in ratings by gender, with male professors receiving higher typical ratings in this dataset. This pattern is consistent with, but does not by itself prove, the presence of pro-male bias in average professor ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3656eb",
   "metadata": {},
   "source": [
    "## Q2. Is there a gender difference in the spread (variance/dispersion) of the ratings distribution? Again, it is advisable to consider the statistical significance of any observed gender differences in this spread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88abf4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again_separate the average ratings into two groups based on gender\n",
    "# Male = 1, Male = 0\n",
    "df2_male = df_filtered_final[df_filtered_final[\"male_gender\"] == 1].copy()\n",
    "df2_female = df_filtered_final[df_filtered_final[\"male_gender\"] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3357c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of professors in each group\n",
    "print(f\"Number of male professors: {df2_male.shape[0]}\")\n",
    "print(f\"Number of female professors: {df2_female.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the variance of average rating dipersion between male and female professors\n",
    "# kernel density plot \n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# plot\n",
    "sns.kdeplot(df2_male['average_rating'], \n",
    "           label='Male', \n",
    "           color='blue', \n",
    "           fill=True, \n",
    "           legend= True, \n",
    "           alpha=0.15\n",
    "           )\n",
    "sns.kdeplot(df2_female['average_rating'],\n",
    "              label='Female', \n",
    "              color='green', \n",
    "              fill=False, \n",
    "              legend= True)\n",
    "\n",
    "# aesthetics\n",
    "plt.title('Average Rating Dispersion by Gender', \n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "# xlabel\n",
    "plt.xlabel('Average Rating',\n",
    "              fontsize = 16)\n",
    "# ylabel\n",
    "plt.ylabel('Density',\n",
    "                fontsize = 16)\n",
    "\n",
    "# axis \n",
    "plt.tick_params(axis = \"both\", \n",
    "                labelsize = 14)\n",
    "# legend \n",
    "plt.legend()\n",
    "# ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bf305",
   "metadata": {},
   "source": [
    "The density curves for average ratings by gender are highly similar, indicating comparable dispersion for male and female professors. Both distributions cover a similar range of ratings and exhibit similar tail behavior, suggesting no substantial difference in variability. While the male distribution appears slightly shifted to the right, consistent with higher typical ratings observed in Q1, the overall spread and shape of the distributions closely align.\n",
    "\n",
    "From a visual inspection, we do not observe meaningful differences in variance or dispersion between the two groups. Any differences in spread appear minor relative to the overall similarity of the distributions and are unlikely to reflect a practically important gender difference in rating variability.\n",
    "\n",
    "This visual evidence suggests that gender differences in ratings are primarily related to central tendency rather than dispersion, motivating a formal statistical test of variance in the subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99374c",
   "metadata": {},
   "source": [
    "### Levene's test \n",
    "\n",
    "`Rationale for the test:` This question asks whether male and female professors differ in the spread of their average rating distributions. We need a test of dispersion, not a test of location. Average ratings are bounded (1 to 5) and do not satisfy normality assumptions, so classical variance tests that rely on normality, such as an F-test, are not appropriate. We therefore use Levene's test because it tests equality of variance while remaining robust under non-normal data. We use the median-centered version to further reduce sensitivity to skewness and outliers. The test is two-tailed because the question asks whether dispersion differs in either direction, not whether one gender has higher variance.\n",
    "\n",
    "`Null hypothesis (H0):` The dispersion (variance) of average ratings for male professors is equal to the dispersion of average ratings for female professors.\n",
    "\n",
    "`Alternative hypothesis (Ha):` The dispersion of average ratings significantly differs between male and female professors.\n",
    "\n",
    "`Significance level:` 0.005\n",
    "\n",
    "If the p-value is smaller than 0.005, we reject H0 and conclude that the dispersion of average ratings differs significantly by gender.\n",
    "\n",
    "If the p-value is larger than 0.005, we fail to reject H0, indicating that the observed difference in dispersion is consistent with random variation and that the data do not provide sufficient statistical evidence of a gender difference in rating variability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ad7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispersion of average ratings \n",
    "# levene's test for equal variances\n",
    "levene_stat, levene_p_value = stats.levene(df2_male['average_rating'], \n",
    "                                      df2_female['average_rating'], \n",
    "                                      center='median')\n",
    "\n",
    "# check the result \n",
    "significance(0.005, levene_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdecc79",
   "metadata": {},
   "source": [
    "Since the p-value (0.00000598) is smaller than the significance level α = 0.005, we reject the null hypothesis. This provides statistically significant evidence that the dispersion of average ratings differs between male and female professors. The observed difference in spread is unlikely to be due to random variation alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5de36",
   "metadata": {},
   "source": [
    "## Q3. What is the likely size of both of these effects (gender bias in average rating, gender bias in spread of average rating), as estimated from this dataset? Please use 95% confidence and make sure to report each/both. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9450bd",
   "metadata": {},
   "source": [
    "### Effect size of gender bias in average rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe35a0d",
   "metadata": {},
   "source": [
    "Because the ratings data are derived from ordinal Likert-scale responses and violate normality assumptions, we do not use parametric effect size measures such as Cohen's d or Hedges' g, which rely on normality and equal variances. Instead, we require a non-parametric effect size that is interpretable and appropriate for rank-based comparisons.\n",
    "\n",
    "We therefore use Cliff's Delta (δ), a non-parametric measure of effect size suitable for ordinal and non-normal data. Cliff's delta quantifies the degree of distributional non-overlap between two groups by comparing all pairwise observations. It ranges from −1 to +1, where 0 indicates complete overlap and larger absolute values indicate stronger separation between groups.\n",
    "\n",
    "Following the recommendations in [Meissel and Yao (2024)](https://github.com/NBeibarys/Project-Hypothesis-Testing-of-MovieRatings-Using-Python/blob/main/project_guidelines/Using%20Cliff%E2%80%99s%20Delta%20as%20a%20Non-Parametric%20Effect%20Size%20Measure.pdf), effect sizes are interpreted as:\n",
    "\n",
    "|δ| < 0.15: negligible effect\n",
    "\n",
    "0.15 ≤ |δ| < 0.33: small effect\n",
    "\n",
    "0.33 ≤ |δ| < 0.47: medium effect\n",
    "\n",
    "|δ| ≥ 0.47: large effect\n",
    "\n",
    "The sign of δ indicates the direction of the effect relative to the reference group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef9fcc",
   "metadata": {},
   "source": [
    "To estimate the uncertainty around the effect sizes, we compute 95% confidence intervals using bootstrapping. Bootstrapping is appropriate in this setting because it does not assume normality of the underlying data and is well suited for ordinal outcomes and non-parametric statistics.\n",
    "\n",
    "For each effect, we repeatedly resample the observed data with replacement and recompute the corresponding effect size on each bootstrap sample. The empirical distribution of these bootstrap estimates is then used to construct percentile-based 95% confidence intervals.\n",
    "\n",
    "This approach allows us to quantify the likely range of both:\n",
    "- the location effect (gender difference in average ratings, measured via Cliff's delta), and\n",
    "- the dispersion effect (gender difference in rating spread, measured via robust dispersion statistics), without relying on parametric assumptions. As a result, the reported confidence intervals reflect the variability inherent in the observed data rather than model-based assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # divide average ratings by groups of gender and convert to arrays (bootstrap expects that)\n",
    "df3_male = df_filtered_final[df_filtered_final[\"male_gender\"] == 1].copy()\n",
    "df3_female = df_filtered_final[df_filtered_final[\"male_gender\"] == 0].copy()\n",
    "\n",
    "# define function for effect size \n",
    "def cliffs_delta_trap(sample1, sample2, axis = 0):\n",
    "    return cd(sample1, sample2)[0]\n",
    "\n",
    "# Perform the bootstrap\n",
    "result = stats.bootstrap(\n",
    "    (df3_male['average_rating'].to_numpy(), \n",
    "    df3_female['average_rating'].to_numpy()), \n",
    "    statistic=cliffs_delta_trap,\n",
    "    confidence_level=0.95, \n",
    "    random_state= n_number, \n",
    "    vectorized= False,\n",
    "    method='BCa', \n",
    "    n_resamples=10000)\n",
    "\n",
    "# compute the confidence interval\n",
    "ci_lower, ci_upper = result.confidence_interval.low, result.confidence_interval.high\n",
    "\n",
    "# print the results \n",
    "# point estimate\n",
    "print(f\"Effect size (Cliffs delta): {cd(df3_male['average_rating'], \n",
    "                                        df3_female['average_rating'])[0]:.4f}\")\n",
    "\n",
    "# confidence interval\n",
    "print(f\"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boostrap distribution of cliffs delta\n",
    "# figure size \n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# plot\n",
    "plt.hist(result.bootstrap_distribution, \n",
    "         bins=40, \n",
    "         density=True, \n",
    "         color = \"skyblue\",\n",
    "         edgecolor = \"black\",\n",
    "         fill = True)\n",
    "\n",
    "# point estimate line\n",
    "plt.axvline(cd(df3_male['average_rating'].to_numpy(), \n",
    "                df3_female['average_rating'].to_numpy())[0], \n",
    "            color = \"orange\", \n",
    "            linestyle = \"dashed\", \n",
    "            label = f\"Effect size: {cd(df3_male['average_rating'].to_numpy(), \n",
    "                                            df3_female['average_rating'].to_numpy())[0]:.4f}\"\n",
    "            )\n",
    "\n",
    "# confidence interval lines\n",
    "# lower\n",
    "plt.axvline(ci_lower, \n",
    "            color = \"red\", \n",
    "            linestyle = \"dashed\", \n",
    "            label = f\"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\"\n",
    "            )\n",
    "# upper\n",
    "plt.axvline(ci_upper, \n",
    "            color = \"red\", \n",
    "            linestyle = \"dashed\")\n",
    "\n",
    "# aesthetics\n",
    "# title \n",
    "plt.title(\"Bootstrap Distribution of Cliff's Delta for Average Rating by Gender\", \n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "\n",
    "# x and y axis labels\n",
    "plt.xlabel(\"Cliff's Delta\", \n",
    "           fontsize = 16)\n",
    "plt.ylabel(\"Density\", \n",
    "           fontsize = 16)\n",
    "\n",
    "# ticks \n",
    "plt.tick_params(axis = \"both\", \n",
    "                labelsize = 14)\n",
    "\n",
    "# legend \n",
    "plt.legend(fontsize = 16)\n",
    "\n",
    "# show the plot \n",
    "plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f612c",
   "metadata": {},
   "source": [
    "A 95 percent confidence interval means that, over repeated samples, 95 percent of such intervals would contain the true Cliff’s delta. In this case, the estimated Cliff’s delta lies between 0.0127 and 0.0466, with a point estimate of 0.0297. This corresponds to a negligible effect size, meaning that the distributions of average ratings for male and female professors are almost entirely overlapping. Although this difference is statistically detectable given the large sample size, it does not represent a practically meaningful difference in central tendency and provides little evidence of substantive gender bias in average ratings.\n",
    "\n",
    "Now we will compute a point estimate and 95% confidence interval for dispersion difference effect size. To measure the effect size of dispersion, we substract the female variance from male variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3f140",
   "metadata": {},
   "source": [
    "### Effect size of gender bias in dispersion of average rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f125e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance difference in average rating for male and female professors\n",
    "def variance_diff(sample1, sample2, axis = 0):\n",
    "    # compute variance for each sample\n",
    "    variance1 = np.var(sample1, ddof = 1)\n",
    "    variance2 = np.var(sample2, ddof = 1)\n",
    "    return variance1 - variance2   # male minus female\n",
    "\n",
    "# compute the bootstrap\n",
    "result_variance = stats.bootstrap(\n",
    "    data=(df3_male['average_rating'].to_numpy(), \n",
    "    df3_female['average_rating'].to_numpy()),\n",
    "    statistic=variance_diff,\n",
    "    confidence_level=0.95,\n",
    "    n_resamples=10000,\n",
    "    method='BCa',\n",
    "    random_state=n_number,\n",
    "    vectorized=False\n",
    ")\n",
    "\n",
    "# compute the confidence interval\n",
    "ci_lower_variance, ci_upper_variance = result_variance.confidence_interval.low, result_variance.confidence_interval.high\n",
    "\n",
    "# print the results \n",
    "# point estimate\n",
    "print(f\"Effect size (variance difference): {variance_diff(df3_male['average_rating'], df3_female['average_rating'])}\")\n",
    "# confidence interval\n",
    "print(f\"95% CI: [{ci_lower_variance:.4f}, {ci_upper_variance:.4f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boostrap distribution of variance difference\n",
    "# figure size \n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# plot\n",
    "plt.hist(result_variance.bootstrap_distribution, \n",
    "         bins=40, \n",
    "         density=True, \n",
    "         color = \"orange\",\n",
    "         edgecolor = \"black\",\n",
    "         fill = True)\n",
    "\n",
    "# point estimate line\n",
    "plt.axvline(variance_diff(df3_male['average_rating'], \n",
    "                df3_female['average_rating']), \n",
    "            color = \"blue\", \n",
    "            linestyle = \"dashed\", \n",
    "            label = f\"Effect size: {variance_diff(df3_male['average_rating'], df3_female['average_rating']):.4f}\"\n",
    "            )\n",
    "\n",
    "# confidence interval lines\n",
    "# lower\n",
    "plt.axvline(ci_lower_variance, \n",
    "            color = \"red\", \n",
    "            linestyle = \"dashed\", \n",
    "            label = f\"95% CI: [{ci_lower_variance:.4f}, {ci_upper_variance:.4f}]\"\n",
    "            )\n",
    "# upper\n",
    "plt.axvline(ci_upper_variance, \n",
    "            color = \"red\", \n",
    "            linestyle = \"dashed\")\n",
    "\n",
    "# aesthetics\n",
    "# title \n",
    "plt.title(\"Bootstrap Distribution of Variance Difference for Average Rating by Gender\", \n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "\n",
    "# x and y axis labels\n",
    "plt.xlabel(\"Variance Difference\", \n",
    "           fontsize = 16)\n",
    "plt.ylabel(\"Density\", \n",
    "           fontsize = 16)\n",
    "\n",
    "# ticks \n",
    "plt.tick_params(axis = \"both\", \n",
    "                labelsize = 14)\n",
    "\n",
    "# legend \n",
    "plt.legend(fontsize = 16)\n",
    "\n",
    "# show the plot \n",
    "plt.show()\n",
    "\n",
    "# show the plot \n",
    "plt.show()\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7f97e",
   "metadata": {},
   "source": [
    "The estimated variance difference in average ratings between male and female professors is −0.0776, with a 95 percent CI from −0.1128 to −0.0411. The interval lies entirely below zero, indicating that average ratings for female professors exhibit slightly greater dispersion than those for male professors. However, the magnitude of this difference is very small in absolute terms. This suggests that, although the difference in spread is statistically detectable due to the large sample size, it is negligible in practice and provides little evidence of meaningful gender bias in rating variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8926c2",
   "metadata": {},
   "source": [
    "## Q4.  Is there a gender difference in the tags awarded by students? Make sure to test each of the 20 tags for a potential gender difference and report which of them exhibit a statistically significant difference. Comment on the 3 most gendered (lowest p-value) and least gendered (highest p-value) tags for a potential gender difference and report which of them exhibit a statistically significant difference. Comment on the 3 most gendered (lowest p-value) and least gendered (highest p-value) tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96586fa",
   "metadata": {},
   "source": [
    "`Rationale for the test:` Each teaching tag is converted into a proportion of all tags awarded to a professor in order to account for differences in the total number of ratings. These tag proportions are bounded between 0 and 1 and exhibit skewness, violating normality and equal variance assumptions. Parametric tests are therefore not appropriate.\n",
    "\n",
    "To compare tag distributions between male and female professors, we use a two-tailed Mann-Whitney U test. This non-parametric test is suitable for comparing two independent groups on ordinal or non-normal data and evaluates whether one group tends to receive systematically higher or lower tag proportions than the other.\n",
    "\n",
    "`Null hypothesis (H0):` The distribution of tag proportions is the same for male and female professors.\n",
    "\n",
    "`Alternative hypothesis (Ha):` The distribution of tag proportions significantly differs between male and female professors.\n",
    "\n",
    "`Significance level:` 0.005\n",
    "\n",
    "The p-value represents the probability of observing a difference in tag proportions, or a more extreme one, assuming the null hypothesis is true.\n",
    "\n",
    "If the p-value is smaller than 0.005, we reject H0 and conclude that the tag is awarded in significantly different proportions to male and female professors.\n",
    "If the p-value is larger than 0.005, we fail to reject H0 and conclude that any observed difference is consistent with random variation.\n",
    "\n",
    "This test is applied separately to each of the 20 teaching tags in the dataset in order to identify which tags exhibit statistically significant gender differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter our the data for this question \n",
    "df4_male = df_filtered_final[df_filtered_final[\"male_gender\"] == 1].copy()\n",
    "df4_female = df_filtered_final[df_filtered_final[\"male_gender\"] == 0].copy()\n",
    "\n",
    "# filter mask for all tags columns\n",
    "tag_columns = df_tags_column_names\n",
    "alpha = 0.005\n",
    "\n",
    "# list for all tags \n",
    "results_q4 = []\n",
    "\n",
    "# for loop for separating each movie subset into two groups and conducting the non-parametric test\n",
    "for tag in tag_columns: \n",
    "    # separating into two groups\n",
    "    males = df4_male[tag]\n",
    "    females = df4_female[tag]\n",
    "\n",
    "    # conducting the test \n",
    "    u_statistic, p_value = stats.mannwhitneyu(males, females, alternative= 'two-sided')\n",
    "    \n",
    "    # save data as well\n",
    "    results_q4.append({\n",
    "    \"tag\": tag,\n",
    "    \"n_female\": len(females),\n",
    "    \"n_male\": len(males),\n",
    "    \"U\": u_statistic,\n",
    "    \"p_value\": p_value,\n",
    "    \"significant\": p_value < alpha\n",
    "    })\n",
    "        \n",
    "    # # use the significance function created \n",
    "    # print(f\"{tag} Result:\")\n",
    "    # print(f\"{significance(0.005, p_value)}\\n\")\n",
    "\n",
    "# check the results\n",
    "results_q4_df = pd.DataFrame(results_q4).sort_values(by = 'p_value', ascending= False)\n",
    "styled = results_q4_df.style.set_properties(**{\n",
    "    \"background-color\": \"white\",\n",
    "    \"color\": \"black\", \n",
    "     \"border\": \"1px solid black\"\n",
    "}).format({\"p_value\": \"{:.6e}\"})\n",
    "\n",
    "# sort by p-value\n",
    "styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382229f4",
   "metadata": {},
   "source": [
    "We tested all 20 tag proportions for gender differences using two tailed Mann-Whitney U tests at a significance level of 0.005. Most tags show statistically detectable gender differences, while a small number do not. The least gendered tags are pop_quizzes and accessible, which have the highest p values and show no evidence of a gender difference, and tough_grader, which shows only weak evidence. The most gendered tags are hilarious, amazing_lectures, and a participation_matters, which have the lowest p values and the strongest statistical evidence of a difference in tag proportions by gender. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e5f3d",
   "metadata": {},
   "source": [
    "## Q5. Is there a gender difference in terms of average difficulty? Again, a significance test is indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into two groups based on gender\n",
    "# Male = 1, Male = 0\n",
    "df5_male = df_filtered_final[df_filtered_final[\"male_gender\"] == 1].copy()\n",
    "df5_female = df_filtered_final[df_filtered_final[\"male_gender\"] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check quantity \n",
    "# it is the same number of rows, so this code is just in case\n",
    "print(f\"Number of male professors: {df5_male.shape[0]}\")\n",
    "print(f\"Number of female professors: {df5_female.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare distributions of average rating between male and female professors\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# box plot for male and female professors \n",
    "sns.boxplot(data=df_filtered_final, \n",
    "            x='average_difficulty', \n",
    "            y='male_gender', \n",
    "            palette = \"Set3\", \n",
    "            showfliers = False, \n",
    "            medianprops = {\"color\": \"red\", \"linewidth\": 2}, \n",
    "            orient= \"horizontal\")\n",
    "\n",
    "# aesthetics \n",
    "plt.title('Average Difficulty Rating Distribution by Gender', \n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "\n",
    "# xlabel\n",
    "plt.xlabel('Average Difficulty Rating', \n",
    "           fontsize = 16)\n",
    "# ylabel\n",
    "plt.ylabel('Gender (1 = Male, 0 = Female)', \n",
    "           fontsize = 16)\n",
    "\n",
    "# legend\n",
    "plt.legend(['0 - Female', \n",
    "            '1 - Male'], \n",
    "            fontsize = 16,\n",
    "           loc = \"upper left\")\n",
    "# ticks \n",
    "plt.tick_params(axis = \"both\", \n",
    "                labelsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4875aed",
   "metadata": {},
   "source": [
    "The box plots of average difficulty ratings for male and female professors show substantial overlap and do not suggest a clear difference in typical difficulty. Visually, there is no strong evidence of a difference between the two groups. To formally assess whether any difference exists, we apply a two-tailed Mann-Whitney U test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c527c68",
   "metadata": {},
   "source": [
    "### Mann-Whitney U Test (two-tailed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92396edc",
   "metadata": {},
   "source": [
    "`Rationale for the test`: We use a two-tailed Mann-Whitney U test to compare average difficulty ratings by gender. This test is appropriate because average ratings are ordinal and non-normal.\n",
    "\n",
    "`Null hypothesis (H0):` Average difficulty ratings for male professors are not different than average difficulty ratings of female professors.\n",
    "\n",
    "`Alternative hypothesis (Ha):` Average difficulty ratings for male professors are significantly different from average difficulty ratings of female professors.\n",
    "\n",
    "`Significance level:` 0.005\n",
    "\n",
    "`Interpretation:` If the p-value is smaller than 0.005, it means such a result would be very unlikely by chance under the null hypothesis, so we reject the H0 and conclude that average difficulty ratings for male professors are significantly different from average difficulty ratings of female professors.\n",
    "\n",
    "If the p-value is larger than 0.005, then the observed difference could easily occur by random variation, so we fail to reject the H0, meaning the data do not provide enough statistical evidence to conclude that there is a significant difference in average difficulty ratings by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da552a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct mann-whitney u test for average rating using our function\n",
    "u_stat, p_value = stats.mannwhitneyu(df5_male['average_difficulty'], \n",
    "                                     df5_female['average_difficulty'], \n",
    "                                     alternative='two-sided')\n",
    "# interpretation \n",
    "significance(0.005, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab1596",
   "metadata": {},
   "source": [
    "The Mann-Whitney U test yielded a p-value of 0.828753, which is substantially larger than the significance level of 0.005. Therefore, we fail to reject the null hypothesis.\n",
    "\n",
    "This result indicates that the observed differences in average difficulty ratings between male and female professors are consistent with random variation. The data provide no statistical evidence of a gender difference in perceived course difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa840d04",
   "metadata": {},
   "source": [
    "## Q6. Please quantify the likely size of this effect at 95% confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5008de",
   "metadata": {},
   "source": [
    "Because average difficulty ratings are ordinal and non-normal, parametric effect size measures such as Cohen's d and Hedges' g are not appropriate. As introduced earlier in this report, we therefore use Cliff's delta (δ) as a non-parametric measure of effect size suitable for ordinal data (Meissel & Yao, 2024).\n",
    "\n",
    "Cliff's delta is interpreted using the same thresholds defined previously: negligible (|δ| < 0.15), small (0.15 ≤ |δ| < 0.33), medium (0.33 ≤ |δ| < 0.47), and large (|δ| ≥ 0.47).\n",
    "\n",
    "We apply this measure here to quantify the magnitude of any gender difference in average difficulty ratings, complementing the hypothesis test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # divide average difficulty ratings by groups of gender and convert to arrays (bootstrap expects that)\n",
    "df6_male = df_filtered_final[df_filtered_final[\"male_gender\"] == 1].copy()\n",
    "df6_female = df_filtered_final[df_filtered_final[\"male_gender\"] == 0].copy()\n",
    "\n",
    "# define function for effect size \n",
    "def cliffs_delta_trap(sample1, sample2, axis = 0):\n",
    "    return cd(sample1, sample2)[0]\n",
    "\n",
    "# Perform the bootstrap\n",
    "result_q6 = stats.bootstrap(\n",
    "    (df6_male['average_difficulty'].to_numpy(), \n",
    "    df6_female['average_difficulty'].to_numpy()), \n",
    "    statistic=cliffs_delta_trap,\n",
    "    confidence_level=0.95, \n",
    "    random_state= n_number, \n",
    "    vectorized= False,\n",
    "    method='BCa', \n",
    "    n_resamples=10000)\n",
    "\n",
    "# compute the confidence interval\n",
    "ci_lower_q6, ci_upper_q6 = result_q6.confidence_interval.low, result_q6.confidence_interval.high\n",
    "\n",
    "# print the results \n",
    "# point estimate\n",
    "print(f\"Effect size (Cliffs delta): {cd(df6_male['average_difficulty'], \n",
    "                                        df6_female['average_difficulty'])[0]:.4f}\")\n",
    "\n",
    "# confidence interval\n",
    "print(f\"95% CI: [{ci_lower_q6:.4f}, {ci_upper_q6:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boostrap distribution of cliffs delta\n",
    "# figure size \n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# plot\n",
    "plt.hist(result_q6.bootstrap_distribution, \n",
    "         bins=40, \n",
    "         density=True, \n",
    "         color = \"darkgreen\",\n",
    "         edgecolor = \"black\",\n",
    "         fill = True)\n",
    "\n",
    "# point estimate line\n",
    "plt.axvline(cd(df6_male['average_difficulty'].to_numpy(), \n",
    "                df6_female['average_difficulty'].to_numpy())[0], \n",
    "            color = \"orange\", \n",
    "            linestyle = \"dashed\", \n",
    "            label = f\"Effect size: {cd(df6_male['average_difficulty'].to_numpy(), \n",
    "                                            df6_female['average_difficulty'].to_numpy())[0]:.4f}\"\n",
    "            )\n",
    "\n",
    "# confidence interval lines\n",
    "# lower\n",
    "plt.axvline(ci_lower_q6, \n",
    "            color = \"red\", \n",
    "            linestyle = \"dashed\", \n",
    "            label = f\"95% CI: [{ci_lower_q6:.4f}, {ci_upper_q6:.4f}]\"\n",
    "            )\n",
    "# upper\n",
    "plt.axvline(ci_upper_q6, \n",
    "            color = \"red\", \n",
    "            linestyle = \"dashed\")\n",
    "\n",
    "# aesthetics\n",
    "# title \n",
    "plt.title(\"Bootstrap Distribution of Cliff's Delta for Average Difficulty Rating by Gender\", \n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "\n",
    "# x and y axis labels\n",
    "plt.xlabel(\"Cliff's Delta\", \n",
    "           fontsize = 16)\n",
    "plt.ylabel(\"Density\", \n",
    "           fontsize = 16)\n",
    "\n",
    "# ticks \n",
    "plt.tick_params(axis = \"both\", \n",
    "                labelsize = 14)\n",
    "\n",
    "# legend \n",
    "plt.legend(fontsize = 16)\n",
    "\n",
    "# show the plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87370db",
   "metadata": {},
   "source": [
    "After computing a 95% confidence interval using bootstrap resampling, the estimated Cliff's delta lies between -0.0152 and 0.0182, with a point estimate of 0.0018. This corresponds to a negligible effect size, indicating that the distributions of average difficulty ratings for male and female professors almost completely overlap.\n",
    "\n",
    "Because the confidence interval includes zero and remains well within the negligible range, there is no practical evidence of a meaningful gender difference in perceived course difficulty. This result reinforces the hypothesis test findings and suggests that any statistically detectable differences are not substantively important at the user-experience level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b418d1",
   "metadata": {},
   "source": [
    "## Q7 - Build a regression model predicting average rating from all numerical predictors (the ones in the rmpCapstoneNum.csv) file. Make sure to include the R2 and RMSE of this model. Which of these factors is most strongly predictive of average rating? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56286d1b",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data for the question \n",
    "df_7 = df_filtered_final[df_num_column_names].copy()\n",
    "\n",
    "# drop female_gender as it is redundant (we have male_gender binary variable already)\n",
    "df_7 = df_7.drop(columns = ['female_gender'])\n",
    "# check \n",
    "df_7.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into x and y \n",
    "# x \n",
    "x_7 = df_7.drop(columns = \"average_rating\")\n",
    "\n",
    "# y \n",
    "y_7 = df_7[\"average_rating\"]\n",
    "\n",
    "# split the data into train and test data \n",
    "x_train_7, x_test_7, y_train_7, y_test_7 = train_test_split(x_7, y_7, test_size=0.2, random_state=n_number)\n",
    "\n",
    "# check distributions of the data \n",
    "print(f'Number of rows for train data:{y_train_7.shape[0]}\\n')\n",
    "print(f'Number of rows for test data:{y_test_7.shape[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63235be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix for x and y \n",
    "corr7_yx = x_train_7.corrwith(y_train_7, \n",
    "                              method = 'pearson').sort_values(ascending= False)\n",
    "# check \n",
    "corr7_yx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55661e",
   "metadata": {},
   "source": [
    "The correlations indicate that average ratings are driven primarily by student satisfaction and course rigor rather than demographics. Would take again shows an extremely strong positive association with ratings, while average difficulty exhibits a strong negative relationship, suggesting that harder courses tend to receive lower evaluations. Received a pepper has a moderate positive association, whereas gender, number of ratings, and online share show near-zero correlations, indicating minimal influence. These results suggest that instructional experience and perceived difficulty dominate rating outcomes, while demographic and exposure-related variables contribute little explanatory value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of features with each other \n",
    "corr7_x = x_train_7.corr()\n",
    "\n",
    "# Plot the heatmap using Seaborn\n",
    "plt.figure(figsize=(12, 10)) # Adjust the figure size as needed\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(corr7_x, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# aesthetics \n",
    "# title \n",
    "plt.title('Correlation Matrix of Numerical Features', fontweight = \"bold\", fontsize = 18)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d41831",
   "metadata": {},
   "source": [
    "The correlation matrix shows no strong pairwise correlations among the numerical predictors, indicating no evidence of multicollinearity. Most correlations are close to zero, including those involving gender, number of ratings, and online ratings. The largest association appears between average difficulty and “would take again” (≈ ~0.52), which reflects a meaningful but expected relationship rather than redundancy. Overall, the predictors capture distinct aspects of professor evaluation and can be included jointly in downstream models without collinearity concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088ec2e",
   "metadata": {},
   "source": [
    "#### KNN Imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40023f81",
   "metadata": {},
   "source": [
    "Only the variable `would_take_again` contains missing values. Dropping these observations would remove a substantial amount of information, especially because this variable shows a strong association with average rating.\n",
    "\n",
    "We therefore apply KNN imputation to recover missing values in `would_take_again` by leveraging similarity across the remaining numerical predictors. This method preserves local structure in the data and avoids the strong distributional assumptions imposed by mean or regression based imputation, which is appropriate given the bounded and ordinal nature of the ratings.\n",
    "\n",
    "The number of neighbors governs the bias variance tradeoff of the imputation. To avoid introducing noise, we tune this parameter within a cross validated pipeline, ensuring that the imputed values improve out of sample performance.\n",
    "\n",
    "We deliberately do not include a missing indicator for `would_take_again`. Although missingness exhibits statistical correlation with ratings, it reflects survey response behavior rather than any interpretable course characteristic. Including such indicators risks capturing artifacts of the data collection process rather than substantive effects and reduces the interpretability and external validity of the model. For this reason, missingness is handled solely through imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation\n",
    "cv = KFold(n_splits=5, \n",
    "           shuffle=True, \n",
    "           random_state=n_number\n",
    "           )\n",
    "\n",
    "# pipeline for tuning \n",
    "knn_pipe = Pipeline([\n",
    "    (\"imputer\", KNNImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# define objective function for optuna \n",
    "def objective(trial):\n",
    "    # parameters to tune\n",
    "    params = {\n",
    "        \"imputer__n_neighbors\": trial.suggest_int(\"imputer__n_neighbors\", 2, 25),\n",
    "        \"imputer__weights\": trial.suggest_categorical(\"imputer__weights\", [\"uniform\", \"distance\"]),\n",
    "        \"imputer__add_indicator\": trial.suggest_categorical(\"imputer__add_indicator\", [False])\n",
    "    }\n",
    "\n",
    "    # clone our model and set parameters \n",
    "    pipe = clone(knn_pipe)\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    # cross-validation score with tuned parameters \n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        x_train_7,\n",
    "        y_train_7,\n",
    "        cv=cv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # return average RMSE \n",
    "    return -scores.mean()\n",
    "\n",
    "# start the study for knn imputer tuning\n",
    "study_knn = start_study_optuna(\n",
    "    objective=objective,\n",
    "    n_trials=25,\n",
    "    sampler_seed=n_number,\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "\n",
    "# set the best parameters to the pipeline\n",
    "knn_pipe.set_params(**study_knn.best_params)\n",
    "\n",
    "# check the model performance with the best parameters \n",
    "score_model(\n",
    "    knn_pipe,\n",
    "    x_train_7,\n",
    "    x_test_7,\n",
    "    y_train_7,\n",
    "    y_test_7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb062ed5",
   "metadata": {},
   "source": [
    "The tuned preprocessing pipeline shows stable cross-validation performance. The best Optuna trial (trial 15) achieves a mean CV RMSE of ~0.584. The train and test results are also consistent, RMSE is 0.5832 on training and ~0.584 on testing, R² is ~0.6019 on training and ~0.62 on testing. This alignment suggests good generalization and no clear overfitting.\n",
    "\n",
    "We use this tuned preprocessing plus linear regression setup as our baseline. It gives a fair reference point because the preprocessing is selected using cross-validation, so the baseline reflects expected out-of-sample behavior.\n",
    "\n",
    "Next, we compare the tuned KNN-based imputation strategy against simpler imputation methods. In particular, we evaluate whether replacing KNN imputation with less complex approaches, such as mean or median imputation, leads to comparable out-of-sample performance. This comparison allows us to assess whether the additional complexity of KNN imputation is justified, or whether simpler imputers are sufficient for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb04e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed CV\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipelines for comparison\n",
    "pipelines = {\n",
    "    \"KNNImputer (tuned)\": Pipeline([\n",
    "        # imputer \n",
    "        (\"imputer\", KNNImputer(n_neighbors=20, \n",
    "                               weights=\"uniform\", \n",
    "                               add_indicator=False)),\n",
    "\n",
    "        # scaler \n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        # model \n",
    "        (\"model\", LinearRegression())\n",
    "    ]),\n",
    "\n",
    "    \"SimpleImputer (mean)\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\", \n",
    "                                  add_indicator=False)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LinearRegression())\n",
    "    ]),\n",
    "    \"SimpleImputer (median)\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\", \n",
    "                                  add_indicator=False)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LinearRegression())\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# compare the pipelines\n",
    "rows = []\n",
    "for name, pipe in pipelines.items():\n",
    "    results = cross_validate(\n",
    "        pipe,\n",
    "        x_train_7,\n",
    "        y_train_7,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        return_train_score=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"R2_mean\": results[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": (-results[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "    })\n",
    "\n",
    "# create a dataframe for comparison\n",
    "compare_imputers_df = (\n",
    "    pd.DataFrame(rows)\n",
    "      .sort_values(\"RMSE_mean\", \n",
    "                   ascending= True)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check the results\n",
    "compare_imputers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00c9c9",
   "metadata": {},
   "source": [
    "The comparison confirms that KNN imputation provides a clear advantage over simpler imputation strategies. The tuned KNN imputer achieves the best cross-validation performance, with an RMSE of ~0.584 and an R² of 0.601. In contrast, both mean and median imputation lead to noticeably worse results, with higher RMSE and lower R².\n",
    "\n",
    "This gap indicates that missing values in `would_take_again` are not random and that borrowing information from similar observations improves predictive accuracy. Simpler imputers fail to capture this structure and introduce additional noise, resulting in inferior out-of-sample performance.\n",
    "\n",
    "Given these results, we retain the tuned KNN imputation strategy as part of the preprocessing pipeline.\n",
    "\n",
    "Next, we move to regularized modeling by fitting a Lasso regression on top of this optimized preprocessing setup. The goal is to assess whether introducing sparsity can simplify the model and improve interpretability without sacrificing out-of-sample performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634cb0cb",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d175d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with all variables \n",
    "# pipeline for Lasso model\n",
    "pipeline_q7 = Pipeline([\n",
    "    (\"imputer\", KNNImputer(n_neighbors=20, \n",
    "                           weights= 'uniform', \n",
    "                           add_indicator= False\n",
    "                           )),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LassoCV(\n",
    "        cv = 5, \n",
    "        random_state = n_number,\n",
    "        alphas= 100, \n",
    "        max_iter= 10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# cross-validation \n",
    "cv = KFold(n_splits=5, \n",
    "           shuffle=True, \n",
    "           random_state=n_number)\n",
    "\n",
    "# run cross-validation\n",
    "cv_results_raw_7 = cross_validate(\n",
    "    pipeline_q7,\n",
    "    x_train_7,\n",
    "    y_train_7,\n",
    "    cv=cv,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    return_train_score=False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# results \n",
    "cv_results_7 = [{\n",
    "    \"model\": \"LassoCV (all features)\", \n",
    "    \"R2_mean\": cv_results_raw_7[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-cv_results_raw_7[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "}]\n",
    "\n",
    "# save to dataframe for comparison later \n",
    "compare_models_q7_df = pd.DataFrame(cv_results_7).sort_values(\"RMSE_mean\").reset_index(drop = True)\n",
    "\n",
    "# check \n",
    "compare_models_q7_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model on test data \n",
    "score_model(pipeline_q7, \n",
    "            x_train_7, \n",
    "            x_test_7, \n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fffe9",
   "metadata": {},
   "source": [
    "The Lasso model achieves nearly identical cross-validated performance compared to the baseline. The change in RMSE is negligible and within expected sampling variability. This suggests that the baseline model already captures the dominant linear relationships in the data, and additional regularization does not provide a meaningful predictive advantage.\n",
    "\n",
    "The value of Lasso in this setting is therefore interpretability rather than accuracy. By shrinking weaker coefficients toward zero, it confirms that only a small subset of predictors contributes materially to the model, while maintaining essentially the same out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e000dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature weights from the model\n",
    "model = pipeline_q7.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names = x_train_7.columns.to_list()\n",
    "# coefficients\n",
    "coef = model.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_7 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_7[\"abs_weight\"] = weights_df_7[\"weight\"].abs()\n",
    "weights_df_7 = weights_df_7.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# check \n",
    "weights_df_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights of the multiple linear regression model \n",
    "# figure size \n",
    "plt.figure(figsize = (12, 10))\n",
    "\n",
    "# bar plot \n",
    "sns.barplot(y = weights_df_7[\"tag\"], \n",
    "        x = weights_df_7[\"weight\"])\n",
    "\n",
    "# aesthetics\n",
    "# title \n",
    "plt.title(\"Lasso Regression Coefficients for Predicting Average Rating\", \n",
    "          fontweight=\"bold\", \n",
    "          fontsize=18)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel(\"Coefficient value\", fontsize=16)\n",
    "plt.ylabel(\"Predictor\", fontsize=16)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(axis=\"both\", labelsize=14)\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50470b72",
   "metadata": {},
   "source": [
    "To focus the model on meaningful predictors, we retain variables whose absolute standardized coefficients exceed 5 percent. This cutoff removes predictors with negligible contribution while keeping variables that materially affect predicted ratings.\n",
    "\n",
    "Based on the Lasso results, three variables meet this criterion.\n",
    "\n",
    "`would_take_again (0.456)`\n",
    "- This is the strongest positive predictor. Professors whom students are willing to take again receive substantially higher ratings. This variable captures core teaching quality and overall student satisfaction.\n",
    "\n",
    "`average_difficulty (-0.271)`\n",
    "- This shows a strong negative relationship. Higher perceived difficulty is associated with lower ratings, even after controlling for other factors. This suggests that difficulty directly shapes student evaluations.\n",
    "\n",
    "`received_a_pepper (0.119)`\n",
    "- This has a moderate positive effect. Perceived attractiveness correlates with higher ratings, but the magnitude is much smaller than the effect of teaching related variables.\n",
    "\n",
    "All remaining variables, including gender, number of ratings, and online rating volume, have coefficients close to zero and are excluded. Their limited weights indicate minimal incremental predictive value once the main drivers are accounted for.\n",
    "\n",
    "This selection step yields a simpler and more interpretable model driven primarily by student experience and course perception rather than demographic or exposure related noise. We now evaluate the performance of this reduced model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70905af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features with |weight| >= 0.05\n",
    "selected_features = (\n",
    "weights_df_7\n",
    ".loc[weights_df_7[\"abs_weight\"] >= 0.05, \"tag\"]\n",
    ".tolist()\n",
    ")\n",
    "\n",
    "# reduce training data \n",
    "x_train_reduced_7 = x_train_7[selected_features]\n",
    "x_test_reduced_7 = x_test_7[selected_features]\n",
    "\n",
    "# cross-validation \n",
    "cv = KFold(n_splits=5, \n",
    "           shuffle=True, \n",
    "           random_state=n_number)\n",
    "\n",
    "# pipeline for model \n",
    "pipeline_q7_reduced = Pipeline([\n",
    "    (\"imputer\", KNNImputer(n_neighbors=20, \n",
    "                           weights= 'uniform', \n",
    "                           add_indicator= False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LassoCV(\n",
    "        cv = cv, \n",
    "        random_state = n_number,\n",
    "        alphas = 100, \n",
    "        max_iter= 10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# run cross-validation\n",
    "cv_results_raw_7_reduced = cross_validate(\n",
    "    pipeline_q7_reduced,\n",
    "    x_train_reduced_7,\n",
    "    y_train_7,\n",
    "    cv=cv,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    return_train_score=False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# save results \n",
    "cv_results_7.append({\n",
    "    \"model\": f\"LassoCV | {len(x_train_reduced_7.columns)} features\",\n",
    "    \"R2_mean\": cv_results_raw_7_reduced[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-cv_results_raw_7_reduced[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "\n",
    "# create dataframe \n",
    "compare_models_q7_df = pd.DataFrame(cv_results_7).sort_values(by = \"RMSE_mean\").reset_index(drop = True)\n",
    "\n",
    "# check results \n",
    "compare_models_q7_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model performance with all features:\\n\")\n",
    "# check the model on test data \n",
    "score_model(pipeline_q7, \n",
    "            x_train_7, \n",
    "            x_test_7, \n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bddbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model performance with the most important features:\\n\")\n",
    "# check on test data again \n",
    "score_model(pipeline_q7_reduced, \n",
    "            x_train_reduced_7, \n",
    "            x_test_reduced_7, \n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model standardized weights\n",
    "# extract feature weights from the model\n",
    "final_model_q7 = pipeline_q7_reduced.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names_final = x_train_reduced_7.columns.to_list()\n",
    "# coefficients\n",
    "coef_final = final_model_q7.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_7_final = pd.DataFrame({\n",
    "    \"tag\": feature_names_final,\n",
    "    \"weight\": coef_final\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_7_final[\"abs_weight\"] = weights_df_7_final[\"weight\"].abs()\n",
    "weights_df_7_final = weights_df_7_final.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# check \n",
    "weights_df_7_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db1fb49",
   "metadata": {},
   "source": [
    "The reduced Lasso model consistently outperforms the full-feature baseline across all evaluation settings. In cross-validation, RMSE decreases from ~0.584 to ~0.57 and R² increases from ~0.601 to ~0.619. \n",
    "\n",
    "The same pattern holds on the hold-out set. Test RMSE drops from ~0.584 to ~0.569, while test R² increases from ~0.616 to ~0.635. Training and testing metrics remain closely aligned, suggesting that the performance gain is not driven by overfitting.\n",
    "\n",
    "These results indicate that several predictors in the full model contribute noise rather than signal. Removing them improves generalization and yields a simpler model with better predictive accuracy.\n",
    "\n",
    "Overall, the reduced model achieves a better tradeoff between performance and complexity. It shows that ratings are primarily driven by teaching experience and course perception, while demographic and exposure-related variables add little once the core predictors are included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381370a6",
   "metadata": {},
   "source": [
    "### Extra: Feature Engineering\n",
    "#### Do ratings change linearly with difficulty and volume?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8888e7",
   "metadata": {},
   "source": [
    "To test for nonlinear relationships, we introduced polynomial and logarithmic transformations of selected predictors. Because these transformations induce multicollinearity, we employed ElasticNetCV, which combines L1 and L2 regularization to stabilize coefficient estimates while controlling overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform average difficulty and number of ratings \n",
    "# average difficulty in a square \n",
    "x_train_7[\"average_difficulty^2\"] = np.square(x_train_7[\"average_difficulty\"])\n",
    "x_test_7[\"average_difficulty^2\"] = np.square(x_test_7[\"average_difficulty\"])\n",
    "\n",
    "# log number of ratings \n",
    "x_train_7[\"number_of_ratings_log\"] = np.log(1 + x_train_7[\"number_of_ratings\"])\n",
    "x_test_7[\"number_of_ratings_log\"] = np.log(1 + x_test_7[\"number_of_ratings\"])\n",
    "    \n",
    "x_train_7[\"number_of_ratings_online_log\"] = np.log(1 + x_train_7[\"number_of_ratings_online\"])\n",
    "\n",
    "x_test_7[\"number_of_ratings_online_log\"] = np.log(1 + x_test_7[\"number_of_ratings_online\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe28a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cv\n",
    "cv = KFold(n_splits = 5, \n",
    "           shuffle = True, \n",
    "           random_state = n_number)\n",
    "\n",
    "# pipeline to test transformations\n",
    "pipeline_q7_transform = Pipeline([\n",
    "    # null values imputer \n",
    "    (\"imputer\", KNNImputer(n_neighbors= 20, \n",
    "                           weights = \"uniform\", \n",
    "                           add_indicator= False)),\n",
    "    \n",
    "    # data standardization (scaler)\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", ElasticNetCV(\n",
    "        cv = cv, \n",
    "        alphas = 100, \n",
    "        max_iter= 10000, \n",
    "        random_state = n_number\n",
    "    ))\n",
    "])\n",
    "\n",
    "# run cross_validation\n",
    "cv_results_transformation_raw_7 = cross_validate(\n",
    "    pipeline_q7_transform, \n",
    "    x_train_7, \n",
    "    y_train_7, \n",
    "    cv = cv, \n",
    "    scoring = (\"r2\", \"neg_root_mean_squared_error\"), \n",
    "    return_train_score = False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# save results \n",
    "cv_results_transformation_7 = {\n",
    "    \"R2_mean\": cv_results_transformation_raw_7[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": -cv_results_transformation_raw_7[\"test_neg_root_mean_squared_error\"].mean()\n",
    "\n",
    "}\n",
    "\n",
    "# save results \n",
    "cv_results_7.append({\n",
    "    \"model\": \"ElasticNetCV (with transformations)\",\n",
    "    \"R2_mean\": cv_results_transformation_raw_7[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-cv_results_transformation_raw_7[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "\n",
    "# create dataframe \n",
    "compare_models_q7_df = pd.DataFrame(cv_results_7).sort_values(by = \"RMSE_mean\").reset_index(drop = True)\n",
    "\n",
    "# check results \n",
    "compare_models_q7_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on test data\n",
    "print(\"Reduced LassoCV performance on test data:\\n\")\n",
    "score_model(pipeline_q7_transform, \n",
    "            x_train_7, \n",
    "            x_test_7, \n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da91549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of reduced model on test data \n",
    "print(\"ElasticNetCV with transformations performance on test data:\\n\")\n",
    "score_model(pipeline_q7_reduced, \n",
    "            x_train_reduced_7, \n",
    "            x_test_reduced_7, \n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42a29d",
   "metadata": {},
   "source": [
    "The ElasticNet model with nonlinear transformations does not meaningfully outperform the reduced Lasso model on either cross-validation or test data. Performance differences are within noise, suggesting that most predictive power is already captured by the reduced feature set.\n",
    "\n",
    "Next, we examine coefficient magnitudes to identify and remove insignificant variables. We also test whether retaining only the squared difficulty term improves model parsimony and interpretability relative to the linear difficulty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature weights from the model\n",
    "model = pipeline_q7_transform.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names = x_train_7.columns.to_list()\n",
    "# coefficients\n",
    "coef = model.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_7 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_7[\"abs_weight\"] = weights_df_7[\"weight\"].abs()\n",
    "weights_df_7 = weights_df_7.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# cleanup\n",
    "weights_df_7 = weights_df_7.drop(columns=\"abs_weight\").reset_index(drop=True)\n",
    "\n",
    "# check \n",
    "weights_df_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0baadf8",
   "metadata": {},
   "source": [
    "The coefficient magnitudes from the ElasticNet model confirm the expected structure:\n",
    "- `would_take_again` remains the dominant predictor.\n",
    "- `average_difficulty²` has a strong negative coefficient, indicating a nonlinear relationship between difficulty and rating.\n",
    "- The linear `average_difficulty` term is substantially weaker once the squared term is included.\n",
    "\n",
    "Other variables have negligible coefficients and contribute little to predictive performance. Given the clear nonlinear effect of difficulty, we remove the `linear average_difficulty` term and retain only its squared transformation. This results in a parsimonious model with three primary predictors.\n",
    "\n",
    "Next, we assess whether introducing the squared term materially increases multicollinearity. If multicollinearity remains controlled, we refit the model using LassoCV rather than ElasticNetCV, since Lasso provides comparable regularization with lower computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features with |weight| >= 0.05\n",
    "selected_features = [\"would_take_again\", \n",
    "                     \"average_difficulty^2\", \n",
    "                     \"received_a_pepper\"]\n",
    "\n",
    "# reduce training data \n",
    "x_train_transformation_reduced_7 = x_train_7[selected_features]\n",
    "x_test_transformation_reduced_7 = x_test_7[selected_features]\n",
    "\n",
    "\n",
    "# correlations between predictors \n",
    "corr_pred_transformation_reduced_7 = x_train_transformation_reduced_7.corr()\n",
    "\n",
    "# Plot the heatmap using Seaborn\n",
    "plt.figure(figsize=(12, 10)) # Adjust the figure size as needed\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(corr_pred_transformation_reduced_7, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# aesthetics \n",
    "# title \n",
    "plt.title('Correlation Matrix of Numerical Features', fontweight = \"bold\", fontsize = 18)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ccbb0",
   "metadata": {},
   "source": [
    "The correlation matrix for the reduced feature set shows no evidence of severe multicollinearity:\n",
    "- Correlations are all well below common concern thresholds of |0.7-0.8|.\n",
    "- The strongest relationship is between `would_take_again` and `average_difficulty²` at -0.53, which reflects a meaningful behavioral association rather than redundancy.\n",
    "- `received_a_pepper` shows only moderate correlation with the other predictors.\n",
    "\n",
    "These results indicate that introducing the squared difficulty term does not materially inflate multicollinearity once the linear difficulty term is removed.\n",
    "Given this, regularization via ElasticNet is no longer strictly necessary, and we proceed by refitting the model using LassoCV for improved simplicity and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation \n",
    "cv = KFold(n_splits=5, \n",
    "           shuffle=True, \n",
    "           random_state=n_number)\n",
    "\n",
    "# pipeline for model \n",
    "pipeline_q7_transformation_reduced = Pipeline([\n",
    "    (\"imputer\", KNNImputer(n_neighbors=20, \n",
    "                           weights= 'uniform', \n",
    "                           add_indicator= False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LassoCV(\n",
    "        cv = cv, \n",
    "        random_state = n_number,\n",
    "        alphas = 100, \n",
    "        max_iter= 10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# run cross-validation\n",
    "cv_results_raw_7_transformation_reduced = cross_validate(\n",
    "    pipeline_q7_transformation_reduced,\n",
    "    x_train_transformation_reduced_7,\n",
    "    y_train_7,\n",
    "    cv=cv,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    return_train_score=False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# save results \n",
    "cv_results_7.append({\n",
    "    \"model\": \"LassoCV | transformation + 3 features\",\n",
    "    \"R2_mean\": cv_results_raw_7_transformation_reduced[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-cv_results_raw_7_transformation_reduced[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "    })\n",
    "\n",
    "# show as a dataframe\n",
    "compare_models_q7_df = pd.DataFrame(cv_results_7).sort_values(by = \"RMSE_mean\").reset_index(drop = True)\n",
    "compare_models_q7_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e666ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV (reduced features) performance on the test data\n",
    "print(\"LassoCV (reduced features) performance on the test data\\n\")\n",
    "score_model(pipeline_q7_reduced, \n",
    "            x_train_reduced_7, \n",
    "            x_test_reduced_7, \n",
    "            y_train_7,\n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV (transformation + reduced features) performance on the test data\n",
    "print(\"LassoCV (transformation + reduced features) performance on the test data:\\n\")\n",
    "score_model(pipeline_q7_transformation_reduced, \n",
    "            x_train_transformation_reduced_7, \n",
    "            x_test_transformation_reduced_7, \n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model weights \n",
    "# extract feature weights from the model\n",
    "model_poly = pipeline_q7_transformation_reduced.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names_poly = x_train_transformation_reduced_7.columns.to_list()\n",
    "# coefficients\n",
    "coef_poly = model_poly.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_7_poly = pd.DataFrame({\n",
    "    \"tag\": feature_names_poly,\n",
    "    \"weight\": coef_poly\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_7_poly[\"abs_weight\"] = weights_df_7_poly[\"weight\"].abs()\n",
    "weights_df_7_poly = weights_df_7_poly.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# check \n",
    "weights_df_7_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db750e8d",
   "metadata": {},
   "source": [
    "Cross-validation results show that introducing nonlinear transformations improves model performance. The best cross-validated specification, LassoCV with transformations and reduced features, achieves an R² of approximately ~0.622 and an RMSE of about ~0.568, outperforming both the linear reduced model with an RMSE of roughly ~0.570 and the full linear model with an RMSE near ~0.584. ElasticNetCV produces nearly identical cross-validation metrics, indicating that multicollinearity is adequately controlled and that Lasso regularization is sufficient.\n",
    "\n",
    "Evaluation on the held-out test set confirms this pattern. The reduced Lasso model attains a testing RMSE of about ~0.567 and an R² of approximately ~0.635, while the transformed reduced Lasso model further improves performance with a testing RMSE of roughly ~0.561 and an R² near 0.644. These results motivate a final assessment of interaction effects to determine whether any additional structure remains unexploited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f5686",
   "metadata": {},
   "source": [
    "#### Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interaction variables \n",
    "# would take gain and difficulty \n",
    "x_train_7[\"wta_x_difficulty\"] = (\n",
    "x_train_7[\"would_take_again\"] * x_train_7[\"average_difficulty\"]\n",
    ")\n",
    "\n",
    "x_test_7[\"wta_x_difficulty\"] = (\n",
    "x_test_7[\"would_take_again\"] * x_test_7[\"average_difficulty\"]\n",
    ")\n",
    "\n",
    "x_train_7[\"wta_x_difficulty2\"] = (\n",
    "x_train_7[\"would_take_again\"] * x_train_7[\"average_difficulty^2\"]\n",
    ")\n",
    "\n",
    "x_test_7[\"wta_x_difficulty2\"] = (\n",
    "x_test_7[\"would_take_again\"] * x_test_7[\"average_difficulty^2\"]\n",
    ")\n",
    "\n",
    "# pepper and would take again \n",
    "x_train_7[\"pepper_x_wta\"] = (\n",
    "x_train_7[\"received_a_pepper\"] * x_train_7[\"would_take_again\"]\n",
    ")\n",
    "\n",
    "x_test_7[\"pepper_x_wta\"] = (\n",
    "x_test_7[\"received_a_pepper\"] * x_test_7[\"would_take_again\"]\n",
    ")\n",
    "\n",
    "# would take again and log_number of ratings \n",
    "x_train_7[\"wta_x_log_number_of_ratings\"] = (x_train_7[\"would_take_again\"] * x_train_7[\"number_of_ratings_online_log\"])\n",
    "x_train_7[\"wta_x_log_number_of_ratings\"] = (x_train_7[\"would_take_again\"] * x_train_7[\"number_of_ratings_online_log\"])\n",
    "\n",
    "x_train_7[\"wta_x_log_number_of_ratings_online\"] = (x_train_7[\"would_take_again\"] * x_train_7[\"number_of_ratings_online_log\"])\n",
    "x_train_7[\"wta_x_log_number_of_ratings_online\"] = (x_train_7[\"would_take_again\"] * x_train_7[\"number_of_ratings_online_log\"])\n",
    "\n",
    "x_test_7[\"wta_x_log_number_of_ratings\"] = (x_test_7[\"would_take_again\"] * x_test_7[\"number_of_ratings_online_log\"])\n",
    "x_test_7[\"wta_x_log_number_of_ratings\"] = (x_test_7[\"would_take_again\"] * x_test_7[\"number_of_ratings_online_log\"])\n",
    "\n",
    "x_test_7[\"wta_x_log_number_of_ratings_online\"] = (x_test_7[\"would_take_again\"] * x_test_7[\"number_of_ratings_online_log\"])\n",
    "x_test_7[\"wta_x_log_number_of_ratings_online\"] = (x_test_7[\"would_take_again\"] * x_test_7[\"number_of_ratings_online_log\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7313ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cv\n",
    "cv = KFold(n_splits = 5, \n",
    "           shuffle = True, \n",
    "           random_state = n_number)\n",
    "\n",
    "# pipeline to test interactions\n",
    "pipeline_q7_interactions = Pipeline([\n",
    "    # null values imputer \n",
    "    (\"imputer\", KNNImputer(n_neighbors= 20, \n",
    "                           weights = \"uniform\", \n",
    "                           add_indicator= False)),\n",
    "    \n",
    "    # data standardization (scaler)\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", ElasticNetCV(\n",
    "        cv = cv, \n",
    "        alphas = 100, \n",
    "        max_iter= 10000, \n",
    "        random_state = n_number\n",
    "    ))\n",
    "])\n",
    "\n",
    "# run cross_validation\n",
    "cv_results_interactions_raw_7 = cross_validate(\n",
    "    pipeline_q7_interactions, \n",
    "    x_train_7, \n",
    "    y_train_7, \n",
    "    cv = cv, \n",
    "    scoring = (\"r2\", \"neg_root_mean_squared_error\"), \n",
    "    return_train_score = False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# save results \n",
    "cv_results_7.append({\n",
    "    \"model\": \"ElasticNet CV (interactions)\",\n",
    "    \"R2_mean\": cv_results_interactions_raw_7[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-cv_results_interactions_raw_7[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "\n",
    "# create dataframe \n",
    "compare_models_q7_df = pd.DataFrame(cv_results_7).sort_values(by = \"RMSE_mean\").reset_index(drop = True)\n",
    "\n",
    "# check results \n",
    "compare_models_q7_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ab73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet CV (interactions) performance on the test data:\n",
    "print(\"ElasticNet CV (interactions) performance on the test data:\")\n",
    "score_model(pipeline_q7_interactions, \n",
    "            x_train_7, \n",
    "            x_test_7, \n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ccc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso CV (transformation + reduced variables) performance on the test data:\n",
    "print(\"Lasso CV (transformation + reduced variables) performance on the test data:\")\n",
    "score_model(pipeline_q7_transformation_reduced,\n",
    "            x_train_transformation_reduced_7,\n",
    "            x_test_transformation_reduced_7,\n",
    "            y_train_7, \n",
    "            y_test_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07aae0",
   "metadata": {},
   "source": [
    "Cross-validation results show that the full interaction-based Elastic Net model performs no better than a much simpler Lasso model with nonlinear transformations and only three features. Both achieve nearly identical cross-validated RMSE (~ 0.568) and R² (~ 0.622), despite the interaction model being substantially more complex.\n",
    "\n",
    "This is already a warning signal. When a full interaction model fails to outperform a reduced transformed model during cross - validation, it suggests that interaction terms are not introducing new signal, but instead re-expressing information already captured by simpler nonlinear transformations.\n",
    "\n",
    "This conclusion is reinforced on the test set. The interaction-based Elastic Net model generalizes worse, with a higher testing RMSE (~ 0.566) and lower R² (~ 0.638), compared to the transformed reduced Lasso model, which achieves a lower testing RMSE (~ 0.561) and higher R² (~ 0.644).\n",
    "\n",
    "Importantly, the interaction-based model increases complexity along several dimensions simultaneously:\n",
    "- Explicit interaction terms inflate the feature space\n",
    "- Elastic Net regularization adds tuning complexity\n",
    "- The model retains more predictors than the reduced alternative\n",
    "- Multicollinearity is reintroduced by construction\n",
    "\n",
    "Yet this added complexity yields neither improved cross-validated performance nor better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3fbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature weights from the model\n",
    "model = pipeline_q7_interactions.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names = x_train_7.columns.to_list()\n",
    "# coefficients\n",
    "coef = model.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_7 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_7[\"abs_weight\"] = weights_df_7[\"weight\"].abs()\n",
    "weights_df_7 = weights_df_7.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# check \n",
    "weights_df_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "elasticnetcv_q7_comparisons_interactions = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features_interactions = weights_df_7[\n",
    "    weights_df_7[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"tag\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_reduced_7_interactions = x_train_7[keep_features_interactions]\n",
    "\n",
    "x_test_reduced_10_interactions = x_train_7[keep_features_interactions]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features_interactions), 0, -1):\n",
    "    keep_pol_interactions = keep_features_interactions[:k]\n",
    "\n",
    "    x_train_reduced_7_interactions = x_train_reduced_7_interactions[keep_pol_interactions]\n",
    "\n",
    "    # pipeline to test interactions\n",
    "    pipeline = Pipeline([\n",
    "    # null values imputer \n",
    "    (\"imputer\", KNNImputer(n_neighbors= 20, \n",
    "                           weights = \"uniform\", \n",
    "                           add_indicator= False)),\n",
    "    \n",
    "    # data standardization (scaler)\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", ElasticNetCV(\n",
    "        cv = cv, \n",
    "        alphas = 100, \n",
    "        max_iter= 10000, \n",
    "        random_state = n_number\n",
    "    ))\n",
    "])\n",
    "    \n",
    "    results_reduced_7_interactions = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_7_interactions,\n",
    "        y_train_7,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    elasticnetcv_q7_comparisons_interactions.append({\n",
    "    \"model\": f\"ElasticNetCV | top-{k} by |w|\",\n",
    "    \"R2_mean\": results_reduced_7_interactions[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-results_reduced_7_interactions[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "    \n",
    "# dataframe \n",
    "drop_df_7_interactions = (\n",
    "            pd.DataFrame(elasticnetcv_q7_comparisons_interactions)\n",
    "            .sort_values(\"RMSE_mean\", ascending=False)\n",
    "            .reset_index(drop=True))\n",
    "drop_df_7_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best models dataframe\n",
    "compare_models_q7_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111dd582",
   "metadata": {},
   "source": [
    "No alternative specification provides a meaningful improvement over the transformed Lasso model with three features. All competing models either show equivalent cross-validated performance within noise or introduce additional complexity without improving generalization. As a result, this model is selected as the final specification under a balanced objective that prioritizes both predictive performance and parsimony.\n",
    "\n",
    "If the modeling objective were instead shifted toward pure performance maximization, interpretability and sparsity would no longer be binding constraints. In that case, more flexible nonlinear models such as gradient-boosted trees could be considered to assess the upper bound of achievable performance. However, under the current objective, the transformed three-feature Lasso model represents the most appropriate and defensible choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0227b",
   "metadata": {},
   "source": [
    "### Extra: Prediction Accuracy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3f11",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044b99f",
   "metadata": {},
   "source": [
    "We first evaluate whether LightGBM benefits from our tuned KNN imputation compared to simpler missing-value strategies. Specifically, we compare four preprocessing options, tuned KNN imputation, mean imputation, median imputation, and no imputation. For each option, we measure cross-validated RMSE and R² using the same 5-fold split to ensure a fair comparison.\n",
    "\n",
    "After selecting the imputation strategy that yields the best cross-validated RMSE, we fix this preprocessing choice and tune the LightGBM hyperparameters with Optuna. This two-stage approach separates preprocessing selection from model tuning and ensures that hyperparameter optimization is performed on the strongest preprocessing setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation \n",
    "cv = KFold(n_splits = 5, \n",
    "           shuffle = True, \n",
    "           random_state = n_number)\n",
    "\n",
    "# pipelines for comparison\n",
    "pipelines_q7_lgbm = {\n",
    "    \"LGBMRegressor (no imputation)\": Pipeline([\n",
    "        # scaler \n",
    "        (\"scaler\", StandardScaler()), \n",
    "\n",
    "        # model \n",
    "        (\"model\", LGBMRegressor(random_state = n_number, \n",
    "                                objective = \"regression\", \n",
    "                                metric = \"rmse\",\n",
    "                                n_jobs = 1, \n",
    "                                verbosity = -1))\n",
    "    ]), \n",
    "\n",
    "    \"LGBMRegreessor (KNNImputer)\": Pipeline([\n",
    "        # imputer\n",
    "        (\"imputer\", KNNImputer(n_neighbors=20, \n",
    "                               weights=\"uniform\", \n",
    "                               add_indicator=False)),\n",
    "\n",
    "        # scaler \n",
    "        (\"scaler\", StandardScaler()),   \n",
    "\n",
    "        # model \n",
    "        (\"model\", LGBMRegressor(random_state = n_number, \n",
    "                                objective = \"regression\", \n",
    "                                metric = \"rmse\",\n",
    "                                n_jobs = 1, \n",
    "                                verbosity = -1)) \n",
    "    ]),\n",
    "\n",
    "    \"LGBMRegressor (SimpleImputer - mean)\": Pipeline([\n",
    "        # imputer\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\", \n",
    "                                  add_indicator=False)),\n",
    "\n",
    "        # scaler \n",
    "        (\"scaler\", StandardScaler()),   \n",
    "\n",
    "        # model \n",
    "        (\"model\", LGBMRegressor(random_state = n_number, \n",
    "                                objective = \"regression\", \n",
    "                                metric = \"rmse\",\n",
    "                                n_jobs = 1, \n",
    "                                verbosity = -1)) \n",
    "    ]), \n",
    "\n",
    "    \"LGBMRegressor (SimpleImputer - median)\": Pipeline([\n",
    "        # imputer\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\", \n",
    "                                  add_indicator=False)),\n",
    "\n",
    "        # scaler \n",
    "        (\"scaler\", StandardScaler()),   \n",
    "\n",
    "        # model \n",
    "        (\"model\", LGBMRegressor(random_state = n_number, \n",
    "                                objective = \"regression\", \n",
    "                                metric = \"rmse\",\n",
    "                                n_jobs = 1, \n",
    "                                verbosity = -1)) \n",
    "    ]),\n",
    "}\n",
    "\n",
    "# compare and cross_validate the pipelines \n",
    "rows_lgbm = []\n",
    "for name, pipe in pipelines_q7_lgbm.items():\n",
    "    results_lgbm = cross_validate(\n",
    "        pipe,\n",
    "        x_train_7,\n",
    "        y_train_7,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        return_train_score=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rows_lgbm.append({\n",
    "        \"model\": name,\n",
    "        \"R2_mean\": results_lgbm[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": (-results_lgbm[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "    })\n",
    "\n",
    "# create a dataframe for comparison\n",
    "compare_lgbm_df = (\n",
    "    pd.DataFrame(rows_lgbm)\n",
    "      .sort_values(\"RMSE_mean\", \n",
    "                   ascending= True)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check the results\n",
    "compare_lgbm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe64002",
   "metadata": {},
   "source": [
    "Cross-validation results indicate that LightGBM performs best without explicit imputation. Models using no imputation achieve an RMSE of approximately ~0.557 with an R² around ~0.636, which is statistically indistinguishable from mean and median imputation variants (RMSE  ~ 0.556 - 0.557).\n",
    "\n",
    "While mean and median imputation yield marginally lower RMSE values, the differences are negligible and fall well within cross-validation noise. In contrast, KNN imputation leads to a noticeable degradation in performance, increasing RMSE to approximately ~0.563.\n",
    "\n",
    "Because LightGBM natively handles missing values by learning optimal split directions, explicit imputation does not introduce additional signal and may remove informative missingness patterns. Given the lack of meaningful performance gains and the added complexity introduced by imputation, we proceed without explicit imputation for LightGBM.\n",
    "\n",
    "This choice preserves model simplicity while maintaining optimal predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline for LightGBM \n",
    "lgbm_pipe_q7 = Pipeline([\n",
    "    # scaler \n",
    "    (\"scaler\", StandardScaler()),\n",
    "    \n",
    "    # model itself \n",
    "    (\"model\", LGBMRegressor(\n",
    "        objective = \"regression\", \n",
    "        metric = \"rmse\", \n",
    "        random_state=n_number,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# optuna objective\n",
    "def objective_lgbm(trial):\n",
    "    # parameters to tune\n",
    "    params = {\n",
    "        # core tree shape\n",
    "        \"model__num_leaves\": trial.suggest_int(\"model__num_leaves\", 15, 255),\n",
    "        \"model__max_depth\": trial.suggest_int(\"model__max_depth\", -1, 16),\n",
    "        \"model__min_child_samples\": trial.suggest_int(\"model__min_child_samples\", 5, 200),\n",
    "\n",
    "        # learning\n",
    "        \"model__n_estimators\": trial.suggest_int(\"model__n_estimators\", 200, 3000),\n",
    "        \"model__learning_rate\": trial.suggest_float(\"model__learning_rate\", 0.005, 0.2, log=True),\n",
    "\n",
    "        # subsampling\n",
    "        \"model__subsample\": trial.suggest_float(\"model__subsample\", 0.6, 1.0),\n",
    "        \"model__colsample_bytree\": trial.suggest_float(\"model__colsample_bytree\", 0.6, 1.0),\n",
    "\n",
    "        # regularization\n",
    "        \"model__reg_alpha\": trial.suggest_float(\"model__reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"model__reg_lambda\": trial.suggest_float(\"model__reg_lambda\", 1e-8, 10.0, log=True),\n",
    "\n",
    "        # split control\n",
    "        \"model__min_split_gain\": trial.suggest_float(\"model__min_split_gain\", 0.0, 0.5),\n",
    "    }\n",
    "\n",
    "    # clone pipeline and set parameters\n",
    "    pipe = clone(lgbm_pipe_q7)\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    # cross-validation score with tuned parameters\n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        x_train_7,\n",
    "        y_train_7,\n",
    "        cv=cv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return -scores.mean()\n",
    "\n",
    "# run optuna\n",
    "study_lgbm = start_study_optuna(\n",
    "    objective=objective_lgbm,\n",
    "    n_trials=50,    # number of trials \n",
    "    sampler_seed=n_number,\n",
    "    direction=\"minimize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate the model with best parameters\n",
    "# set the best parameters to the pipeline\n",
    "lgbm_pipe_q7.set_params(**study_lgbm.best_params)\n",
    "\n",
    "cv_results_raw_lgbm = cross_validate(\n",
    "    lgbm_pipe_q7,\n",
    "    x_train_7,\n",
    "    y_train_7,\n",
    "    cv=cv,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    return_train_score=False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# results\n",
    "cv_results_7.append({\n",
    "    \"model\": \"LightGBM (no imputation)\",\n",
    "    \"R2_mean\": cv_results_raw_lgbm[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": -cv_results_raw_lgbm[\"test_neg_root_mean_squared_error\"].mean()\n",
    "})\n",
    "\n",
    "# add to comparison dataframe \n",
    "compare_models_q7_df = pd.DataFrame(cv_results_7).sort_values(by = \"RMSE_mean\").reset_index(drop = True)\n",
    "\n",
    "# check results \n",
    "compare_models_q7_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c44c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of LightGBM on test data:\n",
    "print(\"LightGBM model performance on test data:\\n\")\n",
    "score_model(\n",
    "    lgbm_pipe_q7,\n",
    "    x_train_7,\n",
    "    x_test_7,\n",
    "    y_train_7,\n",
    "    y_test_7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of LassoCV (transformation + reduced variables) on test data:\n",
    "print(\"performance of LassoCV (transformation + reduced variables) on test data:\\n\")\n",
    "score_model(\n",
    "    pipeline_q7_transformation_reduced,\n",
    "    x_train_transformation_reduced_7,\n",
    "    x_test_transformation_reduced_7,\n",
    "    y_train_7,\n",
    "    y_test_7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48142c57",
   "metadata": {},
   "source": [
    "Cross-validation results show a clear and non-marginal improvement from LightGBM relative to all linear and regularized models. The LightGBM model without imputation achieves a CV RMSE of approximately ~0.554, improving substantially over the best linear specification, whose CV RMSE is around ~0.568. This gain carries over to the test set, where LightGBM attains a testing RMSE of about ~0.552 and an R² of roughly ~0.656, compared to a testing RMSE of approximately ~0.561 and an R² near ~0.644 for the transformed reduced Lasso model. Unlike earlier stages, this improvement is large enough to be considered meaningful and cannot be attributed to noise or minor functional-form adjustments.\n",
    "\n",
    "Given this clear performance gap, we conclude that LightGBM is able to capture nonlinearities and higher-order interactions that linear models fail to exploit, even after extensive feature engineering. Before proceeding to XGBoost for further performance maximization, we first examine LightGBM feature importance to understand which variables drive these gains and whether the model relies on sensible, interpretable structure rather than spurious interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapley values\n",
    "explainer = shap.TreeExplainer(lgbm_pipe_q7.named_steps[\"model\"])\n",
    "shap_values = explainer.shap_values(x_train_7)   \n",
    "\n",
    "# plot \n",
    "shap.summary_plot(shap_values, x_train_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e13c4",
   "metadata": {},
   "source": [
    "`average_difficulty` is the single strongest driver of predictions. Low difficulty values consistently increase predicted ratings, while high difficulty values decrease them. The wide spread of SHAP values shows a strong nonlinear effect, which LightGBM captures directly through splits rather than relying on engineered polynomial terms.\n",
    "\n",
    "`would_take_again` is the second most influential feature and has a strong, monotonic positive impact. Higher values reliably push predictions upward across almost all observations, confirming that student willingness to retake a professor is the most stable sentiment signal in the data.\n",
    "\n",
    "`received_a_pepper`  has a positive but smaller effect. Its SHAP values are more dispersed, indicating that the pepper signal matters, but only conditionally and less consistently than the two dominant variables.\n",
    "\n",
    "All remaining features, including engineered interactions, squared terms, counts, and demographic indicators, have SHAP values concentrated near zero. They contribute little to predictions once difficulty and willingness to retake are accounted for.\n",
    "\n",
    "In short, LightGBM’s performance gains are driven almost entirely by three variables: average difficulty, willingness to take the professor again, and pepper status. Everything else is secondary or redundant. Now we will assess Xgboost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37633caa",
   "metadata": {},
   "source": [
    "#### Xgboost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46cdb50",
   "metadata": {},
   "source": [
    "We now again are going to evaluate Whether Xgboost benefits from our tuned KNN imputation compared to simpler missing-value strategies. As with Lightgbm, we compare four preprocessing options, tuned KNN imputation, mean imputation, median imputation, and no imputation. For each option, we measure cross-validated RMSE and R² using the same 5-fold split to ensure a fair comparison.\n",
    "\n",
    "After selecting the imputation strategy that yields the best cross-validated RMSE, we fix this preprocessing choice and tune the Xgboost hyperparameters with Optuna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe921d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation \n",
    "cv = KFold(n_splits = 5, \n",
    "           shuffle = True, \n",
    "           random_state = n_number)\n",
    "\n",
    "# pipelines for comparison\n",
    "pipelines_q7_xgboost = {\n",
    "    \"XGBRegressor (no imputation)\": Pipeline([\n",
    "        # model \n",
    "        (\"model\", XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"rmse\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=n_number,\n",
    "            verbosity=0,\n",
    "            n_jobs=1\n",
    "        ))\n",
    "    ]), \n",
    "\n",
    "    \"XGBRegressor (KNNImputer)\": Pipeline([\n",
    "        # imputer\n",
    "        (\"imputer\", KNNImputer(\n",
    "            n_neighbors=20, \n",
    "            weights=\"uniform\", \n",
    "            add_indicator=False\n",
    "        )),\n",
    "\n",
    "        # model \n",
    "        (\"model\", XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"rmse\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=n_number,\n",
    "            verbosity = 0,\n",
    "            n_jobs=1\n",
    "        )) \n",
    "    ]),\n",
    "\n",
    "    \"XGBRegressor (SimpleImputer - mean)\": Pipeline([\n",
    "        # imputer\n",
    "        (\"imputer\", SimpleImputer(\n",
    "            strategy=\"mean\", \n",
    "            add_indicator=False\n",
    "        )),\n",
    "\n",
    "        # model \n",
    "        (\"model\", XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"rmse\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=n_number,\n",
    "            verbosity = 0,\n",
    "            n_jobs=1\n",
    "        )) \n",
    "    ]), \n",
    "\n",
    "    \"XGBRegressor (SimpleImputer - median)\": Pipeline([\n",
    "        # imputer\n",
    "        (\"imputer\", SimpleImputer(\n",
    "            strategy=\"median\", \n",
    "            add_indicator=False\n",
    "        )),\n",
    "\n",
    "        # model \n",
    "        (\"model\", XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"rmse\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=n_number,\n",
    "            verbosity = 0,\n",
    "            n_jobs=1\n",
    "        )) \n",
    "    ]),\n",
    "}\n",
    "\n",
    "# compare and cross_validate the pipelines \n",
    "rows_xgboost = []\n",
    "for name, pipe in pipelines_q7_xgboost.items():\n",
    "    results_xgboost = cross_validate(\n",
    "        pipe,\n",
    "        x_train_7,\n",
    "        y_train_7,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        return_train_score=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rows_xgboost.append({\n",
    "        \"model\": name,\n",
    "        \"R2_mean\": results_xgboost[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": (-results_xgboost[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "    })\n",
    "\n",
    "# create a dataframe for comparison\n",
    "compare_xgboost_df = (\n",
    "    pd.DataFrame(rows_xgboost)\n",
    "      .sort_values(\"RMSE_mean\", ascending=True)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check the results\n",
    "compare_xgboost_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b50d48",
   "metadata": {},
   "source": [
    "The best XGBoost specification without imputation achieves a CV RMSE of approximately ~0.567 and an R² of about ~0.623, which is comparable to but slightly worse than LightGBM's CV RMSE of roughly ~0.554. Mean and median imputation lead to nearly identical results, while KNN imputation substantially degrades performance, increasing RMSE to approximately 0.~579.\n",
    "\n",
    "These results suggest that, similar to LightGBM, XGBoost does not benefit from explicit imputation and may leverage missingness implicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline for XGBoost\n",
    "xgb_pipe = Pipeline([\n",
    "    # scaler \n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model itself \n",
    "    (\"model\", XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=n_number,\n",
    "        n_jobs=1,              \n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# optuna objective\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        # core tree shape\n",
    "        \"model__max_depth\": trial.suggest_int(\"model__max_depth\", 2, 12),\n",
    "        \"model__min_child_weight\": trial.suggest_float(\"model__min_child_weight\", 1.0, 50.0, log=True),\n",
    "        \"model__gamma\": trial.suggest_float(\"model__gamma\", 0.0, 5.0),\n",
    "\n",
    "        # learning\n",
    "        \"model__n_estimators\": trial.suggest_int(\"model__n_estimators\", 200, 3000),\n",
    "        \"model__learning_rate\": trial.suggest_float(\"model__learning_rate\", 0.005, 0.2, log=True),\n",
    "\n",
    "        # subsampling\n",
    "        \"model__subsample\": trial.suggest_float(\"model__subsample\", 0.6, 1.0),\n",
    "        \"model__colsample_bytree\": trial.suggest_float(\"model__colsample_bytree\", 0.6, 1.0),\n",
    "        \"model__colsample_bylevel\": trial.suggest_float(\"model__colsample_bylevel\", 0.6, 1.0),\n",
    "\n",
    "        # regularization\n",
    "        \"model__reg_alpha\": trial.suggest_float(\"model__reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"model__reg_lambda\": trial.suggest_float(\"model__reg_lambda\", 1e-8, 50.0, log=True),\n",
    "\n",
    "        # split control\n",
    "        \"model__max_delta_step\": trial.suggest_int(\"model__max_delta_step\", 0, 10),\n",
    "    }\n",
    "\n",
    "    pipe = clone(xgb_pipe)\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        x_train_7,\n",
    "        y_train_7,\n",
    "        cv=cv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return -scores.mean()\n",
    "\n",
    "# run optuna\n",
    "study_xgb = start_study_optuna(\n",
    "    objective=objective_xgb,\n",
    "    n_trials=50,\n",
    "    sampler_seed=n_number,\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "\n",
    "# after tuning, set best params and cross-validate for R2 + RMSE\n",
    "xgb_pipe.set_params(**study_xgb.best_params)\n",
    "\n",
    "# raw results \n",
    "cv_results_raw_xgb = cross_validate(\n",
    "    xgb_pipe,\n",
    "    x_train_7,\n",
    "    y_train_7,\n",
    "    cv=cv,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    return_train_score=False,\n",
    "    return_estimator=True\n",
    ")\n",
    "\n",
    "# append the results \n",
    "cv_results_7.append({\n",
    "    \"model\": \"XGBoost\",\n",
    "    \"R2_mean\": cv_results_raw_xgb[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": -cv_results_raw_xgb[\"test_neg_root_mean_squared_error\"].mean()\n",
    "})\n",
    "\n",
    "\n",
    "# show and save as pdf \n",
    "compare_models_q7_df = (pd.DataFrame(cv_results_7).sort_values(\"RMSE_mean\").reset_index(drop=True)\n",
    ")\n",
    "compare_models_q7_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee99141",
   "metadata": {},
   "source": [
    "Based on cross-validation performance alone, there is no justification to pursue XGBoost further. Although XGBoost marginally outperforms LightGBM in R², the difference in RMSE is negligible, and both models fall well within the same performance range. Given the additional tuning complexity and reduced interpretability of XGBoost, further analysis of its test-set performance or feature importance is unwarranted.\n",
    "\n",
    "Among nonlinear models, LightGBM offers the best balance of predictive accuracy and practical interpretability, achieving strong performance while still allowing partial explanation through feature importance and SHAP values. When performance is the sole objective, LightGBM is therefore the preferred model.\n",
    "\n",
    "From a perspective that balances explainability and performance, the LassoCV model with nonlinear transformations and a reduced feature set remains the most appropriate choice. It delivers competitive predictive accuracy relative to other linear specifications while maintaining transparent and stable coefficient interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22d07b",
   "metadata": {},
   "source": [
    "## Q8 - Build a regression model predicting average ratings from all tags (the ones in the rmpCapstoneTags.csv) file. Make sure to include the R2 and RMSE of this model. Which of these tags is most strongly predictive of average rating?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data for the question\n",
    "df_8 = df_filtered_final[df_tags_column_names + [\"average_rating\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac58691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the dataset\n",
    "df_8.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7199b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into x and y \n",
    "# x \n",
    "x_8 = df_8.drop(columns = \"average_rating\")\n",
    "\n",
    "# y \n",
    "y_8 = df_8[\"average_rating\"]\n",
    "\n",
    "# split the data into train and test data \n",
    "x_train_8, x_test_8, y_train_8, y_test_8 = train_test_split(x_8, y_8, test_size=0.2, random_state=n_number)\n",
    "\n",
    "# check distributions of the data \n",
    "print(f'Number of rows for train data:{y_train_8.shape[0]}\\n')\n",
    "print(f'Number of rows for test data:{y_test_8.shape[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of predictors with the target \n",
    "corr_8_y = x_train_8.corrwith(y_train_8, \n",
    "                              method = 'pearson').sort_values(ascending= False)\n",
    "\n",
    "# check \n",
    "corr_8_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41efc75",
   "metadata": {},
   "source": [
    "These correlations reveal a strong and structured relationship between course characteristics and average ratings. On the positive side, teaching quality and instructor behavior dominate. Being respected shows the highest positive correlation with ratings (0.46), followed closely by caring (0.43) and good feedback (0.42). Engaging delivery also matters. Amazing lectures (0.38), inspirational teaching (0.36), and being humorous (0.30) all show moderate positive associations. Clear grading (0.22) and accessibility (0.14) contribute positively but with smaller magnitudes, while extra credit has only a minor association (0.10).\n",
    "\n",
    "Negative correlations are even more pronounced and largely reflect course rigor and grading strictness. Tough grading exhibits the strongest relationship in absolute value (-0.67), making it the single most influential correlate in the list. Lecture-heavy formats (-0.37), lots of homework (-0.34), lots to read (-0.30), and strict attendance or assessment rules such as dont skip class (-0.28) and test heavy (-0.27) are all associated with substantially lower ratings. Smaller but still negative effects appear for so many papers (-0.23), graded by few things (-0.17), pop quizzes (-0.14), and group projects (-0.13).\n",
    "\n",
    "At the same time, these magnitudes suggest substantial overlap across predictors. Many workload-related tags cluster tightly in the range from about -0.27 to -0.37, while instructor-quality tags cluster between roughly 0.30 and 0.46. This raises concerns that individual coefficients in a multivariate model may reflect shared signal rather than distinct effects.\n",
    "\n",
    "For this reason, the next step focuses on assessing multicollinearity among these variables, to determine how much redundancy exists and how it may affect coefficient stability and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77536ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the heatmap of correlations between predictors \n",
    "# figure size \n",
    "plt.figure(figsize = (12, 10))\n",
    "\n",
    "# plot itself \n",
    "sns.heatmap(x_train_8.corr(), \n",
    "            annot = True, \n",
    "            cmap = \"coolwarm\", \n",
    "            fmt = \".2f\")\n",
    "\n",
    "# aesthetics \n",
    "# title \n",
    "plt.title(\"Correlation of Tags\", \n",
    "          fontweight = \"bold\", \n",
    "          fontsize = 18)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b63d45",
   "metadata": {},
   "source": [
    "The correlation matrix shows limited multicollinearity among the tag variables. Most pairwise correlations fall between -0.30 and 0.30, well below common concern thresholds. The strongest relationships appear where expected but remain moderate. For example, tough grader correlates negatively with caring (-0.37) and respected (-0.35), while respected correlates positively with caring (0.22) and inspirational (0.27). Workload-related tags such as lots of homework, lots to read, lecture heavy, and test heavy show correlations mostly in the -0.20 to -0.30 range.\n",
    "\n",
    "There are no clusters with correlations approaching 0.7 or higher. While several tags capture related aspects of teaching quality or course rigor, the overlap is not strong enough to raise serious multicollinearity concerns.\n",
    "\n",
    "Given this, we proceed to fit several regression models using the tag variables jointly, with coefficient estimates expected to remain stable and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fbc3e1",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70538ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed CV\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipelines for linear model comparison (no imputation)\n",
    "pipelines_linear = {\n",
    "    \"LinearRegression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LinearRegression())\n",
    "    ]),\n",
    "\n",
    "    \"RidgeCV\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", RidgeCV(\n",
    "            alphas=np.logspace(-4, 4, 50),\n",
    "            cv=cv\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"LassoCV\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LassoCV(\n",
    "            alphas=100,\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"ElasticNetCV\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", ElasticNetCV(\n",
    "            l1_ratio=[0.1, 0.5, 0.9],\n",
    "            alphas=100,\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# compare models\n",
    "rows = []\n",
    "for name, pipe in pipelines_linear.items():\n",
    "    results = cross_validate(\n",
    "        pipe,\n",
    "        x_train_8,\n",
    "        y_train_8,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"R2_mean\": results[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": (-results[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "    })\n",
    "\n",
    "compare_models_q8_df = (\n",
    "    pd.DataFrame(rows)\n",
    "      .sort_values(\"RMSE_mean\", ascending=True)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results \n",
    "compare_models_q8_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3048971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check models on test data\n",
    "for name, pipe in pipelines_linear.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(score_model(\n",
    "        pipe,\n",
    "        x_train_8,\n",
    "        x_test_8,\n",
    "        y_train_8,\n",
    "        y_test_8\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfedd1",
   "metadata": {},
   "source": [
    "We fitted several linear models using the tag variables, including ordinary least squares, Ridge, Lasso, and Elastic Net, under an identical cross-validation and preprocessing setup. All four models deliver virtually identical performance. Cross-validated R² is approximately 0.708 across all models, and RMSE is tightly clustered around 0.50. Differences in error are extremely small, on the order of 0.0003 to 0.0004, indicating that regularization does not materially affect predictive accuracy in this setting.\n",
    "\n",
    "Hold-out test results confirm the same pattern. Training and testing metrics are closely aligned for all models, with test RMSE around 0.504 and test R² around 0.713. This consistency indicates stable generalization and reinforces the conclusion that plain linear regression already captures most of the available signal.\n",
    "\n",
    "These findings are consistent with the earlier multicollinearity analysis. Since the tag variables exhibit only moderate pairwise correlations, coefficient instability is limited, and shrinkage provides little benefit in terms of prediction. Ridge and Elastic Net therefore offer no practical advantage over ordinary least squares.\n",
    "\n",
    "Despite this, Lasso remains a reasonable choice for the final linear model. Although its predictive performance is nearly indistinguishable from the other models, Lasso introduces coefficient shrinkage and implicit feature selection. This improves interpretability and provides a safeguard against overfitting if additional or noisier predictors are introduced.\n",
    "\n",
    "We therefore proceed with the Lasso model and examine its estimated coefficients. The next step is to assess whether removing weak predictors identified by Lasso improves predictive performance or simply yields a more parsimonious model with comparable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa014a7f",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed CV \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline for Lasso Regression model \n",
    "pipeline_q8 = Pipeline([\n",
    "    # scaler \n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", LassoCV(\n",
    "        cv=cv,\n",
    "        random_state=n_number,\n",
    "        alphas=100,\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# fit \n",
    "pipeline_q8.fit(x_train_8, \n",
    "                y_train_8)\n",
    "\n",
    "# extract feature weights from the model\n",
    "model = pipeline_q8.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names = x_train_8.columns.to_list()\n",
    "# coefficients\n",
    "coef = model.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_8 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_8[\"abs_weight\"] = weights_df_8[\"weight\"].abs()\n",
    "weights_df_8 = weights_df_8.sort_values(\"abs_weight\", ascending=False).reset_index(drop = True)\n",
    "\n",
    "# check \n",
    "weights_df_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5855da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights of the multiple linear regression model \n",
    "# figure size \n",
    "plt.figure(figsize = (12, 10))\n",
    "\n",
    "# bar plot \n",
    "sns.barplot(x=weights_df_8[\"weight\"],\n",
    "        y = weights_df_8[\"tag\"])\n",
    "\n",
    "# aesthetics\n",
    "# title \n",
    "plt.title(\"Lasso Regression Coefficients for Predicting Average Rating\", \n",
    "          fontweight=\"bold\", \n",
    "          fontsize=18)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel(\"Predictor\", fontsize=16)\n",
    "plt.ylabel(\"Coefficient value\", fontsize=16)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(axis=\"both\", labelsize=14)\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9275c2",
   "metadata": {},
   "source": [
    "We examined the standardized coefficients from the Lasso model to assess the relative importance of each numerical predictor. As expected based on the earlier correlation analysis, variables related to perceived teaching quality and classroom experience carry the largest coefficients, while many workload and structural indicators have coefficients close to zero once stronger predictors are included. This confirms that much of the predictive signal is concentrated in a relatively small subset of tags.\n",
    "\n",
    "Rather than treating a single cutoff as definitive, we use the coefficient magnitudes as a diagnostic tool to guide systematic feature reduction. Predictors with very small absolute coefficients contribute little marginal information after accounting for collinearity and shared variance among tags. To balance interpretability and performance, we therefore evaluate multiple coefficient thresholds and refit the model under each reduced subset. By comparing cross-validated R² and RMSE across these thresholds, we assess how aggressively the feature set can be reduced before predictive performance begins to deteriorate. This approach allows us to identify a parsimonious model that retains the dominant explanatory factors without sacrificing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "lassocv_q8_comparisons = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features_q8 = weights_df_8[\n",
    "    weights_df_8[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"tag\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_reduced_8 = x_train_8[keep_features_q8]\n",
    "\n",
    "x_test_reduced_8 = x_test_8[keep_features_q8]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features_q8), 0, -1):\n",
    "    keep_pol_q8 = keep_features_q8[:k]\n",
    "\n",
    "    x_train_reduced_8 = x_train_reduced_8[keep_pol_q8]\n",
    "\n",
    "    # pipeline to test\n",
    "    pipeline = Pipeline([\n",
    "    \n",
    "    # data standardization (scaler)\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", LassoCV(\n",
    "        cv = cv, \n",
    "        alphas = 100, \n",
    "        max_iter= 10000, \n",
    "        random_state = n_number\n",
    "    ))\n",
    "])\n",
    "    \n",
    "    results_reduced_8 = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_8, \n",
    "        y_train_8, \n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    lassocv_q8_comparisons.append({\n",
    "    \"model\": f\"LassoCV | top-{k} by |w|\",\n",
    "    \"R2_mean\": results_reduced_8[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-results_reduced_8[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "    \n",
    "# dataframe \n",
    "drop_df_8_comparisons = (\n",
    "            pd.DataFrame(lassocv_q8_comparisons)\n",
    "            .sort_values(\"RMSE_mean\", ascending=True)\n",
    "            .reset_index(drop=True))\n",
    "drop_df_8_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc4a7d",
   "metadata": {},
   "source": [
    "The results from systematic feature pruning indicate that only modest reduction in the number of predictors is possible without incurring a meaningful loss in predictive performance. Performance remains essentially stable when reducing the model from the full specification to approximately 11 - 12 predictors, with only marginal changes in R² and RMSE.\n",
    "\n",
    "However, increasing the pruning threshold beyond this point leads to clear and monotonic degradation in both metrics. When the model is restricted to 10 or fewer predictors, predictive accuracy declines steadily, and more aggressive reduction to 7 or fewer variables results in a substantial loss of explanatory power.\n",
    "\n",
    "This behavior suggests that, while many predictors have relatively small individual coefficients, they collectively contribute meaningful information to the model. Removing too many of these weak but informative predictors eliminates shared signal rather than noise.\n",
    "\n",
    "The practical limit of parsimony for the linear specification lies at approximately 11 - 12 predictors. Further feature removal materially harms model performance and is therefore not justified. Given that the full model achieves R² ~ 0.71 and RMSE ~ 0.50, pruning to 11 predictors preserves nearly all predictive power while reducing complexity in a controlled and defensible manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98be45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort features by absolute weight\n",
    "weights_df_8_sorted = weights_df_8.copy()\n",
    "weights_df_8_sorted = weights_df_8_sorted.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# keep top 11 features\n",
    "selected = weights_df_8_sorted.head(11)[\"tag\"].tolist()\n",
    "\n",
    "# reduce data\n",
    "x_train_reduced_8 = x_train_8[selected]\n",
    "x_test_reduced_8  = x_test_8[selected]\n",
    "\n",
    "# pipeline\n",
    "pipeline_q8_reduced = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LassoCV(\n",
    "        cv=cv,\n",
    "        random_state=n_number,\n",
    "        alphas=100,\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36bac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-validation to save results \n",
    "results_reduced_8 = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_8, \n",
    "        y_train_8, \n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "rows.append({\n",
    "    \"model\": f\"LassoCV | top-{len(x_train_reduced_8.columns)} by |w|\",\n",
    "    \"R2_mean\": results_reduced_8[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-results_reduced_8[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "    \n",
    "# dataframe \n",
    "compare_models_q8_df = (\n",
    "            pd.DataFrame(rows)\n",
    "            .sort_values(\"RMSE_mean\", ascending=True)\n",
    "            .reset_index(drop=True))\n",
    "compare_models_q8_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d098b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with reduced number variables on test data\n",
    "print(\"LassoCV with 11 variables:\\n\")\n",
    "score_model(pipeline_q8_reduced,\n",
    "            x_train_reduced_8, \n",
    "            x_test_reduced_8, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV on test data \n",
    "print(\"LassoCV with 20 variables:\\n\")\n",
    "score_model(pipeline_q8,\n",
    "            x_train_8, \n",
    "            x_test_8, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a791dcf",
   "metadata": {},
   "source": [
    "We evaluated how far the linear model can be reduced while preserving predictive performance by progressively pruning features based on Lasso coefficient magnitude. With all 20 numerical predictors, the LassoCV model achieves a test RMSE of approximately 0.504 and a test R² of about 0.714, which serves as the linear performance benchmark.\n",
    "\n",
    "When the model is restricted to the top 11 predictors ranked by absolute standardized coefficient, performance remains strong. On the test set, the reduced model attains an RMSE of approximately 0.516 and an R² of about 0.700. Although this represents a measurable decline relative to the full specification, the degradation is modest and does not indicate a loss of generalization stability.\n",
    "\n",
    "Importantly, the reduced model substantially improves interpretability and simplicity while maintaining competitive predictive accuracy. More aggressive pruning beyond this point leads to increasingly rapid deterioration in both RMSE and R², indicating that additional predictors, while individually weak, collectively contribute meaningful signal.\n",
    "\n",
    "In summary, while the full 20-variable Lasso model provides the strongest linear performance, the 11-variable specification represents the practical limit of parsimony. Beyond this point, further feature removal yields disproportionate losses in predictive accuracy relative to the gains in simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature weights from the model\n",
    "model = pipeline_q8_reduced.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names = x_train_reduced_8.columns.to_list()\n",
    "# coefficients\n",
    "coef = model.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_8 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_8[\"abs_weight\"] = weights_df_8[\"weight\"].abs()\n",
    "weights_df_8 = weights_df_8.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# cleanup\n",
    "weights_df_8 = weights_df_8.drop(columns=\"abs_weight\").reset_index(drop=True)\n",
    "# check \n",
    "weights_df_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18627dd0",
   "metadata": {},
   "source": [
    "These coefficients represent standardized effects from the reduced Lasso model and therefore allow direct comparison of relative importance across predictors. As expected, variables capturing teaching quality and instructor - student interaction dominate the model.\n",
    "\n",
    "The strongest positive predictors of average rating are `good_feedback`, `amazing_lectures`, `respected`, and `caring`, indicating that clarity, engagement, and perceived respect are the primary drivers of student evaluations. `Hilarious` also exhibits a sizable positive coefficient, suggesting that an engaging and enjoyable classroom atmosphere contributes meaningfully to overall ratings.\n",
    "\n",
    "In contrast, `tough_grader` is the only predictor with a large negative coefficient, reflecting the well-known trade-off between grading strictness and student satisfaction. Structural and behavioral indicators such as `clear_grading`, `extra_credit`, `participation_matters`, `inspirational`, and accessible contribute positively but with smaller magnitudes, indicating secondary effects once core teaching quality is accounted for.\n",
    "\n",
    "Overall, the reduced model aligns closely with domain intuition. Student ratings are primarily driven by perceived teaching quality, respect, clarity, and engagement, while grading strictness acts as the main negative factor. At the same time, the model remains parsimonious, relying on a compact and interpretable set of predictors while retaining most of the predictive performance of the full specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22488fe9",
   "metadata": {},
   "source": [
    "### Extra: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17ecec",
   "metadata": {},
   "source": [
    "#### Index Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea24838",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_8.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eeb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of predictors with the target \n",
    "corr_8_y = x_train_8.corrwith(y_train_8, \n",
    "                              method = 'pearson').sort_values(ascending= False)\n",
    "\n",
    "# check \n",
    "corr_8_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teaching quality group\n",
    "teaching_quality = [\n",
    "    \"good_feedback\", \"respected\", \"inspirational\", \"hilarious\", \n",
    "    \"amazing_lectures\", \"caring\", \"accessible\", \"clear_grading\", \n",
    "    \"accessible\"]\n",
    "\n",
    "# structure group\n",
    "structure_positive = [\"extra_credit\", \n",
    "             \"participation_matters\"]\n",
    "\n",
    "structure_negative = [\"group_projects\", \n",
    "                      \"pop_quizzes\", \n",
    "                      \"graded_by_few_things\", \n",
    "                      \"dont_skip_class_or_you_will_not_pass\"\n",
    "                      ]\n",
    "\n",
    "# workload group\n",
    "workload = [\"so_many_papers\", \n",
    "            \"test_heavy\",\n",
    "            \"lots_to_read\", \n",
    "            \"lots_of_homework\", \n",
    "            \"lecture_heavy\", \n",
    "            \"tough_grader\"]\n",
    "\n",
    "\n",
    "\n",
    "# create new index features \n",
    "# train data \n",
    "x_train_8[\"teaching_quality_index\"] = x_train_8[teaching_quality].mean(axis=1)\n",
    "x_train_8[\"workload_index\"] = x_train_8[workload].mean(axis=1)\n",
    "x_train_8[\"structure_index_positive\"] = x_train_8[structure_positive].mean(axis=1)\n",
    "x_train_8[\"structure_index_negative\"] = x_train_8[structure_negative].mean(axis=1)\n",
    "\n",
    "# test data \n",
    "x_test_8[\"teaching_quality_index\"] = x_test_8[teaching_quality].mean(axis=1)\n",
    "x_test_8[\"workload_index\"] = x_test_8[workload].mean(axis=1)\n",
    "x_test_8[\"structure_index_positive\"] = x_test_8[structure_positive].mean(axis=1)\n",
    "x_test_8[\"structure_index_negative\"] = x_test_8[structure_negative].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c53467",
   "metadata": {},
   "source": [
    "We introduced index features to capture latent dimensions underlying the individual tag proportions. Many of the original tags are conceptually related, as they describe overlapping aspects of the student experience. Modeling them separately forces the regression to distribute weight across multiple noisy proxies for the same underlying construct, which can reduce stability and interpretability.\n",
    "\n",
    "To address this, we grouped tags into three conceptually coherent indices. The `teaching_quality_index` aggregates indicators related to instructional clarity, engagement, and instructor support. The `workload_index` summarizes perceived course rigor and assessment intensity. The `structure_index` captures organizational and grading-related aspects of the course. Each index is computed as the mean proportion of its constituent tags, preserving scale while reducing dimensionality.\n",
    "\n",
    "These indices serve two purposes. First, they mitigate multicollinearity by collapsing correlated predictors into a single, more stable signal. Second, they improve interpretability by allowing the model to reason in terms of high-level educational constructs rather than individual tags. This approach provides a structured summary of the tag space while retaining the core information needed for prediction.\n",
    "\n",
    "Now let's look at correlations between predictors and target, and between predictors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations of predictors and target variables \n",
    "cor_8_y_feature = (\n",
    "    x_train_8\n",
    "    .corrwith(y_train_8)\n",
    "    .sort_values(key=lambda x: x.abs(), ascending=False)\n",
    ")\n",
    "\n",
    "cor_8_y_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb3fdc",
   "metadata": {},
   "source": [
    "The correlation results summarize the marginal relationships between each predictor and the target variable, average rating. The strongest positive association is observed for the `teaching_quality_index`, indicating that instructional quality, engagement, and instructor support are the primary drivers of student evaluations. In contrast, the `workload_index` exhibits a strong negative correlation, suggesting that heavier perceived workload and assessment intensity are associated with lower ratings.\n",
    "\n",
    "Among individual tags, variables such as `respected`, `caring`, `good_feedback`, `amazing_lectures`, and `inspirational` show moderate positive correlations, reinforcing the importance of clarity, engagement, and positive instructor - student interaction. Conversely, `tough_grader`, `lecture_heavy`, `lots_of_homework`, `test_heavy`, `lots_to_read`, and `so_many_papers` display moderate negative correlations, reflecting the consistent trade-off between academic rigor and student satisfaction.\n",
    "\n",
    "Structure-related measures exhibit weaker marginal relationships. While `clear_grading` and `accessible` are positively correlated with ratings, their magnitudes are noticeably smaller than those of teaching quality indicators. Several structural and policy-related tags, including `participation_matters`, `group_projects`, `extra_credit`, and `pop_quizzes`, show correlations close to zero, indicating limited standalone association with overall ratings when considered individually.\n",
    "\n",
    "Overall, the correlation analysis suggests that most explanatory power resides in teaching quality and workload intensity, while structural and assessment mechanics play a secondary role at the marginal level. Since many of these predictors are conceptually related and may capture overlapping information, the next step is to evaluate multicollinearity among predictors before proceeding to multivariate modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations between predictors \n",
    "corr_8_x = x_train_8.corr()\n",
    "\n",
    "# plot \n",
    "plt.figure(figsize=(12, 10)) # Adjust the figure size as needed\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(corr_8_x, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# aesthetics \n",
    "# title \n",
    "plt.title('Correlation Matrix of Numerical Features', fontweight = \"bold\", fontsize = 18)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d790f0",
   "metadata": {},
   "source": [
    "The correlation matrix shows clear and non-trivial multicollinearity, driven primarily by the constructed indices and their underlying tag variables.\n",
    "\n",
    "The strongest dependencies appear between `teaching_quality_index` and several individual teaching-related tags. In particular, `teaching_quality_index` is highly correlated with `good_feedback` (≈ 0.48), `respected` (≈ 0.55), `caring` (≈ 0.56), `amazing_lectures` (≈ 0.40), and `hilarious` (≈ 0.38). These magnitudes indicate that the index is capturing much of the same signal as these individual variables, which is expected given its construction as an aggregate measure.\n",
    "\n",
    "Similarly, `workload_index` exhibits strong correlations with workload-related tags, including `tough_grader` (≈ 0.73), `lots_of_homework` (≈ 0.51), `lots_to_read` (≈ 0.52), `test_heavy` (≈ 0.32), and `lecture_heavy` (≈ 0.45). This confirms that workload_index encodes a shared dimension of assessment intensity and course burden already present in the raw features.\n",
    "\n",
    "Importantly, the two indices themselves are strongly negatively correlated (~ −0.82), indicating that courses perceived as high quality tend to be perceived as lower workload, and vice versa. Including both indices alongside their component tags therefore introduces overlapping predictors that encode highly similar information.\n",
    "\n",
    "Outside of the indices, correlations among individual tags are generally moderate rather than extreme, typically in the 0.2-0.4 range in absolute value. This suggests partial redundancy but not exact duplication, meaning these variables contribute related but non-identical information.\n",
    "\n",
    "The structure-related variables show weaker and more diffuse correlations. While `structure_index_positive` and `structure_index_negative` are correlated with each other and with a small subset of structural tags, their associations with teaching quality and workload variables are noticeably smaller. This supports earlier findings that structural characteristics play a secondary role relative to teaching quality and workload.\n",
    "\n",
    "Overall, the correlation structure indicates that standard linear regression would suffer from unstable coefficient estimates due to groups of predictors encoding overlapping dimensions, particularly when indices and raw tags are included together. This motivates the use of regularized linear models. Lasso alone risks arbitrarily selecting a single variable from a correlated group, while Ridge retains all predictors without sparsity. Given the observed correlation patterns, `ElasticNet` is well-suited, as it allows correlated predictors to be shrunk together while still performing feature selection, leading to more stable and interpretable models under this dependency structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7da28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "pipeline_q8_features = Pipeline([\n",
    "        # scaler\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # model\n",
    "        (\"model\", ElasticNetCV(\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            n_alphas=100,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# cross_validation\n",
    "cv_q8_raw_features = cross_validate(\n",
    "        pipeline_q8_features,\n",
    "        x_train_8,\n",
    "        y_train_8,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "# append to the results \n",
    "rows.append({\n",
    "    \"model\": \"ElaticNetCV | eng.features\",\n",
    "        \"R2_mean\": cv_q8_raw_features[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": -cv_q8_raw_features[\"test_neg_root_mean_squared_error\"].mean()\n",
    "    })\n",
    "\n",
    "# create and show dataframe \n",
    "compare_models_q8_df = ( \n",
    "pd.DataFrame(rows)\n",
    ".reset_index(drop = True)\n",
    ".sort_values(by = \"RMSE_mean\") )\n",
    "\n",
    "# check the data \n",
    "compare_models_q8_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with top 11 features performance on the test data:\n",
    "print(\"LassoCV with top 11 features:\\n\")\n",
    "score_model(pipeline_q8_reduced,\n",
    "            x_train_reduced_8, \n",
    "            x_test_reduced_8, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNetCV with engineered features on test data:\n",
    "print(\"ElasticNetCV with engineered variables:\\n\")\n",
    "score_model(pipeline_q8_features,\n",
    "            x_train_8, \n",
    "            x_test_8, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4dc30d",
   "metadata": {},
   "source": [
    "After introducing the engineered index features, we observe a small but consistent performance improvement for models that retain the full predictor set. Under 5-fold cross-validation, ElasticNetCV with engineered features achieves an RMSE of approximately 0.500 and an R² of about 0.708, which is marginally better than the reduced Lasso specification and comparable to unregularized Linear Regression and Ridge. On the held-out test set, the same model attains a test RMSE of approximately 0.504 and a test R² of about 0.714, confirming that the gain is not limited to cross-validation noise.\n",
    "\n",
    "By contrast, the Lasso model pruned to 11 predictors exhibits noticeably weaker generalization, with a test RMSE of approximately 0.516 and a test R² of about 0.700. This indicates that while aggressive feature reduction improves simplicity, it comes at a measurable cost in predictive accuracy.\n",
    "\n",
    "However, the improved ElasticNet model is not sparse. It retains all original tag variables in addition to the engineered indices, which conflicts with the objective of parsimony. Moreover, ElasticNet is computationally more expensive than Lasso due to simultaneous tuning of L1 and L2 penalties. As a result, the observed performance gains appear to come from feature enrichment rather than efficient feature selection.\n",
    "\n",
    "Therefore, while engineered indices clearly add useful signal when combined with the full feature set, the next step is to inspect coefficient magnitudes and prune weak predictors. The goal is to determine whether most of the performance improvement can be preserved under a reduced specification. Given the strong correlations between indices and their constituent tags, we will also re-evaluate multicollinearity in any reduced subset to decide whether Lasso remains sufficient or whether ElasticNet is still required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = pipeline_q8_features.named_steps[\"model\"]\n",
    "\n",
    "# extract feature names and coefficients\n",
    "feature_names = x_train_8.columns\n",
    "coef = model.coef_\n",
    "\n",
    "\n",
    "# create the dataframe\n",
    "weights_df_8 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort \n",
    "weights_df_8[\"abs_weight\"] = weights_df_8[\"weight\"].abs()\n",
    "\n",
    "weights_df_8 = (\n",
    "    weights_df_8\n",
    "    .sort_values(\"abs_weight\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results\n",
    "weights_df_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f09a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "elasticnet_q8_comparisons = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features_q8_index = weights_df_8[\n",
    "    weights_df_8[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"tag\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_reduced_8_index = x_train_8[keep_features_q8_index]\n",
    "\n",
    "x_test_reduced_8_index = x_test_8[keep_features_q8_index]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features_q8_index), 0, -1):\n",
    "    keep_pol_q8_index = keep_features_q8_index[:k]\n",
    "\n",
    "    x_train_reduced_8_index = x_train_reduced_8_index[keep_pol_q8_index]\n",
    "\n",
    "    # pipeline to test\n",
    "    pipeline = Pipeline([\n",
    "    \n",
    "    # data standardization (scaler)\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", ElasticNetCV(\n",
    "        cv = cv, \n",
    "        alphas = 100, \n",
    "        max_iter= 10000, \n",
    "        random_state = n_number\n",
    "    ))\n",
    "])\n",
    "    \n",
    "    results_reduced_8_index = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_8_index, \n",
    "        y_train_8, \n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    elasticnet_q8_comparisons.append({\n",
    "    \"model\": f\"ElasticNetCV | top-{k} indexes\",\n",
    "    \"R2_mean\": results_reduced_8_index[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-results_reduced_8_index[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "    \n",
    "# dataframe \n",
    "drop_df_8_comparisons_index = (\n",
    "            pd.DataFrame(elasticnet_q8_comparisons)\n",
    "            .sort_values(\"RMSE_mean\", ascending=True)\n",
    "            .reset_index(drop=True))\n",
    "drop_df_8_comparisons_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0348fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best results dataframe\n",
    "compare_models_q8_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd34959",
   "metadata": {},
   "source": [
    "Progressively pruning the engineered ElasticNet model shows that modest sparsification is possible, but only up to a point. Reducing the model to 8 or 7 predictors yields cross-validated RMSE values of approximately 0.506-0.507 and R² around 0.699, which is noticeably worse than the full engineered ElasticNet model (RMSE ~ 0.500, R² ~ 0.708), but still materially better than the aggressively pruned Lasso model with 11 features (RMSE ~ 0.510, R² ~ 0.696).\n",
    "\n",
    "Once the model is reduced below 7 predictors, performance deteriorates rapidly. RMSE increases beyond 0.51 and R² falls below 0.69, indicating that further pruning removes variables that collectively carry meaningful signal. This pattern suggests that the engineered indices and a small subset of original tags jointly encode complementary information, and overly aggressive sparsification breaks that balance.\n",
    "\n",
    "From a practical standpoint, the 7-feature ElasticNet model represents the maximum reduction achievable without severe loss of predictive accuracy. However, it still underperforms the full engineered model by a non-trivial margin and remains worse than simpler linear baselines such as Ridge and unregularized Linear Regression in cross-validation.\n",
    "\n",
    "Now we need to check and verify the multicollinearity of our selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort features by absolute weight\n",
    "weights_df_8_sorted_index = weights_df_8.copy()\n",
    "weights_df_8_sorted_index = weights_df_8_sorted_index.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# keep top 7 features\n",
    "selected = weights_df_8_sorted_index.head(7)[\"tag\"].tolist()\n",
    "\n",
    "# reduce data\n",
    "x_train_reduced_8_index = x_train_8[selected]\n",
    "x_test_reduced_8_index  = x_test_8[selected]\n",
    "\n",
    "# correlations between predictors \n",
    "corr_8_x_index = x_train_reduced_8_index.corr()\n",
    "\n",
    "# plot \n",
    "plt.figure(figsize=(12, 10)) # Adjust the figure size as needed\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(corr_8_x_index, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# aesthetics \n",
    "# title \n",
    "plt.title('Correlation Matrix of Numerical Features', fontweight = \"bold\", fontsize = 18)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e4da7",
   "metadata": {},
   "source": [
    "The correlation matrix for the reduced feature set shows no severe multicollinearity. Using a conservative threshold of |ρ| ≥ 0.70 to indicate problematic dependence, none of the retained predictors exceed this level. The strongest association is between `teaching_quality_index` and `tough_grader` (~ −0.62), which is substantial but still below levels typically associated with unstable coefficient estimates. Other correlations involving `teaching_quality_index`, such as with `good_feedback` (~ 0.48), `amazing_lectures` (~ 0.40), and `accessible` (~ 0.40), are moderate and expected given the conceptual overlap between these variables.\n",
    "\n",
    "Correlations among the remaining predictors are generally weak to moderate, with most falling below |ρ| ≈ 0.30. In particular, `structure_index_positive` shows low correlations with all other variables, and the remaining tag variables do not exhibit strong pairwise dependence.\n",
    "\n",
    "Given this correlation structure, multicollinearity is unlikely to materially distort coefficient estimates in a linear model. As a result, `ElasticNet` regularization is no longer necessary for stability. We can safely fit a LassoCV model on this reduced feature set to retain sparsity and interpretability without sacrificing numerical stability or generalization performance.\n",
    "\n",
    "This simplifies the modeling pipeline while preserving the key predictive signal, making Lasso the preferred regularized linear specification for the reduced model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline_q8_features_reduced_index = Pipeline([\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()),\n",
    "\n",
    "    # model \n",
    "    (\"model\", LassoCV(\n",
    "        cv=cv, \n",
    "        random_state=n_number, \n",
    "        alphas = 100, \n",
    "        max_iter=10000))\n",
    "    ])\n",
    "\n",
    "# cross_validation\n",
    "cv_raw_features_8 = cross_validate(\n",
    "        pipeline_q8_features_reduced_index,\n",
    "        x_train_reduced_8_index,\n",
    "        y_train_8,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "# append to the results \n",
    "rows.append({\n",
    "        \"model\": f\"LassoCV | top-{len(x_train_reduced_8_index.columns)} by |w\",\n",
    "        \"R2_mean\": cv_raw_features_8[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": -cv_raw_features_8[\"test_neg_root_mean_squared_error\"].mean()\n",
    "    })\n",
    "\n",
    "# return results as dataframe \n",
    "compare_models_q8_df = pd.DataFrame(rows).reset_index(drop = True).sort_values(by = \"RMSE_mean\")\n",
    "\n",
    "# check \n",
    "compare_models_q8_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2484e",
   "metadata": {},
   "source": [
    "Compared to the previously selected sparse linear model, the LassoCV with top-11 features, the engineered-feature Lasso allows for a modest additional reduction in dimensionality without materially affecting predictive performance. The earlier `11-feature model` achieved a cross-validated RMSE of approximately 0.509 and an R² of about 0.696, while the `engineered Lasso with 7 features` retained predictors achieves a CV RMSE of approximately 0.507 and an R² close to `0.699`.\n",
    "\n",
    "The improvement is small but consistent, and the difference in RMSE relative to the earlier sparse model is within cross-validation noise. Importantly, this result indicates that feature engineering enables a better `sparsity–performance trade-off`: we are able to remove several additional predictors while maintaining essentially the same explanatory power.\n",
    "\n",
    "However, the table also shows that further reduction beyond this point leads to clear degradation. Models restricted to fewer than seven predictors exhibit noticeable drops in R² and increases in RMSE, indicating that the remaining variables no longer capture enough independent signal.\n",
    "\n",
    "From a cross-validation perspective, this establishes the engineered-feature Lasso with roughly `7 predictors as the best sparse linear specification`. It improves interpretability relative to the earlier `11-feature model` while preserving nearly identical predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with interactions on test data \n",
    "print(\"LassoCV with top 7 engineered features:\\n\")\n",
    "score_model(pipeline_q8_features_reduced_index,\n",
    "            x_train_reduced_8_index, \n",
    "            x_test_reduced_8_index, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f732f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with 11 variables \n",
    "print(\"LassoCV with top 11 variables:\\n\")\n",
    "score_model(pipeline_q8_reduced,\n",
    "            x_train_reduced_8, \n",
    "            x_test_reduced_8, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdaea54",
   "metadata": {},
   "source": [
    "On held-out test data, the `LassoCV` model with 7 engineered features slightly outperforms the earlier `11-feature Lasso model`. The 7-feature specification achieves a test RMSE of approximately 0.514 and a test R² of about 0.702, compared to a test RMSE of approximately 0.516 and a test R² of about 0.700 for the 11-feature model.\n",
    "\n",
    "Although the absolute performance difference is small, it is directionally consistent with cross-validation results and does not reflect overfitting. The train-test gaps remain comparable across both models, indicating stable generalization.\n",
    "\n",
    "Crucially, this improvement is achieved with four fewer predictors, meaning the engineered-feature Lasso delivers a better `sparsity–performance trade-off`. The engineered indices allow the model to retain core signal while eliminating redundant variables that were previously required in the raw-feature formulation.\n",
    "\n",
    "In summary, on test data as well as in cross-validation, the 7-feature engineered Lasso dominates the 11-feature raw Lasso, providing equal or slightly better predictive accuracy with a substantially simpler and more interpretable model. This makes the 7-feature engineered Lasso the preferred sparse linear specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e697cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = pipeline_q8_features_reduced_index.named_steps[\"model\"]\n",
    "\n",
    "# extract feature names and coefficients\n",
    "feature_names = x_train_reduced_8_index.columns\n",
    "coef = model.coef_\n",
    "\n",
    "\n",
    "# create the dataframe\n",
    "weights_df_8 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort \n",
    "weights_df_8[\"abs_weight\"] = weights_df_8[\"weight\"].abs()\n",
    "\n",
    "weights_df_8 = (\n",
    "    weights_df_8\n",
    "    .sort_values(\"abs_weight\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results\n",
    "weights_df_8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee62437",
   "metadata": {},
   "source": [
    "These coefficients represent standardized effects from the final LassoCV model with engineered features and therefore allow direct comparison of relative importance across predictors.\n",
    "\n",
    "The dominant predictor is `teaching_quality_index`, which has by far the largest positive coefficient. This indicates that overall instructional quality, capturing clarity, engagement, respect, and feedback, is the primary driver of average professor ratings. Its magnitude substantially exceeds that of all other variables, confirming that teaching quality explains most of the systematic variation in ratings.\n",
    "\n",
    "The strongest negative effect comes from `tough_grader`, reflecting a clear trade-off between grading strictness and student satisfaction. Even after accounting for teaching quality, stricter grading remains associated with lower average ratings.\n",
    "\n",
    "`structure_index_positive` enters with a moderate positive coefficient, suggesting that well-organized courses with clear structure and policies contribute positively to evaluations, though their impact is secondary to instructional quality itself.\n",
    "\n",
    "Several individual tags retain smaller but non-negligible effects. `accessible` has a negative coefficient, indicating that once teaching quality and structure are controlled for, accessibility-related perceptions add limited incremental signal and may partially overlap with broader quality measures. `dont_skip_class_or_you_will_not_pass`, `amazing_lectures`, and `good_feedback` have modest positive coefficients, suggesting localized contributions beyond what is already captured by the teaching quality index.\n",
    "\n",
    "Overall, this final model shows that student ratings are primarily driven by broad teaching quality, with grading strictness acting as the main negative factor and course structure and select engagement indicators providing additional but smaller effects. The model achieves strong predictive performance while remaining highly parsimonious, relying on a compact and interpretable set of predictors that align closely with domain intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4e880",
   "metadata": {},
   "source": [
    "#### Contrast Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at correlation again\n",
    "corr_8_x_y = (x_train_8.corrwith(y_train_8)\n",
    "              .sort_values(key=lambda x: x.abs(), ascending=False))\n",
    "corr_8_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de12eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast indexes between teaching quality and workload indexes \n",
    "x_train_8[\"quality_minus_workload\"] = (\n",
    "    x_train_8[\"teaching_quality_index\"] - x_train_8[\"workload_index\"]\n",
    ")\n",
    "x_test_8[\"quality_minus_workload\"] = (\n",
    "    x_test_8[\"teaching_quality_index\"] - x_test_8[\"workload_index\"]\n",
    ")\n",
    "\n",
    "# contrast indexes between teaching quality and structure indexes\n",
    "x_train_8[\"quality_minus_structure_negative\"] = (\n",
    "    x_train_8[\"teaching_quality_index\"] - x_train_8[\"structure_index_negative\"]\n",
    ")\n",
    "x_test_8[\"quality_minus_structure_negative\"] = (\n",
    "    x_test_8[\"teaching_quality_index\"] - x_test_8[\"structure_index_negative\"]\n",
    ")\n",
    "\n",
    "# contrast indexes between workload and positive structure index \n",
    "x_train_8[\"workload_minus_structure_positive\"] = (\n",
    "    x_train_8[\"workload_index\"] - x_train_8[\"structure_index_positive\"]\n",
    ")\n",
    "\n",
    "x_test_8[\"workload_minus_structure_positive\"] = (\n",
    "    x_test_8[\"workload_index\"] - x_test_8[\"structure_index_positive\"]\n",
    ")\n",
    "\n",
    "# contrast indexes between structure indexes \n",
    "x_train_8[\"structure_index_difference\"] = (\n",
    "    x_train_8[\"structure_index_positive\"] - x_train_8[\"structure_index_negative\"]\n",
    ")\n",
    "\n",
    "x_test_8[\"structure_index_difference\"] = (\n",
    "    x_test_8[\"structure_index_positive\"] - x_test_8[\"structure_index_negative\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fffdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations of predictors and target variables \n",
    "cor_8_y_features_contrast = (\n",
    "    x_train_8\n",
    "    .corrwith(y_train_8)\n",
    "    .sort_values(key=lambda x: x.abs(), ascending=False)\n",
    ")\n",
    "\n",
    "cor_8_y_features_contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596d287",
   "metadata": {},
   "source": [
    "The newly constructed contrast features capture relative trade-offs that are central to how students evaluate courses. Rather than measuring absolute levels of quality or workload, these variables encode balance, which is more aligned with how ratings are formed in practice.\n",
    "\n",
    "The strongest correlation is observed for `quality_minus_workload` (~ 0.81). This indicates that ratings increase most when indicators of teaching quality substantially outweigh indicators of workload intensity. In other words, even demanding courses receive high ratings when strong instructional quality compensates for the workload, while heavy workload without sufficient teaching quality is penalized. This pattern directly reflects the intuitive evaluation process students use and explains why this contrast dominates both raw indices.\n",
    "\n",
    "The contrast `quality_minus_structure_negative` (~ 0.67) captures a related mechanism. Poor structure harms ratings primarily when it is not compensated by strong teaching. When instructional quality is high, negative structural elements become less salient in student evaluations.\n",
    "\n",
    "Other contrast variables, such as `workload_minus_structure_positive` and `structure_index_difference`, show more moderate correlations, indicating secondary trade-offs. These features still contribute information, but their effects are weaker than those involving teaching quality versus workload or grading rigor.\n",
    "\n",
    "Notably, the contrast features exhibit stronger correlations than many individual tags and even the raw indices themselves, confirming that relative comparisons encode more signal than absolute measures. This is expected, as students rarely evaluate workload, grading, or structure in isolation. Instead, they judge whether these aspects feel justified by instructional quality.\n",
    "\n",
    "Overall, the correlation structure validates the feature engineering strategy. The contrast indices behave exactly as intended, amplifying meaningful trade-offs while compressing overlapping information from multiple tags. This makes them particularly effective for sparsification, as they replace pairs of correlated variables with a single, more informative predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eda637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline\n",
    "pipeline_q8_features_contrast = Pipeline([\n",
    "        # scaler\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # model\n",
    "        (\"model\", ElasticNetCV(\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            alphas = 100,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# cross_validation\n",
    "cv_q8_raw_features_contrast = cross_validate(\n",
    "        pipeline_q8_features_contrast,\n",
    "        x_train_8,\n",
    "        y_train_8,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "# append to the results \n",
    "rows.append({\n",
    "    \"model\": \"ElaticNetCV | eng.contrast indexes\",\n",
    "    \"n_features\": len(x_train_8.columns),\n",
    "        \"R2_mean\": cv_q8_raw_features_contrast[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": -cv_q8_raw_features_contrast[\"test_neg_root_mean_squared_error\"].mean()\n",
    "    })\n",
    "\n",
    "# create and show dataframe \n",
    "compare_models_q8_df = ( \n",
    "pd.DataFrame(rows)\n",
    ".reset_index(drop = True)\n",
    ".sort_values(by = \"RMSE_mean\") )\n",
    "\n",
    "# check the data \n",
    "compare_models_q8_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc745c",
   "metadata": {},
   "source": [
    "Looking at cross-validation results, the models that incorporate engineered indices and contrast features achieve nearly identical performance to the strongest linear baselines. `The ElasticNetCV` model with contrast indices attains an RMSE of approximately 0.500 and an R² around 0.708, which is statistically indistinguishable from Linear Regression, RidgeCV, and the earlier ElasticNet and Lasso specifications. The difference in RMSE across these top models is on the order of 10⁻⁴, well within cross-validation noise.\n",
    "\n",
    "Importantly, while the `contrast-index ElasticNet` performs as expected and slightly compresses signal relative to using separate quality, workload, and structure indices, it does not provide a meaningful performance advantage. Moreover, this specification still relies on a large feature set, retaining many predictors with small coefficients. As a result, it does not satisfy the objective of sparsity and interpretability, despite its strong predictive performance.\n",
    "\n",
    "In contrast, the `sparse Lasso models` demonstrate the expected trade-off. Reducing the model to 11 or 7 predictors leads to higher RMSE values (~ 0.510 - 507), indicating a modest but real loss in explanatory power. However, these models offer substantial gains in simplicity and interpretability, which are often more valuable in applied settings.\n",
    "\n",
    "Overall, the cross-validation results indicate that feature engineering and contrast construction can successfully preserve performance, but they do not materially raise the linear performance ceiling. The key benefit of these engineered features lies in enabling feature compression, not in improving predictive accuracy beyond existing linear baselines. This motivates using the engineered features primarily as a tool for further sparsification, rather than as an end in themselves, and sets up the next step of selecting a final sparse specification that balances performance, interpretability, and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aadfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check contrast model on test data\n",
    "print(\"ElasticNetCV with contrast index:\\n\")\n",
    "score_model(pipeline_q8_features_contrast, \n",
    "            x_train_8, \n",
    "            x_test_8, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare our contrast model to top 9 features engineered model \n",
    "print(\"LassoCV with top 7 engineered features:\\n\")\n",
    "score_model(pipeline_q8_features_reduced_index, \n",
    "            x_train_reduced_8_index, \n",
    "            x_test_reduced_8_index, \n",
    "            y_train_8, \n",
    "            y_test_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e22f7",
   "metadata": {},
   "source": [
    "On the held-out test set, the ElasticNetCV model with the contrast index also performs marginally better than the sparse Lasso alternative. It achieves a lower test RMSE and a slightly higher test R² than the LassoCV model with the top 7 engineered features, indicating improved generalization rather than overfitting. This consistency between cross-validation and test performance reinforces the value of the contrast feature as a meaningful representation of the underlying signal.\n",
    "\n",
    "At the same time, this ElasticNet specification still relies on the full predictor set and therefore does not satisfy our goal of sparsity and parsimony. To address this, the next step is to examine the standardized coefficients of the ElasticNet model and prune weak predictors. By removing variables with negligible weights, we aim to recover a more interpretable and computationally efficient model while retaining most of the predictive gains introduced by the contrast index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = pipeline_q8_features_contrast.named_steps[\"model\"]\n",
    "\n",
    "# extract feature names and coefficients\n",
    "feature_names = x_train_8.columns\n",
    "coef = model.coef_\n",
    "\n",
    "\n",
    "# create the dataframe\n",
    "weights_df_8 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort \n",
    "weights_df_8[\"abs_weight\"] = weights_df_8[\"weight\"].abs()\n",
    "\n",
    "weights_df_8 = (\n",
    "    weights_df_8\n",
    "    .sort_values(\"abs_weight\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results\n",
    "weights_df_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab68dc8",
   "metadata": {},
   "source": [
    "Inspection of the final `ElasticNet coefficient estimates` shows clear convergence toward the same small subset of predictors identified in the previous feature-engineering step. The largest standardized coefficient remains `teaching_quality_index`, which dominates all other variables and captures the primary source of variation in ratings. The next strongest effects consistently correspond to `tough_grader`, `structure_index_positive`, and core teaching quality tags such as `amazing_lectures`, `good_feedback`, `accessible`, and `dont_skip_class_or_you_will_not_pass`. These variables form the same core group that defined the earlier `7-feature engineered Lasso model`.\n",
    "\n",
    "All remaining predictors either have substantially smaller coefficients or are shrunk exactly to zero. Notably, the engineered contrast variables that were introduced to compress signal, such as `quality_minus_workload` and `quality_minus_structure_negative`, are fully eliminated once the model is regularized. This indicates that their informational content is already absorbed by the retained indices and tags and that they do not provide additional incremental signal in the multivariate setting.\n",
    "\n",
    "This repeated selection of the same small group of variables across different specifications and regularization schemes provides strong evidence that the model has stabilized. Further rounds of feature engineering or pruning are unlikely to yield meaningful gains in either performance or interpretability. At this point, additional complexity would primarily introduce noise rather than signal.\n",
    "\n",
    "We therefore stop here.\n",
    "\n",
    "The final sparse linear specification is the `7-feature engineered Lasso model`, which represents the best balance between predictive performance, interpretability, and computational efficiency. It preserves nearly all of the explanatory power of the full linear models while relying on a compact and stable set of predictors.\n",
    "\n",
    "If the modeling objective were instead shifted from this balance toward pure `performance maximization`, then linear sparsity would no longer be the primary concern. In that case, more flexible nonlinear models such as gradient-boosted trees would be appropriate to evaluate, as they can automatically capture higher-order interactions and nonlinear effects that linear models intentionally ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c84b50",
   "metadata": {},
   "source": [
    "### Extra: Prediction Accuracy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52ef53",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa91764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline for LightGBM\n",
    "lgbm_pipe_q8 = Pipeline([\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        metric=\"rmse\",\n",
    "        random_state=n_number,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# optuna objective\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        # tree complexity\n",
    "        \"model__num_leaves\": trial.suggest_int(\"model__num_leaves\", 16, 512),\n",
    "        \"model__max_depth\": trial.suggest_int(\"model__max_depth\", -1, 20),\n",
    "        \"model__min_child_samples\": trial.suggest_int(\"model__min_child_samples\", 5, 300),\n",
    "        \"model__min_sum_hessian_in_leaf\": trial.suggest_float(\"model__min_sum_hessian_in_leaf\", 1e-3, 10.0, log=True),\n",
    "\n",
    "        # learning\n",
    "        \"model__n_estimators\": trial.suggest_int(\"model__n_estimators\", 500, 8000),\n",
    "        \"model__learning_rate\": trial.suggest_float(\"model__learning_rate\", 0.003, 0.2, log=True),\n",
    "\n",
    "        # sampling\n",
    "        \"model__subsample\": trial.suggest_float(\"model__subsample\", 0.5, 1.0),\n",
    "        \"model__subsample_freq\": trial.suggest_int(\"model__subsample_freq\", 1, 10),\n",
    "        \"model__colsample_bytree\": trial.suggest_float(\"model__colsample_bytree\", 0.5, 1.0),\n",
    "\n",
    "        # regularization\n",
    "        \"model__reg_alpha\": trial.suggest_float(\"model__reg_alpha\", 1e-8, 50.0, log=True),\n",
    "        \"model__reg_lambda\": trial.suggest_float(\"model__reg_lambda\", 1e-8, 50.0, log=True),\n",
    "\n",
    "        # split control\n",
    "        \"model__min_split_gain\": trial.suggest_float(\"model__min_split_gain\", 0.0, 1.0),\n",
    "\n",
    "        # binning\n",
    "        \"model__max_bin\": trial.suggest_int(\"model__max_bin\", 63, 511),\n",
    "}\n",
    "\n",
    "    # clone parameters and model pipeline\n",
    "    pipe = clone(lgbm_pipe_q8)\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    # cross-validation\n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        x_train_8,\n",
    "        y_train_8,\n",
    "        cv=cv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return -scores.mean()\n",
    "\n",
    "# start optuna study \n",
    "study_lgbm = start_study_optuna(\n",
    "    objective=objective_lgbm,\n",
    "    n_trials=50,\n",
    "    sampler_seed=n_number,\n",
    "    direction=\"minimize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate the model with best parameters\n",
    "# set the best parameters to the pipeline\n",
    "lgbm_pipe_q8.set_params(**study_lgbm.best_params)\n",
    "\n",
    "cv_results_raw_lgbm_q8 = cross_validate(\n",
    "    lgbm_pipe_q8,\n",
    "    x_train_8,\n",
    "    y_train_8,\n",
    "    cv=cv,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    return_train_score=False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# results\n",
    "rows.append({\n",
    "    \"model\": \"Tuned LightGBM\",\n",
    "    \"R2_mean\": cv_results_raw_lgbm_q8[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-cv_results_raw_lgbm_q8[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "})\n",
    "\n",
    "# show as dataframe \n",
    "compare_models_q8_df = (\n",
    "    pd.DataFrame(rows)\n",
    "    .reset_index(drop = True)\n",
    "    .sort_values(by = \"RMSE_mean\"))\n",
    "\n",
    "# check \n",
    "compare_models_q8_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec3fba",
   "metadata": {},
   "source": [
    "The performance gap between linear models and LightGBM is substantial rather than marginal. Under 5-fold cross-validation, the tuned LightGBM model achieves an RMSE of approximately 0.48 and an R² of about 0.74, compared to RMSE ~ 0.51 and R² ~ 0.70 for the best linear models. This represents a meaningful reduction in prediction error relative to our strongest linear baseline.\n",
    "\n",
    "Unlike the linear models, LightGBM is able to capture nonlinear effects and higher-order interactions among tags that linear regression, Lasso, and ElasticNet are structurally unable to represent. The magnitude of the improvement suggests that a nontrivial portion of the signal in the data is inherently nonlinear, rather than simply a result of noise reduction or regularization.\n",
    "\n",
    "At this stage, cross-validation alone strongly favors LightGBM on predictive grounds. We therefore proceed to evaluate its performance on the held-out test set to confirm that this improvement reflects genuine generalization rather than overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6185716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our baseline model \n",
    "# against lgbm model \n",
    "print(\"LightGBM model performance:\\n\")\n",
    "score_model(\n",
    "    lgbm_pipe_q8,\n",
    "    x_train_8,\n",
    "    x_test_8,\n",
    "    y_train_8,\n",
    "    y_test_8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our LassoCV with top 9 engineered features on test data \n",
    "# against lgbm model \n",
    "print(\"LassoCV top 7 engineered features on test data:\\n\")\n",
    "score_model(\n",
    "    pipeline_q8_features_reduced_index,\n",
    "    x_train_reduced_8_index,\n",
    "    x_test_reduced_8_index,\n",
    "    y_train_8,\n",
    "    y_test_8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84173fe",
   "metadata": {},
   "source": [
    "This gap in performance persists on the held-out test set, where LightGBM achieves test RMSE ~ 0.475 and test R² ~ 0.745, substantially outperforming the best Lasso-based model with engineered features (test RMSE ~ 0.514, test R² ~ 0.705). The consistency between cross-validation and hold-out performance indicates that this gain reflects genuine improvements in generalization rather than overfitting.\n",
    "\n",
    "This result provides an important conclusion. The linear models, even with careful regularization, feature selection, and domain-driven feature engineering, appear to have fully exhausted the linear signal in the data. Further gains require modeling nonlinearities and interactions among tags, which tree-based gradient boosting methods are explicitly designed to capture. The magnitude of the performance jump suggests that student evaluations are influenced by nonlinear trade-offs and conditional effects, such as workload penalties depending on teaching quality, that cannot be represented within a linear framework.\n",
    "\n",
    "While LightGBM delivers the strongest predictive performance, it sacrifices direct coefficient-based interpretability. To understand which tags and engineered features drive its predictions, and how they interact, we therefore turn to SHAP values. This allows us to decompose LightGBM predictions into feature-level contributions, providing a transparent explanation of the nonlinear structure responsible for the performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapley values\n",
    "explainer = shap.TreeExplainer(lgbm_pipe_q8.named_steps[\"model\"])\n",
    "shap_values = explainer.shap_values(x_train_8)   \n",
    "\n",
    "# plot \n",
    "shap.summary_plot(shap_values, x_train_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953362b",
   "metadata": {},
   "source": [
    "The SHAP results show clear convergence in the signal structure. `Teaching quality` is the dominant driver of predicted ratings, with higher values of the `teaching_quality_index` consistently pushing predictions upward and lower values pulling them down. The contrast features behave as intended: `quality_minus_workload` and `quality_minus_structure_negative` have strong positive effects, indicating that when perceived teaching quality outweighs workload intensity or negative course structure, overall ratings increase. \n",
    "\n",
    "Grading strictness remains a robust negative factor, as `tough_grader` exerts a sizable downward impact even after accounting for teaching quality. Individual teaching-related tags such as `amazing_lectures` and `respected` add refinement but contribute less than the aggregated index. Workload-related variables like `lecture_heavy`, `lots_to_read`, `lots_of_homework`, and `test_heavy` reduce predictions, but their effects are smaller and more localized. The slightly negative contribution of accessible is likely due to redundancy with the `teaching_quality_index` rather than a true adverse effect. Overall, the model has learned a stable and intuitive structure: ratings are primarily driven by perceived teaching quality and its tradeoff with workload, while additional features provide only marginal incremental information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22751f4",
   "metadata": {},
   "source": [
    "LightGBM with tuning does not materially improve RMSE beyond ~0.48. SHAP shows the same structure the linear models learned. Teaching quality dominates. Workload and grading strictness subtract. Contrast features encode the main signal. This means the problem is signal-limited, not model-limited.\n",
    "\n",
    "Trying XGBoost now would do three things:\n",
    "- Give almost identical performance to LightGBM\n",
    "- Learn the same interactions we already exposed explicitly\n",
    "- Add training cost and explanation burden without payoff\n",
    "\n",
    "In industry terms, this is diminishing returns. When two different model families - Linear with engineered structure and boosted trees with automatic interactions converge to the same performance and same feature importance, that is a stopping signal.\n",
    "\n",
    "From a balance perspective, the best model is the `7-feature engineered Lasso`, which preserves nearly all predictive power while remaining simple, stable, and fully interpretable. It achieves strong generalization with minimal computational cost and clear coefficient-based explanations, making it the preferred choice when interpretability and robustness are prioritized.\n",
    "\n",
    "From a pure performance perspective, LightGBM remains the strongest model. It achieves the lowest RMSE and captures nonlinear interactions automatically, while SHAP analysis confirms that it relies on the same underlying structure identified by the linear models. Importantly, `LightGBM` retains a reasonable degree of interpretability through feature attributions, making it suitable when performance is the primary objective.\n",
    "\n",
    "In a production setting, `LightGBM` could be further optimized by pruning weak features and tightening tree constraints to reduce computation time and improve efficiency. However, this additional optimization is not necessary here, as the current `LightGBM` model already demonstrates the attainable performance ceiling for this feature space.\n",
    "\n",
    "Overall, the modeling results show clear convergence. The choice between models depends on the objective: the engineered Lasso offers the best balance of simplicity and accuracy, while LightGBM provides marginally better performance at the cost of increased complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81995dd5",
   "metadata": {},
   "source": [
    "## Q9. Build a regression model predicting average difficulty from all tags (the ones in the rmpCapstoneTags.csv) file. Make sure to include the R2 and RMSE of this model. Which of these tags is most strongly predictive of average difficulty? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdae6d",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, make a subset of all variables that are needed. \n",
    "df_9 = df_filtered_final[[\"average_difficulty\"] + df_tags_column_names].copy()\n",
    "\n",
    "# check \n",
    "df_9.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d26142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate y (dependent variable) and x(independent variables)\n",
    "y = df_9[\"average_difficulty\"]\n",
    "x = df_9.drop(columns = \"average_difficulty\")\n",
    "\n",
    "# check \n",
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "x_train_9, x_test_9, y_train_9, y_test_9 = train_test_split(x, y, test_size=0.2, random_state=n_number)\n",
    "\n",
    "# check distributions of the data \n",
    "print(f'Number of rows for train data:{y_train_9.shape[0]}\\n')\n",
    "print(f'Number of rows for test data:{y_test_9.shape[0]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649ccab",
   "metadata": {},
   "source": [
    "Quite good amount of rows both in training and test data, we can proceed with the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations between y and x predictors \n",
    "# based on training data\n",
    "corr_yx_q9= x_train_9.corrwith(y_train_9, \n",
    "                            method = 'pearson').sort_values(ascending= False)\n",
    "# check \n",
    "corr_yx_q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation of y and x \n",
    "plt.figure(figsize= (12, 10))\n",
    "\n",
    "# plot itself \n",
    "sns.barplot(corr_yx_q9, \n",
    "            orient= 'h', \n",
    "            color = 'skyblue', \n",
    "            edgecolor = 'black')\n",
    "\n",
    "# title \n",
    "plt.title(\"Correlation of Predictors with Average Difficulty\", \n",
    "          fontsize = 18, \n",
    "          fontweight = \"bold\")\n",
    "\n",
    "# xlabel\n",
    "plt.xlabel(\"Correlation coefficient\", \n",
    "           fontsize = 16)\n",
    "# ylabel \n",
    "plt.ylabel(\"Predictor\", \n",
    "           fontsize = 16)\n",
    "# ticks \n",
    "plt.tick_params(size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e68e3d",
   "metadata": {},
   "source": [
    "These correlations describe how students associate course characteristics with perceived difficulty.\n",
    "\n",
    "Difficulty is driven primarily by grading strictness and workload.\n",
    "The strongest positive correlation is tough grader (0.66). This dominates all other predictors and clearly anchors students’ notion of difficulty. Additional workload signals reinforce this perception. Don’t skip class or you will not pass (0.29), lots of homework (0.27), lots to read (0.25), test heavy (0.24), and lecture heavy (0.18) all move in the same direction. These tags describe time pressure, assessment intensity, and rigid structure, which students consistently interpret as higher difficulty.\n",
    "\n",
    "Administrative and minor course features matter little.\n",
    "Tags such as accessible (0.01), graded by few things (-0.00), and group projects (-0.02) show near-zero correlation. They do not meaningfully influence perceived difficulty.\n",
    "\n",
    "Teaching quality works in the opposite direction.\n",
    "All strong negative correlations reflect instructor quality and classroom experience. Caring (-0.35), respected (-0.34), hilarious (-0.28), inspirational (-0.27), clear grading (-0.27), good feedback (-0.25), and amazing lectures (-0.21) are associated with lower perceived difficulty. Students interpret clarity, support, and engagement as making courses feel easier, even when content may still be demanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ae2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix among tags\n",
    "corr_xx_q9 = x_train_9.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation between predictors \n",
    "plt.figure(figsize= (12, 10))\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(\n",
    "    corr_xx_q9,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar=True,\n",
    "    annot=True,\n",
    "    fmt=\".1f\"\n",
    ")\n",
    "\n",
    "# title \n",
    "plt.title(\"Correlation Matrix of Tag Proportions\", \n",
    "          fontweight = 'bold', \n",
    "          fontsize = 18)\n",
    "\n",
    "# tickts \n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tick_params(size = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26eb3b",
   "metadata": {},
   "source": [
    "This correlation matrix confirms the absence of multicollinearity among the tag proportions.\n",
    "\n",
    "All pairwise correlations remain well below 0.7. Most values lie between -0.4 and 0.4, with no tightly coupled clusters. Even conceptually related tags, such as `caring`, `respected`, `amazing` lectures, and `inspirational`, show only moderate correlations. Workload-related tags also move together weakly rather than forming a single block.\n",
    "\n",
    "This means the tags capture distinct dimensions of course experience rather than redundant information. As a result, linear models do not suffer from coefficient instability driven by multicollinearity, and regularization is not required for numerical reasons.\n",
    "\n",
    "We therefore fit and compare three linear specifications: ordinary least squares, Ridge, Lasso, and ElasticNet using the same cross-validation setup.\n",
    "\n",
    "The purpose here is not to resolve multicollinearity, since the correlation analysis shows no problematic dependence among predictors, but to verify whether regularization improves stability or predictive performance. Ordinary least squares serves as the baseline. Ridge tests whether mild shrinkage improves generalization when predictors are moderately correlated. Lasso tests whether automatic feature selection yields a more parsimonious model without sacrificing accuracy.\n",
    "\n",
    "Comparing these three models allows us to confirm whether regularization provides any practical benefit in this setting and to justify the final linear specification on empirical grounds rather than assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed CV\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipelines for linear model comparison (no imputation)\n",
    "pipelines_linear_q9 = {\n",
    "    \"LinearRegression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LinearRegression())\n",
    "    ]),\n",
    "\n",
    "    \"RidgeCV\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", RidgeCV(\n",
    "            alphas=np.logspace(-4, 4, 50),\n",
    "            cv=cv\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"LassoCV\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LassoCV(\n",
    "            alphas=100,\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"ElasticNetCV\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", ElasticNetCV(\n",
    "            l1_ratio=[0.1, 0.5, 0.9],\n",
    "            alphas=100,\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# compare models\n",
    "rows = []\n",
    "for name, pipe in pipelines_linear_q9.items():\n",
    "    results = cross_validate(\n",
    "        pipe,\n",
    "        x_train_9,\n",
    "        y_train_9,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"R2_mean\": results[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": (-results[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "    })\n",
    "\n",
    "compare_models_q9_df = (\n",
    "    pd.DataFrame(rows)\n",
    "      .sort_values(\"RMSE_mean\", ascending=True)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results \n",
    "compare_models_q9_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check models on test data\n",
    "for name, pipe in pipelines_linear_q9.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(score_model(\n",
    "        pipe,\n",
    "        x_train_9,\n",
    "        x_test_9,\n",
    "        y_train_9,\n",
    "        y_test_9\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca797d",
   "metadata": {},
   "source": [
    "All four linear models perform almost identically. Cross-validated RMSE differs by less than 0.001, and test-set RMSE and R² are essentially the same across models. These differences are far below any practical significance threshold.\n",
    "\n",
    "Given this, model choice should be driven by structure and interpretability rather than marginal performance.\n",
    "\n",
    "Lasso is preferred for three reasons.\n",
    "\n",
    "First, it delivers comparable predictive accuracy to OLS, Ridge, and Elastic Net. There is no meaningful loss in fit.\n",
    "\n",
    "Second, it enforces sparsity. Coefficients for weak or redundant tags are shrunk exactly to zero, yielding a simpler model that is easier to explain and reason about.\n",
    "\n",
    "Third, it aligns with the goal of identifying the most influential teaching attributes rather than stabilizing all coefficients. Ridge and Elastic Net improve numerical stability, but earlier analysis showed no serious multicollinearity. That benefit is therefore unnecessary here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe89f8d",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed CV \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline for Lasso Regression model \n",
    "pipeline_q9 = Pipeline([\n",
    "    # scaler \n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", LassoCV(\n",
    "        cv=cv,\n",
    "        random_state=n_number,\n",
    "        alphas=100,\n",
    "\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# fit \n",
    "pipeline_q9.fit(x_train_9, \n",
    "                y_train_9)\n",
    "\n",
    "# extract feature weights from the model\n",
    "model_q9 = pipeline_q9.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names_q9 = x_train_9.columns.to_list()\n",
    "# coefficients\n",
    "coef_q9 = model_q9.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_q9 = pd.DataFrame({\n",
    "    \"tag\": feature_names_q9,\n",
    "    \"weight\": coef_q9\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_q9[\"abs_weight\"] = weights_df_q9[\"weight\"].abs()\n",
    "weights_df_q9 = weights_df_q9.sort_values(\"abs_weight\", ascending=False).reset_index(drop = True)\n",
    "\n",
    "# check \n",
    "weights_df_q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weights (coefficients)\n",
    "# figure size \n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# plot itself \n",
    "sns.barplot(\n",
    "    data=weights_df_q9,\n",
    "    x=\"weight\",\n",
    "    y=\"tag\",\n",
    "    color=\"yellow\",\n",
    "    edgecolor=\"black\", \n",
    "    orient= \"h\"\n",
    ")\n",
    "\n",
    "# title \n",
    "plt.title(\"Linear Regression Weights Sorted by Magnitude for Predicting Average Difficulty\", \n",
    "          fontsize = 18, \n",
    "          fontweight = \"bold\")\n",
    "\n",
    "# xlabel \n",
    "plt.xlabel(\"Tag\", fontsize = 16)\n",
    "# ylabel\n",
    "plt.ylabel(\"Standardized regression coefficient\", \n",
    "           fontsize = 16)\n",
    "\n",
    "# ticks\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tick_params(size = 14)\n",
    "\n",
    "# show the plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5f512",
   "metadata": {},
   "source": [
    "We can see that the `average difficulty` of professors is primarily explained by a small subset of the 13 predictors. In particular, `tough_grader` shows by far the largest standardized weight, indicating that it is the strongest contributor to perceived difficulty. A few additional variables (such as `clear_grading`, `hilarious`, `caring`, `extra_credit`, and `test_heavy`) have moderate effects, while most of the remaining predictors have standardized coefficients that are very small or close to zero, suggesting minimal influence on the response variable.\n",
    "\n",
    "Therefore, to make our model more parsimonious and sparse - without sacrificing predictive performance - we remove predictors whose absolute standardized weight is below 0.05 and then iteratively reduce number of features, choosing the most optimal model possible. This feature selection step simplifies the model while retaining the most informative variables for predicting average professor difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "lassocv_q9_comparison = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features_q9 = weights_df_q9[\n",
    "    weights_df_q9[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"tag\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_reduced_9 = x_train_9[keep_features_q9]\n",
    "\n",
    "x_test_reduced_9 = x_test_9[keep_features_q9]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features_q9), 0, -1):\n",
    "    keep_pol_q9 = keep_features_q9[:k]\n",
    "\n",
    "    x_train_reduced_9 = x_train_reduced_9[keep_pol_q9]\n",
    "\n",
    "    # pipeline to test\n",
    "    pipeline = Pipeline([\n",
    "    \n",
    "    # data standardization (scaler)\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", LassoCV(\n",
    "        cv = cv, \n",
    "        alphas = 100, \n",
    "        max_iter= 10000, \n",
    "        random_state = n_number\n",
    "    ))\n",
    "])\n",
    "    \n",
    "    results_reduced_9 = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_9, \n",
    "        y_train_9, \n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    lassocv_q9_comparison.append({\n",
    "    \"model\": f\"LassoCV | top-{k} by |w|\",\n",
    "    \"R2_mean\": results_reduced_9[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-results_reduced_9[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "    \n",
    "# dataframe \n",
    "drop_df_9_comparisons = (\n",
    "            pd.DataFrame(lassocv_q9_comparison)\n",
    "            .sort_values(\"RMSE_mean\", ascending=True)\n",
    "            .reset_index(drop=True))\n",
    "drop_df_9_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best models dataframe\n",
    "compare_models_q9_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f211fa",
   "metadata": {},
   "source": [
    "The progressive pruning results show that meaningful sparsification is very limited in this setting. Performance declines monotonically as predictors are removed, with no flat region beyond the very top of the ranking. The best sparse Lasso specification retains `13 predictors`, achieving a cross-validated RMSE of approximately 0.543, which is close to the linear benchmark RMSE of ~0.541 obtained by Linear Regression, Ridge, ElasticNet, and the full Lasso.\n",
    "\n",
    "Once the model is reduced below 13 predictors, both RMSE and R² degrade steadily. By the time the model is restricted to 10 or fewer variables, the loss in predictive accuracy becomes clearly non-trivial, and aggressive pruning to fewer than 7 predictors leads to substantial deterioration. This indicates that while individual coefficients may be small, many predictors contribute non-negligible signal collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort features by absolute weight\n",
    "weights_df_9_sorted = weights_df_q9.copy()\n",
    "weights_df_9_sorted = weights_df_9_sorted.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# keep top 13 features\n",
    "selected = weights_df_9_sorted.head(13)[\"tag\"].tolist()\n",
    "\n",
    "# reduce data\n",
    "x_train_9_reduced = x_train_9[selected]\n",
    "x_test_9_reduced  = x_test_9[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline_q9_reduced = Pipeline([\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()),\n",
    "\n",
    "    # model \n",
    "    (\"model\", LassoCV(\n",
    "        cv=cv, \n",
    "        random_state=n_number, \n",
    "        alphas = 100, \n",
    "        max_iter=10000))\n",
    "    ])\n",
    "\n",
    "results_reduced_9 = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_9_reduced, \n",
    "        y_train_9, \n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "rows.append({\n",
    "    \"model\": f\"LassoCV | top-{len(x_train_9_reduced.columns)} by |w|\",\n",
    "    \"R2_mean\": results_reduced_9[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-results_reduced_9[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "    \n",
    "# dataframe \n",
    "compare_models_q9_df = (\n",
    "            pd.DataFrame(rows)\n",
    "            .sort_values(\"RMSE_mean\", ascending=True)\n",
    "            .reset_index(drop=True))\n",
    "compare_models_q9_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model on test data \n",
    "print(\"Model performance with all features:\\n\")\n",
    "score_model(pipeline_q9, \n",
    "            x_train_9, \n",
    "            x_test_9, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model on test data \n",
    "print(\"Model performance with 13 features:\\n\")\n",
    "score_model(pipeline_q9_reduced, \n",
    "            x_train_9_reduced, \n",
    "            x_test_9_reduced, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f399b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final model weights \n",
    "# extract feature weights from the model\n",
    "model_q9 = pipeline_q9_reduced.named_steps[\"model\"]\n",
    "\n",
    "# feature names\n",
    "feature_names_q9 = x_train_9_reduced.columns.to_list()\n",
    "# coefficients\n",
    "coef_q9 = model_q9.coef_\n",
    "\n",
    "# build dataframe\n",
    "weights_df_q9 = pd.DataFrame({\n",
    "    \"tag\": feature_names_q9,\n",
    "    \"weight\": coef_q9\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_q9[\"abs_weight\"] = weights_df_q9[\"weight\"].abs()\n",
    "weights_df_q9 = weights_df_q9.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# cleanup\n",
    "weights_df_q9 = weights_df_q9.drop(columns=\"abs_weight\").reset_index(drop=True)\n",
    "# check \n",
    "weights_df_q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a42820",
   "metadata": {},
   "source": [
    "### Extra: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df689b2",
   "metadata": {},
   "source": [
    "#### Index Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08398039",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_9.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768090b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation again \n",
    "corr_yx_q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teaching quality group\n",
    "teaching_quality = [\n",
    "    \"good_feedback\", \"respected\", \"inspirational\", \"hilarious\", \n",
    "    \"amazing_lectures\", \"caring\", \"accessible\", \"clear_grading\", \n",
    "    \"accessible\"]\n",
    "\n",
    "# structure group\n",
    "structure_positive = [\"dont_skip_class_or_you_will_not_pass\", \n",
    "                      \"pop_quizzes\"]\n",
    "\n",
    "structure_negative = [\"group_projects\", \n",
    "                      \"graded_by_few_things\", \n",
    "                      \"participation_matters\", \n",
    "                      \"extra_credit\"]\n",
    "    \n",
    "# workload group\n",
    "workload = [\"so_many_papers\", \n",
    "            \"test_heavy\",\n",
    "            \"lots_to_read\", \n",
    "            \"lots_of_homework\", \n",
    "            \"lecture_heavy\", \n",
    "            \"tough_grader\"]\n",
    "\n",
    "\n",
    "\n",
    "# create new index features \n",
    "# train data \n",
    "x_train_9[\"teaching_quality_index\"] = x_train_9[teaching_quality].mean(axis=1)\n",
    "x_train_9[\"workload_index\"] = x_train_9[workload].mean(axis=1)\n",
    "x_train_9[\"structure_index_positive\"] = x_train_9[structure_positive].mean(axis=1)\n",
    "x_train_9[\"structure_index_negative\"] = x_train_9[structure_negative].mean(axis=1)\n",
    "\n",
    "# test data \n",
    "x_test_9[\"teaching_quality_index\"] = x_test_9[teaching_quality].mean(axis=1)\n",
    "x_test_9[\"workload_index\"] = x_test_9[workload].mean(axis=1)\n",
    "x_test_9[\"structure_index_positive\"] = x_test_9[structure_positive].mean(axis=1)\n",
    "x_test_9[\"structure_index_negative\"] = x_test_9[structure_negative].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations of predictors and target variables \n",
    "cor_9_y_features = (\n",
    "    x_train_9\n",
    "    .corrwith(y_train_9)\n",
    "    .sort_values(key=lambda x: x.abs(), ascending=False)\n",
    ")\n",
    "\n",
    "cor_9_y_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8886f",
   "metadata": {},
   "source": [
    "To capture a larger share of the underlying signal and potentially reduce RMSE, we construct the same set of engineered indices as in Question 8, aggregating related tags into broader dimensions such as teaching quality, workload, and course structure. The correlation results above show that these indices are strongly associated with their component variables, confirming that they successfully compress meaningful variation. However, because the indices and individual tags encode overlapping information, this transformation also introduces the risk of multicollinearity. We therefore proceed by explicitly examining the correlation structure among predictors to assess the extent of multicollinearity before fitting multivariate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a36561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations between predictors \n",
    "corr_9_x = x_train_9.corr()\n",
    "\n",
    "# plot \n",
    "plt.figure(figsize=(12, 10)) # Adjust the figure size as needed\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(corr_9_x, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# aesthetics \n",
    "# title \n",
    "plt.title('Correlation Matrix of Numerical Features', fontweight = \"bold\", fontsize = 18)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80060c5",
   "metadata": {},
   "source": [
    "As expected from the correlation matrix, several predictors exhibit strong dependence, especially between the engineered indices and their underlying tag variables. Teaching quality, workload, and structure indices are all highly correlated with multiple individual features, creating clear multicollinearity in the full specification. In this setting, fitting Lasso directly would be unstable, since it would arbitrarily select one variable from a correlated group and discard the rest. Therefore, the correct first step is to fit ElasticNet, which can handle correlated predictors by shrinking them jointly. Only after identifying a stable set of important variables through ElasticNet does it make sense to perform feature reduction and then refit a Lasso model for sparsity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1976e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline\n",
    "pipeline_q9_features = Pipeline([\n",
    "        # scaler\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # model\n",
    "        (\"model\", ElasticNetCV(\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            n_alphas=100,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# cross_validation\n",
    "cv_q9_raw_features = cross_validate(\n",
    "        pipeline_q9_features,\n",
    "        x_train_9,\n",
    "        y_train_9,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "# append to the results \n",
    "rows.append({\n",
    "    \"model\": \"ElaticNetCV | eng.features\",\n",
    "        \"R2_mean\": cv_q9_raw_features[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": -cv_q9_raw_features[\"test_neg_root_mean_squared_error\"].mean()\n",
    "    })\n",
    "\n",
    "# create and show dataframe \n",
    "compare_models_q9_df = ( \n",
    "pd.DataFrame(rows)\n",
    ".reset_index(drop = True)\n",
    ".sort_values(by = \"RMSE_mean\") )\n",
    "\n",
    "# check the data \n",
    "compare_models_q9_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with top 13 features performance on the test data:\n",
    "print(\"LassoCV with top 13 features:\\n\")\n",
    "score_model(pipeline_q9_reduced,\n",
    "            x_train_9_reduced, \n",
    "            x_test_9_reduced, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c794a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNetCV with engineered features on test data:\n",
    "print(\"ElasticNetCV with engineered variables:\\n\")\n",
    "score_model(pipeline_q9_features,\n",
    "            x_train_9, \n",
    "            x_test_9, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946346a8",
   "metadata": {},
   "source": [
    "Cross-validation shows that the `ElasticNet` model with engineered features achieves the best linear performance, with RMSE ~ 0.541 and R² ~ 0.551. This slightly but consistently outperforms all baseline linear specifications, including plain Linear Regression, Ridge, and Lasso. The improvement is small in absolute terms, but it is systematic across folds.\n",
    "\n",
    "The test-set results confirm the same ordering. The engineered `ElasticNet` model achieves a test RMSE of approximately 0.541 and a test R² of about 0.548, compared to the `13-feature Lasso model`, which reaches a higher test RMSE of roughly 0.544 and a lower test R² of about 0.543. This indicates a genuine generalization gain rather than cross-validation noise.\n",
    "\n",
    "However, this improvement comes at the cost of complexity. The engineered ElasticNet model retains the full feature set, including correlated indices and individual tags. As a result, the model is not sparse and is harder to interpret, even though its predictive performance is the best among linear models.\n",
    "\n",
    "Given this tradeoff, the next step is clear. We now fix the engineered ElasticNet as the performance upper bound for linear models and attempt to reduce the number of predictors. The goal is to remove weak coefficients while preserving most of the performance gains. If the reduced feature set does not exhibit problematic multicollinearity, we can refit a Lasso model to recover sparsity and interpretability with minimal loss in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b467de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = pipeline_q9_features.named_steps[\"model\"]\n",
    "\n",
    "# extract feature names and coefficients\n",
    "feature_names = x_train_9.columns\n",
    "coef = model.coef_\n",
    "\n",
    "\n",
    "# create the dataframe\n",
    "weights_df_9 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort \n",
    "weights_df_9[\"abs_weight\"] = weights_df_9[\"weight\"].abs()\n",
    "\n",
    "weights_df_9 = (\n",
    "    weights_df_9\n",
    "    .sort_values(\"abs_weight\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results\n",
    "weights_df_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ed438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "elastic_q9_comparison_index = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features_q9_index = weights_df_9[\n",
    "    weights_df_9[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"tag\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_9_reduced_index = x_train_9[keep_features_q9_index]\n",
    "\n",
    "x_test_9_reduced_index = x_test_9[keep_features_q9_index]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features_q9_index), 0, -1):\n",
    "    keep_pol_q9_index = keep_features_q9_index[:k]\n",
    "\n",
    "    x_train_9_reduced_index = x_train_9_reduced_index[keep_pol_q9_index]\n",
    "\n",
    "    # pipeline to test\n",
    "    pipeline = Pipeline([\n",
    "    \n",
    "    # data standardization (scaler)\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", ElasticNetCV(\n",
    "        cv = cv, \n",
    "        alphas = 100, \n",
    "        max_iter= 10000, \n",
    "        random_state = n_number\n",
    "    ))\n",
    "])\n",
    "    \n",
    "    results_reduced_9_index = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_9_reduced_index, \n",
    "        y_train_9, \n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    elastic_q9_comparison_index.append({\n",
    "    \"model\": f\"ElasticNetCV | top-{k} by |w|\",\n",
    "    \"R2_mean\": results_reduced_9_index[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-results_reduced_9_index[\"test_neg_root_mean_squared_error\"]).mean()}\n",
    ")\n",
    "    \n",
    "# dataframe \n",
    "drop_df_9_comparisons_index = (\n",
    "            pd.DataFrame(elastic_q9_comparison_index)\n",
    "            .sort_values(\"RMSE_mean\", ascending=True)\n",
    "            .reset_index(drop=True))\n",
    "drop_df_9_comparisons_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best models dataframe\n",
    "compare_models_q9_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b41832",
   "metadata": {},
   "source": [
    "Weselect the 8-feature `ElasticNet specification` as the final reduced linear model. Relative to the full engineered ElasticNet, this model incurs only a very small loss in cross-validated performance, with RMSE increasing from approximately 0.541 to about 0.544, which is well within expected cross-validation noise. At the same time, it substantially improves parsimony by removing a large number of weak predictors whose contribution to predictive accuracy is negligible.\n",
    "\n",
    "Further reduction beyond eight variables is not justified. Once the model is constrained to seven or fewer predictors, RMSE increases more sharply and R² declines meaningfully, indicating that relevant signal is being discarded rather than redundant noise. Thus, eight predictors represent the practical lower bound at which the engineered linear model retains nearly all of the explanatory power of the full specification.\n",
    "\n",
    "In summary, the 8-feature ElasticNet model provides the best balance between predictive performance, interpretability, and stability. It captures the dominant structure in the data while avoiding unnecessary complexity, making it the preferred linear specification under a parsimony-aware modeling objective.\n",
    "\n",
    "\n",
    "Before finalizing the reduced model, we next examine multicollinearity among the selected eight predictors. Feature reduction alone does not guarantee coefficient stability, especially when engineered indices and their component variables may still encode overlapping information.\n",
    "\n",
    "We therefore inspect pairwise correlations within the 8-feature subset. If correlations remain moderate and below commonly used thresholds for concern, the remaining predictors no longer pose a serious multicollinearity risk. In that case, the benefits of ElasticNet diminish, since its L2 component is primarily needed to stabilize estimates under strong dependence.\n",
    "\n",
    "If no severe multicollinearity is present, we can refit the model using LassoCV on these eight variables. This yields a simpler and more efficient estimator, preserves sparsity, and improves interpretability by relying purely on L1 regularization. ElasticNet is only retained if meaningful collinearity persists; otherwise, Lasso becomes the appropriate final linear model for this reduced feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort features by absolute weight\n",
    "weights_df_9_sorted_index = weights_df_9.copy()\n",
    "weights_df_9_sorted_index = weights_df_9_sorted_index.sort_values(\"abs_weight\", ascending=False)\n",
    "\n",
    "# keep top 8 features\n",
    "selected = weights_df_9_sorted_index.head(8)[\"tag\"].tolist()\n",
    "\n",
    "# reduce data\n",
    "x_train_9_reduced_index = x_train_9[selected]\n",
    "x_test_9_reduced_index = x_test_9[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations between predictors \n",
    "corr_9_x_index = x_train_9_reduced_index.corr()\n",
    "\n",
    "# plot \n",
    "plt.figure(figsize=(12, 10)) # Adjust the figure size as needed\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(corr_9_x_index, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# aesthetics \n",
    "# title \n",
    "plt.title('Correlation Matrix of Numerical Features', fontweight = \"bold\", fontsize = 18)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fe47c",
   "metadata": {},
   "source": [
    "The correlation matrix for the reduced eight-feature set shows no problematic multicollinearity. All pairwise correlations remain well below the usual concern threshold of 0.70. The strongest relationship is between `teaching_quality_index` and caring at about 0.56, which is moderate and expected, since caring reflects one aspect of perceived teaching quality. The negative correlation between `teaching_quality_index` and `tough_grader` at roughly -0.62 reflects a meaningful tradeoff rather than redundancy and does not indicate instability. All other correlations are small to moderate, often close to zero, especially among `structure_index_negative`, `clear_grading`, `hilarious`, and `test_heavy`.\n",
    "\n",
    "Given this structure, the predictors do not encode the same information in a way that would distort coefficient estimates. The stabilization benefit of ElasticNet is therefore no longer necessary. We are free to refit this reduced specification using LassoCV, achieving a sparse, interpretable model without sacrificing numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline_q9_features_reduced = Pipeline([\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()),\n",
    "\n",
    "    # model \n",
    "    (\"model\", LassoCV(\n",
    "        cv=cv, \n",
    "        random_state=n_number, \n",
    "        alphas = 100, \n",
    "        max_iter=10000))\n",
    "    ])\n",
    "\n",
    "# cross_validation\n",
    "cv_raw_features_9 = cross_validate(\n",
    "        pipeline_q9_features_reduced,\n",
    "        x_train_9_reduced_index,\n",
    "        y_train_9,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "# append to the results \n",
    "rows.append({\n",
    "        \"model\": f\"Lasso CV | top-{len(x_train_9_reduced_index.columns)} with indexes\",\n",
    "        \"R2_mean\": cv_raw_features_9[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": -cv_raw_features_9[\"test_neg_root_mean_squared_error\"].mean()\n",
    "    })\n",
    "\n",
    "# return results as dataframe \n",
    "compare_models_q9_df = pd.DataFrame(rows).reset_index(drop = True).sort_values(by = \"RMSE_mean\")\n",
    "\n",
    "# check \n",
    "compare_models_q9_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd5145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with interactions on test data \n",
    "print(\"LassoCV with top 8 engineered features:\\n\")\n",
    "score_model(pipeline_q9_features_reduced,\n",
    "            x_train_9_reduced_index, \n",
    "            x_test_9_reduced_index, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7edcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with 13 variables \n",
    "print(\"LassoCV with top 13 variables:\\n\")\n",
    "score_model(pipeline_q9_reduced,\n",
    "            x_train_9_reduced, \n",
    "            x_test_9_reduced, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = pipeline_q9_features_reduced.named_steps[\"model\"]\n",
    "\n",
    "# extract feature names and coefficients\n",
    "feature_names = x_train_9_reduced_index.columns\n",
    "coef = model.coef_\n",
    "\n",
    "\n",
    "# create the dataframe\n",
    "weights_df_9 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort \n",
    "weights_df_9[\"abs_weight\"] = weights_df_9[\"weight\"].abs()\n",
    "\n",
    "weights_df_9 = (\n",
    "    weights_df_9\n",
    "    .sort_values(\"abs_weight\", ascending=False)\n",
    "    .drop(columns=\"abs_weight\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results\n",
    "weights_df_9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966bd07",
   "metadata": {},
   "source": [
    "The dominant driver of higher average difficulty is `tough_grader`. A larger value strongly increases perceived difficulty, which is consistent with students associating strict grading with harder courses. This effect is the largest in the model and clearly dominates all others.\n",
    "\n",
    "`teaching_quality_index` has a negative coefficient, indicating that higher teaching quality is associated with lower perceived difficulty, holding grading strictness constant. This suggests that good instruction, clarity, and support make courses feel easier even when the material itself may be challenging.\n",
    "\n",
    "`structure_index_negative` also has a negative coefficient. This index aggregates features such as group projects, fewer grading components, participation-based evaluation, and extra credit. Higher values therefore reflect more flexible or forgiving course structures, which are associated with lower perceived difficulty.\n",
    "\n",
    "The remaining variables play secondary roles. `accessible` slightly increases perceived difficulty, likely capturing that accessibility often co-occurs with demanding courses that require frequent interaction. `clear_grading` and `hilarious` both reduce difficulty modestly, suggesting that transparency and classroom atmosphere soften how demanding a course feels. `test_heavy` increases difficulty, but its effect is small once grading strictness and structure are accounted for. `caring` has a minor negative effect, reinforcing that instructor support marginally reduces perceived difficulty.\n",
    "\n",
    "Overall, this final model is coherent and interpretable. Perceived difficulty is primarily driven by grading strictness, then moderated downward by teaching quality and course structure. The model achieves this with a small set of features and minimal redundancy, preserving interpretability while retaining most of the predictive signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ce6b2",
   "metadata": {},
   "source": [
    "#### Contrast Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ccaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlations again \n",
    "corr_yx_q9_indexes = ( x_train_9.corrwith(y_train_9).\n",
    "                      sort_values(key = lambda x: x.abs(), ascending = False))\n",
    "corr_yx_q9_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main tradeoff: quality vs workload\n",
    "x_train_9[\"quality_minus_workload\"] = (\n",
    "    x_train_9[\"teaching_quality_index\"] - x_train_9[\"workload_index\"]\n",
    ")\n",
    "x_test_9[\"quality_minus_workload\"] = (\n",
    "    x_test_9[\"teaching_quality_index\"] - x_test_9[\"workload_index\"]\n",
    ")\n",
    "\n",
    "# quality vs structure domain \n",
    "x_train_9[\"quality_minus_structure_positive\"] = (\n",
    "    x_train_9[\"teaching_quality_index\"] - x_train_9[\"structure_index_positive\"]\n",
    ")\n",
    "\n",
    "x_test_9[\"quality_minus_structure_positive\"] = (\n",
    "    x_test_9[\"teaching_quality_index\"] - x_test_9[\"structure_index_positive\"]\n",
    ")\n",
    "\n",
    "\n",
    "# workload vs structure domain\n",
    "x_train_9[\"workload_minus_structure_negative\"] = (\n",
    "    x_train_9[\"workload_index\"] - x_train_9[\"structure_index_negative\"]\n",
    ")\n",
    "x_test_9[\"workload_minus_structure_negative\"] = (\n",
    "    x_test_9[\"workload_index\"] - x_test_9[\"structure_index_negative\"]\n",
    ")\n",
    "\n",
    "# optional: a single net-structure score (strictness-ish vs leniency-ish)\n",
    "x_train_9[\"structure_net\"] = (\n",
    "    x_train_9[\"structure_index_positive\"] - x_train_9[\"structure_index_negative\"]\n",
    ")\n",
    "\n",
    "x_test_9[\"structure_net\"] = (\n",
    "    x_test_9[\"structure_index_positive\"] - x_test_9[\"structure_index_negative\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations of predictors and target variables \n",
    "cor_9_y_features_contrast = (\n",
    "    x_train_9\n",
    "    .corrwith(y_train_9)\n",
    "    .sort_values(key=lambda x: x.abs(), ascending=False)\n",
    ")\n",
    "\n",
    "cor_9_y_features_contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273325c",
   "metadata": {},
   "source": [
    "We engineered index and contrast features to capture relative pressure, not just isolated course characteristics. The correlation results justify this design choice.\n",
    "\n",
    "The strongest correlations are not individual tags but aggregated and contrast features. `tough_grader` and `workload_index` have the highest positive correlations, confirming that perceived difficulty is driven by grading strictness and overall workload intensity. More importantly, contrast features such as `quality_minus_workload` and `workload_minus_structure_negative` appear among the top signals with large magnitudes. This shows that difficulty is determined by imbalance, specifically when workload dominates teaching quality or when workload overwhelms flexible structure. These effects are stronger than many raw tags, which validates encoding tradeoffs explicitly rather than relying on the model to infer them indirectly.\n",
    "\n",
    "Teaching quality variables consistently load in the opposite direction. `teaching_quality_index` and `quality_minus_structure_positive` are strongly negatively correlated, indicating that strong instruction and positive structure offset difficulty even in demanding courses. Individual quality tags such as `caring`, `respected`, `good_feedback`, `amazing_lectures`, `hilarious`, and `inspirational` all reinforce this pattern with moderate negative correlations. This confirms that these tags capture the same underlying dimension, which motivated collapsing them into a single index in the first place.\n",
    "\n",
    "Structural features behave asymmetrically, which explains why we separated them into positive and negative components. Enforcement-based structure indicators like `dont_skip_class_or_you_will_not_pass`, `test_heavy`, and `lecture_heavy` correlate positively with difficulty, while flexibility-oriented features such as `extra_credit`, `participation_matters`, and `group_projects` have weak or negative correlations. The derived structure indices reflect this split and show smaller magnitudes than workload or grading, indicating that structure matters, but mainly through how it interacts with workload rather than as a standalone driver.\n",
    "\n",
    "Overall, the correlation table confirms the core modeling hypothesis same as from Q8. Difficulty is not driven by single tags in isolation. It is driven by relative dominance of workload and grading over teaching quality and flexible structure. The engineered indices and contrast features capture this mechanism directly, reduce redundancy among correlated tags, and concentrate signal into a smaller number of meaningful predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f376ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline\n",
    "pipeline_q9_features_contrast = Pipeline([\n",
    "        # scaler\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # model\n",
    "        (\"model\", ElasticNetCV(\n",
    "            cv=cv,\n",
    "            random_state=n_number,\n",
    "            alphas = 100,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# cross_validation\n",
    "cv_q9_raw_features_contrast = cross_validate(\n",
    "        pipeline_q9_features_contrast,\n",
    "        x_train_9,\n",
    "        y_train_9,\n",
    "        cv=cv,\n",
    "        scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "# append to the results \n",
    "rows.append({\n",
    "    \"model\": \"ElaticNetCV | with contrast indexes\",\n",
    "        \"R2_mean\": cv_q9_raw_features_contrast[\"test_r2\"].mean(),\n",
    "        \"RMSE_mean\": -cv_q9_raw_features_contrast[\"test_neg_root_mean_squared_error\"].mean()\n",
    "    })\n",
    "\n",
    "# create and show dataframe \n",
    "compare_models_q9_df = ( \n",
    "pd.DataFrame(rows)\n",
    ".reset_index(drop = True)\n",
    ".sort_values(by = \"RMSE_mean\") )\n",
    "\n",
    "# check the data \n",
    "compare_models_q9_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check contrast model on test data\n",
    "print(\"ElasticNetCV with contrast index:\\n\")\n",
    "score_model(pipeline_q9_features_contrast, \n",
    "            x_train_9, \n",
    "            x_test_9, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV with interactions on test data \n",
    "print(\"LassoCV with top 8 engineered features:\\n\")\n",
    "score_model(pipeline_q9_features_reduced,\n",
    "            x_train_9_reduced_index, \n",
    "            x_test_9_reduced_index, \n",
    "            y_train_9, \n",
    "            y_test_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fc4e0",
   "metadata": {},
   "source": [
    "Again, the `contrast-index` specification outperforms the earlier model with eight engineered features in both cross-validation and test performance. This indicates that explicitly encoding tradeoffs, such as quality versus workload and workload versus structure, captures the underlying signal more efficiently than modeling these components separately.\n",
    "\n",
    "However, we do not treat this as a final improvement yet. The contrast model still relies on the full predictor set, which inflates complexity and weakens interpretability. The observed gain may partly reflect additional degrees of freedom rather than a genuinely better structural representation. As before, performance alone is not sufficient. The next step is to inspect coefficients and apply reduction to determine whether the contrast features retain their advantage once sparsity is enforced. Only if the improvement survives pruning will we accept the contrast specification as a superior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = pipeline_q9_features_contrast.named_steps[\"model\"]\n",
    "\n",
    "# extract feature names and coefficients\n",
    "feature_names = x_train_9.columns\n",
    "coef = model.coef_\n",
    "\n",
    "\n",
    "# create the dataframe\n",
    "weights_df_9 = pd.DataFrame({\n",
    "    \"tag\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort \n",
    "weights_df_9[\"abs_weight\"] = weights_df_9[\"weight\"].abs()\n",
    "\n",
    "weights_df_9 = (\n",
    "    weights_df_9\n",
    "    .sort_values(\"abs_weight\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check results\n",
    "weights_df_9\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc4e62",
   "metadata": {},
   "source": [
    "The coefficient pattern confirms convergence. After introducing contrast indices and refitting with regularization, the model again concentrates its mass on the same small subset of predictors that was found before. The largest absolute coefficients are assigned to `tough_grader`, `teaching_quality_index`, `structure_index_negative`, `accessible`, `clear_grading`, `hilarious`, `caring`, and `test_heavy`. All engineered contrast features are shrunk to zero, along with most secondary tags. This means the additional feature engineering does not introduce new independent signal once regularization is applied.\n",
    "\n",
    "This convergence is the key stopping criterion. Multiple modeling paths - raw tags, aggregated indices, contrast features, Elastic Net followed by pruning - consistently recover the same core drivers. At this point, further linear feature engineering only reshuffles representations of the same information and does not change what the model learns.\n",
    "\n",
    "We therefore stop. Under the objective of balancing interpretability, stability, and performance, the final model is the sparse linear specification with these eight predictors. If the objective were changed from balance to pure performance maximization, we would instead select to train `LightGBM`, which achieves slightly lower RMSE but at the cost of higher computational complexity and weaker interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99569b13",
   "metadata": {},
   "source": [
    "### Extra: Prediction Accuracy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d019d80",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c898d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# pipeline for LightGBM\n",
    "lgbm_pipe_q9 = Pipeline([\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        metric=\"rmse\",\n",
    "        random_state=n_number,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# optuna objective\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        # tree complexity\n",
    "        \"model__num_leaves\": trial.suggest_int(\"model__num_leaves\", 16, 512),\n",
    "        \"model__max_depth\": trial.suggest_int(\"model__max_depth\", -1, 20),\n",
    "        \"model__min_child_samples\": trial.suggest_int(\"model__min_child_samples\", 5, 300),\n",
    "        \"model__min_sum_hessian_in_leaf\": trial.suggest_float(\"model__min_sum_hessian_in_leaf\", 1e-3, 10.0, log=True),\n",
    "\n",
    "        # learning\n",
    "        \"model__n_estimators\": trial.suggest_int(\"model__n_estimators\", 500, 8000),\n",
    "        \"model__learning_rate\": trial.suggest_float(\"model__learning_rate\", 0.003, 0.2, log=True),\n",
    "\n",
    "        # sampling\n",
    "        \"model__subsample\": trial.suggest_float(\"model__subsample\", 0.5, 1.0),\n",
    "        \"model__subsample_freq\": trial.suggest_int(\"model__subsample_freq\", 1, 10),\n",
    "        \"model__colsample_bytree\": trial.suggest_float(\"model__colsample_bytree\", 0.5, 1.0),\n",
    "\n",
    "        # regularization\n",
    "        \"model__reg_alpha\": trial.suggest_float(\"model__reg_alpha\", 1e-8, 50.0, log=True),\n",
    "        \"model__reg_lambda\": trial.suggest_float(\"model__reg_lambda\", 1e-8, 50.0, log=True),\n",
    "\n",
    "        # split control\n",
    "        \"model__min_split_gain\": trial.suggest_float(\"model__min_split_gain\", 0.0, 1.0),\n",
    "\n",
    "        # binning\n",
    "        \"model__max_bin\": trial.suggest_int(\"model__max_bin\", 63, 511),\n",
    "}\n",
    "    # clone parameters and model pipeline\n",
    "    pipe = clone(lgbm_pipe_q9)\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    # cross-validation\n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        x_train_9,\n",
    "        y_train_9,\n",
    "        cv=cv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return -scores.mean()\n",
    "\n",
    "# start optuna study \n",
    "study_lgbm = start_study_optuna(\n",
    "    objective=objective_lgbm,\n",
    "    n_trials=50,\n",
    "    sampler_seed=n_number,\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate the model with best parameters\n",
    "# set the best parameters to the pipeline\n",
    "lgbm_pipe_q9.set_params(**study_lgbm.best_params)\n",
    "\n",
    "cv_results_raw_lgbm_q9 = cross_validate(\n",
    "    lgbm_pipe_q9,\n",
    "    x_train_9,\n",
    "    y_train_9,\n",
    "    cv=cv,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    return_train_score=False, \n",
    "    return_estimator = True\n",
    ")\n",
    "\n",
    "# results\n",
    "rows.append({\n",
    "    \"model\": \"Tuned LightGBM\",\n",
    "    \"R2_mean\": cv_results_raw_lgbm_q9[\"test_r2\"].mean(),\n",
    "    \"RMSE_mean\": (-cv_results_raw_lgbm_q9[\"test_neg_root_mean_squared_error\"]).mean()\n",
    "})\n",
    "\n",
    "# show as dataframe \n",
    "cv_results_raw_lgbm_q9 = (\n",
    "    pd.DataFrame(rows)\n",
    "    .reset_index(drop = True)\n",
    "    .sort_values(by = \"RMSE_mean\"))\n",
    "\n",
    "# check \n",
    "cv_results_raw_lgbm_q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our baseline model \n",
    "# against lgbm model \n",
    "print(\"LightGBM model performance:\\n\")\n",
    "score_model(\n",
    "    lgbm_pipe_q9,\n",
    "    x_train_9,\n",
    "    x_test_9,\n",
    "    y_train_9,\n",
    "    y_test_9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our LassoCV with top 8 engineered features on test data \n",
    "# against lgbm model \n",
    "print(\"LassoCV top 8 engineered features on test data:\\n\")\n",
    "score_model(\n",
    "    pipeline_q9_features_reduced,\n",
    "    x_train_9_reduced_index,\n",
    "    x_test_9_reduced_index,\n",
    "    y_train_9,\n",
    "    y_test_9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb278be",
   "metadata": {},
   "source": [
    "The results are fully consistent with expectations. Once the objective shifts from parsimony to pure predictive performance, the tuned LightGBM clearly dominates all linear specifications. In cross-validation, `LightGBM` improves RMSE from roughly 0.54 for the best linear and `LassoCV` models down to about 0.526, with a corresponding increase in R². This advantage persists on the test set, where `LightGBM` achieves a testing RMSE of approximately 0.529 and R² of about 0.567, compared to the best sparse Lasso model's RMSE of roughly 0.545 and R² near 0.541.\n",
    "\n",
    "The gap is economically meaningful and well outside cross-validation noise. This confirms that nonlinear tree-based models are able to extract additional signal beyond what linear models capture, even after extensive feature engineering. Under a performance-maximization objective, `LightGBM` is therefore the preferred final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapley values\n",
    "explainer = shap.TreeExplainer(lgbm_pipe_q9.named_steps[\"model\"])\n",
    "shap_values = explainer.shap_values(x_train_9)   \n",
    "\n",
    "# plot \n",
    "shap.summary_plot(shap_values, x_train_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd8bde",
   "metadata": {},
   "source": [
    "This SHAP summary confirms and sharpens everything learned earlier. The model is driven first by grading strictness and workload. High values of `tough_grader` and `workload_index` push predictions strongly toward higher difficulty, with large, asymmetric impacts. The contrast feature `quality_minus_workload` works exactly as intended. When teaching quality outweighs workload, predicted difficulty drops, and when workload dominates, difficulty rises. \n",
    "\n",
    "`Test_heavy` reinforces this pattern, adding positive difficulty pressure independently of general workload. `Teaching_quality_index`, `hilarious`, `accessible`, `clear_grading`, and `extra_credit` all reduce predicted difficulty, but with smaller and more localized effects. These features act as moderators rather than primary drivers. \n",
    "\n",
    "Structural and participation-related variables cluster tightly around zero, indicating negligible marginal contribution once workload and grading severity are known. Importantly, the nonlinear model assigns importance in the same order and direction as the linear and ElasticNet models, but exploits threshold effects and interaction strength more efficiently. This convergence across model families indicates the problem is signal-limited rather than model-limited. At this point, testing `XGBoost` would almost certainly reproduce the same structure and deliver marginal or no improvement while increasing training cost and explanation burden. We are already squeezing the available signal. The `LightGBM` model represents the performance ceiling, and further modeling effort would have diminishing returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575bdfe7",
   "metadata": {},
   "source": [
    "If the goal is interpretability, `the linear model with engineered indices` and a small, stable feature set is the clear choice. It captures the dominant mechanisms, workload and grading severity versus teaching quality, with transparent coefficients and minimal complexity.\n",
    "\n",
    "If the goal is pure predictive performance with no concern for interpretability, `LightGBM` is the top performer. It consistently achieves lower RMSE by exploiting nonlinearities and interaction strength automatically.\n",
    "\n",
    "That said, the improvement is small, roughly a 2 percent gain in RMSE. Given the added training cost, tuning overhead, and explanation burden, this gain is not compelling enough to justify replacing the linear specification. In practical terms, the models converge on the same signal, and the marginal performance advantage of `LightGBM` does not outweigh the loss in simplicity and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee1c65",
   "metadata": {},
   "source": [
    "## Q10. Build a classification model that predicts whether a professor receives a “pepper” from all available factors (both tags and numerical). Make sure to include model quality metrics such as AU(RO)C and also address class imbalance concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, make a subset of all variables that are needed. \n",
    "df_10 = df_filtered_final.copy()\n",
    "\n",
    "# check \n",
    "df_10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop female_gender as it is redundant \n",
    "# drop major, university, and state \n",
    "df_10 = df_10.drop(columns = ['female_gender', \n",
    "                            'major', \n",
    "                            'university', \n",
    "                            'state'])\n",
    "# check \n",
    "df_10.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b486f55",
   "metadata": {},
   "source": [
    "We drop female_gender because gender is already fully captured by the male_gender indicator. Including both would introduce perfect redundancy without adding information.\n",
    "\n",
    "We also remove major, university, and state. These variables are high-cardinality identifiers and do not have a clear behavioral relationship with whether a professor receives a pepper. The pepper reflects student perception of teaching style and engagement, which is already directly captured by the tag variables and numerical ratings. Once these behavioral signals are included, institutional or geographic identifiers are unlikely to add meaningful, generalizable information.\n",
    "\n",
    "From a modeling perspective, keeping major, university, or state would substantially increase dimensionality and the risk of overfitting while primarily capturing dataset-specific patterns rather than transferable structure. Since the goal is to build a behavior-driven classification model that generalizes beyond specific institutions, these identifiers are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to x and y \n",
    "x_10 = df_10.drop(columns = \"received_a_pepper\")\n",
    "y_10 = df_10[\"received_a_pepper\"]\n",
    "\n",
    "# split between train and test data \n",
    "x_train_10, x_test_10, y_train_10, y_test_10 = train_test_split(x_10, y_10, test_size=0.2, random_state=n_number)\n",
    "\n",
    "# correlations between y and x predictors \n",
    "corr_yx_10 = x_train_10.corrwith(y_train_10)\n",
    "\n",
    "# check \n",
    "corr_yx_10.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation of y and x \n",
    "plt.figure(figsize= (12, 10))\n",
    "\n",
    "# plot itself \n",
    "sns.barplot(corr_yx_10, \n",
    "            orient= 'h', \n",
    "            color = 'purple', \n",
    "            edgecolor = 'black')\n",
    "\n",
    "# title \n",
    "plt.title(\"Correlation of Predictors with Received a pepper\", \n",
    "          fontsize = 18, \n",
    "          fontweight = \"bold\")\n",
    "\n",
    "# xlabel\n",
    "plt.xlabel(\"Correlation coefficient\", \n",
    "           fontsize = 16)\n",
    "# ylabel \n",
    "plt.ylabel(\"Predictor\", \n",
    "           fontsize = 16)\n",
    "# ticks \n",
    "plt.tick_params(size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1157330",
   "metadata": {},
   "source": [
    "The correlation analysis provides a clear first look at which factors are associated with receiving a pepper. The strongest positive relationships are driven by overall student sentiment. Average rating shows the highest correlation at about 0.45, followed closely by would take again at about 0.43. This is expected. Peppers are primarily awarded to professors students genuinely like.\n",
    "\n",
    "Teaching quality tags also matter. Inspirational, amazing lectures, respected, good feedback, caring, and hilarious all show moderate positive correlations in the 0.18-0.25 range. These variables capture engagement, clarity, and emotional connection, which aligns well with the informal nature of the pepper signal.\n",
    "\n",
    "Course popularity plays a smaller role. Number of ratings is positively correlated at around 0.11, suggesting that more visible professors are slightly more likely to receive a pepper, but exposure alone is not a dominant factor.\n",
    "\n",
    "On the negative side, difficulty and workload clearly work against receiving a pepper. Tough grader shows the strongest negative correlation at about -0.28. Average difficulty, lecture heavy, lots of homework, lots to read, and test heavy all fall between roughly -0.13 and -0.24. This indicates that stricter and heavier courses reduce the likelihood of a pepper, even if teaching quality is strong.\n",
    "\n",
    "Administrative or structural factors have little impact. Male gender, number of online ratings, participation matters, accessible, and extra credit all sit close to zero, suggesting minimal direct association with receiving a pepper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae611de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix among tags\n",
    "corr_xx = x_train_10.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation between predictors \n",
    "plt.figure(figsize= (12, 10))\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(\n",
    "    corr_xx,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar=True,\n",
    "    annot=True,\n",
    "    fmt=\".1f\"\n",
    ")\n",
    "\n",
    "# title \n",
    "plt.title(\"Correlation Matrix of Independent variables\", \n",
    "          fontweight = 'bold', \n",
    "          fontsize = 18)\n",
    "\n",
    "# tickts \n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tick_params(size = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f253373",
   "metadata": {},
   "source": [
    "The correlation matrix shows clear multicollinearity among several predictors, with absolute correlations reaching and exceeding 0.7.\n",
    "Examples include strong relationships between `average rating`, `would take again`, and multiple teaching quality tags, as well as between difficulty-related variables.\n",
    "\n",
    "This level of correlation violates the independence assumption required for stable coefficient estimation in standard logistic regression and makes coefficient interpretation unreliable.\n",
    "\n",
    "Since we want to implement both feature selection and coefficient interpretability, `Elastic Net` is the appropriate choice at this stage.\n",
    "\n",
    "Elastic Net is preferred because:\n",
    "- L1 regularization promotes sparsity and feature selection\n",
    "- L2 regularization stabilizes coefficient estimates under multicollinearity\n",
    "\n",
    "Using Elastic Net during feature selection allows us to reduce redundancy while maintaining stable, interpretable coefficients, which would not be achievable with pure Lasso under strong multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distributions of the data \n",
    "print(f'Number of rows for train data:{y_train_10.shape[0]}\\n')\n",
    "print(f'Number of rows for test data:{y_test_10.shape[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check classes in train data\n",
    "y_train_10.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56195ab",
   "metadata": {},
   "source": [
    "Both classes are reasonably well represented. The training set contains 8,504 observations without a pepper and 6,233 with a pepper, which corresponds to roughly a 58-42 split. This is not a severe imbalance, so extreme remedies such as aggressive resampling are unnecessary. However, the classes are still not perfectly balanced, so evaluation will rely on threshold-independent metrics such as ROC-AUC rather than accuracy.\n",
    "\n",
    "Given this setup, we proceed without class resampling and instead focus on model specification and preprocessing choices. As a first step, we tune KNN imputation within a regular logistic regression framework to fill null values in `would_take_again` feature. This allows us to assess whether a more sophisticated imputation strategy meaningfully improves predictive performance compared to simpler alternatives such as mean or median imputation. Once the best imputation approach is identified, we will carry it forward to the regularized logistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ca739",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c4325",
   "metadata": {},
   "source": [
    "#### KNN Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation (classification needs stratification)\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=n_number\n",
    ")\n",
    "# pipeline for tuning \n",
    "knn_pipe_q10 = Pipeline([\n",
    "    # imputer \n",
    "    (\"imputer\", KNNImputer()),\n",
    "\n",
    "    # model \n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=10000,\n",
    "        solver=\"lbfgs\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# define objective function for optuna\n",
    "def objective(trial):\n",
    "    # parameters to tune (numeric imputer only)\n",
    "    params = {\n",
    "        \"imputer__n_neighbors\": trial.suggest_int(\"imputer__n_neighbors\", 2, 25),\n",
    "        \"imputer__weights\": trial.suggest_categorical(\"imputer__weights\", [\"uniform\", \"distance\"]),\n",
    "        \"imputer__add_indicator\": False\n",
    "    }\n",
    "\n",
    "    # clone our model and set parameters\n",
    "    pipe = clone(knn_pipe_q10)\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    # cross-validation score with tuned parameters (classification metric)\n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        x_train_10,\n",
    "        y_train_10,\n",
    "        cv=cv,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # maximize ROC-AUC\n",
    "    return scores.mean()\n",
    "\n",
    "# start the study for knn imputer tuning\n",
    "study_knn_q10 = start_study_optuna(\n",
    "    objective=objective,\n",
    "    n_trials=15,\n",
    "    sampler_seed=n_number,\n",
    "    direction=\"maximize\"\n",
    ")\n",
    "\n",
    "# set the best parameters to the pipeline\n",
    "knn_pipe_q10.set_params(**study_knn_q10.best_params)\n",
    "\n",
    "# assess \n",
    "score_classifier(\n",
    "    knn_pipe_q10,\n",
    "    x_train_10,\n",
    "    x_test_10,\n",
    "    y_train_10,\n",
    "    y_test_10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea07e13",
   "metadata": {},
   "source": [
    "The logistic regression with tuned KNN imputation performs well and generalizes cleanly. ROC-AUC is about 0.793 on both training and test data, indicating strong discriminative ability with no signs of overfitting. The near-identical train and test AUC confirms stable generalization. The use of Stratified K-Fold cross-validation ensures that class proportions are preserved across folds, so these results are not driven by sampling artifacts.\n",
    "\n",
    "Overall accuracy is roughly 0.72, which is reasonable given the subjective and noisy nature of pepper assignments. Test precision is about 0.66, meaning roughly two thirds of professors predicted to receive a pepper actually do. Test recall is about 0.69, so the model identifies close to 70 percent of true pepper cases. Specificity is about 0.74, indicating slightly better performance at identifying non-pepper cases. The F1 score of approximately 0.68 reflects a balanced tradeoff between precision and recall.\n",
    "\n",
    "The confusion matrix confirms this balance. Errors are distributed across both classes, and there is no evidence that the model systematically favors one class over the other. Class imbalance is therefore not a limiting factor in this setting, and no reweighting or resampling is required at this stage.\n",
    "\n",
    "With this strong and stable baseline in place, the next step is to compare different imputation strategies. We will evaluate whether simpler imputers, such as mean or median imputation, perform comparably to the tuned KNN imputer, or whether the added complexity of KNN imputation provides a measurable advantage in classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34678d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed cross-validation (classification needs stratification)\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=n_number\n",
    ")\n",
    "\n",
    "# pipelines for comparison\n",
    "pipelines_q10 = {\n",
    "    \"KNNImputer (tuned)\": Pipeline([\n",
    "        # imputer\n",
    "        (\"imputer\", KNNImputer(\n",
    "            n_neighbors=22,\n",
    "            weights=\"uniform\",\n",
    "            add_indicator=False\n",
    "        )),\n",
    "\n",
    "        # scaler\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        # model\n",
    "        (\"model\", LogisticRegression(\n",
    "            max_iter=10000,\n",
    "            solver=\"lbfgs\",\n",
    "            random_state=n_number\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"SimpleImputer (mean)\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(\n",
    "            strategy=\"mean\",\n",
    "            add_indicator=False\n",
    "        )),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(\n",
    "            max_iter=10000,\n",
    "            solver=\"lbfgs\",\n",
    "            random_state=n_number\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"SimpleImputer (median)\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(\n",
    "            strategy=\"median\",\n",
    "            add_indicator=False\n",
    "        )),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(\n",
    "            max_iter=10000,\n",
    "            solver=\"lbfgs\",\n",
    "            random_state=n_number\n",
    "        ))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# compare the pipelines\n",
    "rows = []\n",
    "for name, pipe in pipelines_q10.items():\n",
    "    results = cross_validate(\n",
    "        pipe,\n",
    "        x_train_10,\n",
    "        y_train_10,\n",
    "        cv=cv,\n",
    "        scoring={\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"precision\": \"precision\",\n",
    "            \"recall\": \"recall\",\n",
    "            \"f1\": \"f1\"\n",
    "        },\n",
    "        return_train_score=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"ROC_AUC_mean\": results[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": results[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": results[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": results[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": results[\"test_f1\"].mean()\n",
    "    })\n",
    "\n",
    "# create a dataframe for comparison\n",
    "compare_imputers_df_q10 = (\n",
    "    pd.DataFrame(rows)\n",
    "      .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check the results\n",
    "compare_imputers_df_q10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a79ecc",
   "metadata": {},
   "source": [
    "All three imputers produce nearly identical performance across all metrics.\n",
    "`ROC AUC` differences are below 0.001 and precision, recall, and F1 are effectively the same.\n",
    "\n",
    "Since KNN and median imputation are more computationally expensive and do not provide any measurable performance benefit, mean imputation is the most reasonable choice.\n",
    "\n",
    "Mean imputation is:\n",
    "- Simpler\n",
    "- Faster\n",
    "- Deterministic\n",
    "- Easier to maintain in production\n",
    "\n",
    "Because there is no accuracy or ranking advantage from more complex imputers, using mean imputation improves operational simplicity without sacrificing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d661b",
   "metadata": {},
   "source": [
    "#### Logistic Regression tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=n_number\n",
    ")\n",
    "\n",
    "# pipelines for logistic models\n",
    "pipelines_q10_logistic = Pipeline([\n",
    "    # imputer \n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()),\n",
    "\n",
    "    # model itself\n",
    "    (\"model\", LogisticRegressionCV(\n",
    "        cv=5,\n",
    "        l1_ratios=[0.1, 0.5, 0.9],\n",
    "        Cs = np.logspace(-3, 2, 6),\n",
    "        penalty='elasticnet',  \n",
    "        solver='saga',\n",
    "        scoring='roc_auc',\n",
    "        max_iter=10000,\n",
    "        random_state=n_number,\n",
    "        n_jobs= 1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# cross-validation\n",
    "results_logistic = cross_validate(\n",
    "    pipelines_q10_logistic,\n",
    "    x_train_10,\n",
    "    y_train_10,\n",
    "    cv=cv,\n",
    "    scoring=['roc_auc', \n",
    "             'accuracy', \n",
    "             'precision', \n",
    "             'recall', \n",
    "             'f1'],\n",
    "    n_jobs=-1,  \n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# copy rows with results \n",
    "models_comparison = rows.copy()\n",
    "\n",
    "models_comparison.append({\n",
    "        \"model\": \"Logistic Regression with ElasticNet penalty\",\n",
    "        \"ROC_AUC_mean\": results_logistic[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": results_logistic[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": results_logistic[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": results_logistic[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": results_logistic[\"test_f1\"].mean()\n",
    "    })\n",
    "\n",
    "# save to the dataset \n",
    "compare_models_q10_logistic_df = (\n",
    "    pd.DataFrame(models_comparison)\n",
    "      .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check \n",
    "compare_models_q10_logistic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409934f",
   "metadata": {},
   "source": [
    "After introducing L1 and L2 penalties through Elastic Net, we observe a marginal decrease across all evaluation metrics in relation to SimpleImputer (mean) logistic regression model. This behavior is expected, as regularization deliberately introduces bias in exchange for reduced variance, improved generalization, and better handling of multicollinearity.\n",
    "\n",
    "Despite the slight metric drop, cross-validation results remain stable, indicating that the model is not overfitting and that performance differences are within noise.\n",
    "\n",
    "While the regularized model performs comparably to the unregularized baseline, fitting `Elastic Net` with the full set of predictors is computationally more expensive and adds unnecessary complexity. Since the goal includes interpretability and deployment robustness, retaining all variables is not justified.\n",
    "\n",
    "As a result, after looking at test data sample performance, we proceed by pruning predictors based on standardized Elastic Net coefficients, retaining only features with meaningful contribution to the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ce526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess on test data\n",
    "print(\"Elastic-Net Logistic Regression performance on test data:\")\n",
    "score_classifier(\n",
    "    pipelines_q10_logistic,\n",
    "    x_train_10,\n",
    "    x_test_10,\n",
    "    y_train_10,\n",
    "    y_test_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9f639",
   "metadata": {},
   "source": [
    "Held-out test performance closely matches cross-validation results, indicating strong generalization and model stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights from the fitted model\n",
    "logistic_model = pipelines_q10_logistic.named_steps[\"model\"]\n",
    "\n",
    "# feature names after preprocessing\n",
    "feature_names = pipelines_q10_logistic.named_steps[\"imputer\"].get_feature_names_out(\n",
    "    x_train_10.columns\n",
    ")\n",
    "\n",
    "# coefficients (binary classification -> 1D)\n",
    "coef = logistic_model.coef_.ravel()\n",
    "\n",
    "# build dataframe of weights\n",
    "weights_df_q10_elastic = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_q10_elastic[\"abs_weight\"] = weights_df_q10_elastic[\"weight\"].abs()\n",
    "weights_df_q10_elastic = (\n",
    "    weights_df_q10_elastic\n",
    "      .sort_values(\"abs_weight\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check\n",
    "weights_df_q10_elastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489bed78",
   "metadata": {},
   "source": [
    "The standardized Elastic Net coefficients do not exhibit a clear elbow or sharp cutoff point. Instead, weights decay gradually, indicating that several predictors contribute weakly and may be redundant under multicollinearity.\n",
    "\n",
    "Given the absence of a natural breakpoint, we adopt a pragmatic threshold of |weight| < 0.05 as an initial pruning criterion. Coefficients below this level are small relative to the dominant predictors and likely contribute marginal signal.\n",
    "\n",
    "This threshold is not treated as a hard rule, but as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "elasticnetcv_q10_comparisons = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features = weights_df_q10_elastic[\n",
    "    weights_df_q10_elastic[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"feature\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_reduced_10 = x_train_10[keep_features]\n",
    "x_test_reduced_10 = x_test_10[keep_features]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features), 0, -1):\n",
    "    keep_pool = keep_features[:k]\n",
    "\n",
    "    x_train_reduced_10 = x_train_10[keep_pool]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegressionCV(\n",
    "            cv=5,\n",
    "            l1_ratios=[0.1, 0.5, 0.9],\n",
    "            Cs=np.logspace(-3, 2, 6),\n",
    "            penalty=\"elasticnet\",\n",
    "            solver=\"saga\",\n",
    "            scoring=\"roc_auc\",\n",
    "            max_iter=10000,\n",
    "            random_state=n_number,\n",
    "            n_jobs=1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # save results \n",
    "    results_reduced_10 = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_10,\n",
    "        y_train_10,\n",
    "        cv=cv,\n",
    "        scoring=[\"roc_auc\", \"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    # append results \n",
    "    elasticnetcv_q10_comparisons.append({\n",
    "        \"model\": f\"ElasticNetCV | top-{k} by |w|\",\n",
    "        \"ROC_AUC_mean\": results_reduced_10[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": results_reduced_10[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": results_reduced_10[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": results_reduced_10[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": results_reduced_10[\"test_f1\"].mean(),\n",
    "        \"num_features\": k\n",
    "    })\n",
    "\n",
    "# dataframe \n",
    "drop1_df = (pd.DataFrame(elasticnetcv_q10_comparisons)\n",
    "            .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "            .reset_index(drop=True))\n",
    "drop1_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7733996",
   "metadata": {},
   "source": [
    "As features are progressively removed based on `standardized Elastic Net weights`, model performance degrades smoothly rather than abruptly. This indicates that weaker predictors contribute marginal signal but are partially redundant.\n",
    "\n",
    "The `6-feature model` represents the best trade-off between performance and parsimony:\n",
    "- ROC AUC remains high at ~0.787\n",
    "- F1 score is near its maximum\n",
    "- Recall slightly improves relative to larger models\n",
    "- Accuracy remains stable\n",
    "- Model complexity is reduced by ~70%\n",
    "\n",
    "Below 6 features, performance begins to degrade more noticeably, particularly in `ROC AUC` and stability. Above 6 features, additional predictors provide diminishing returns while increasing model complexity and multicollinearity risk.\n",
    "\n",
    "We therefore select the top 6 features by absolute standardized Elastic Net weight as the final feature set.\n",
    "\n",
    "This choice preserves predictive performance while improving interpretability, robustness, and deployment simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddde20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep features of 6-variable model \n",
    "keep_features = weights_df_q10_elastic.head(6)[\"feature\"].tolist()\n",
    "\n",
    "# reduce data\n",
    "x_train_reduced_10_final = x_train_10[keep_features]\n",
    "x_train_reduced_10_final = x_test_10[keep_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced pipeline\n",
    "pipelines_q10_logistic_reduced = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegressionCV(\n",
    "        cv=5,\n",
    "        l1_ratios=[0.1, 0.5, 0.9],\n",
    "        Cs=np.logspace(-3, 2, 6),\n",
    "        penalty=\"elasticnet\",\n",
    "        solver=\"saga\",\n",
    "        scoring=\"roc_auc\",\n",
    "        max_iter=10000,\n",
    "        random_state=n_number,\n",
    "        n_jobs=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# cross-validation \n",
    "results_reduced = cross_validate(\n",
    "    pipelines_q10_logistic_reduced,\n",
    "    x_train_reduced_10_final,\n",
    "    y_train_10,\n",
    "    cv=cv,\n",
    "    scoring=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# appends results to our dictionary \n",
    "models_comparison.append({\n",
    "    \"model\": f\"ElasticNetCV | top-{len(x_train_reduced_10_final.columns)} features by |w|\",\n",
    "    \"ROC_AUC_mean\": results_reduced[\"test_roc_auc\"].mean(),\n",
    "    \"Accuracy_mean\": results_reduced[\"test_accuracy\"].mean(),\n",
    "    \"Precision_mean\": results_reduced[\"test_precision\"].mean(),\n",
    "    \"Recall_mean\": results_reduced[\"test_recall\"].mean(),\n",
    "    \"F1_mean\": results_reduced[\"test_f1\"].mean()\n",
    "})\n",
    "\n",
    "# check \n",
    "compare_models_q10_logistic_df = (\n",
    "    pd.DataFrame(models_comparison)\n",
    "      .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "compare_models_q10_logistic_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1df22",
   "metadata": {},
   "source": [
    "Reducing the feature set from the full model to the top 6 Elastic Net - selected predictors resulted in only a marginal decrease in ROC AUC (~0.004), while precision, recall, and F1 remained stable, indicating that the pruned model retains most of the predictive signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f74ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess 6 variable model on test data\n",
    "print(\"ElasticNet CV with 6 features:\\n\")\n",
    "score_classifier(\n",
    "    pipelines_q10_logistic_reduced,\n",
    "    x_train_reduced_10_final,\n",
    "    x_test_reduced_10_final,\n",
    "    y_train_10,\n",
    "    y_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess all variables model on test data\n",
    "print(\"ElasticNet CV with all features:\\n\")\n",
    "score_classifier(\n",
    "    pipelines_q10_logistic,\n",
    "    x_train_10,\n",
    "    x_test_10,\n",
    "    y_train_10,\n",
    "    y_test_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d40e8",
   "metadata": {},
   "source": [
    "The 6 - feature `Elastic Net` model shows only a modest reduction in ROC AUC compared to the full model (≈ 0.006 - 0.008), while maintaining nearly identical F1 score and slightly improving recall on the test set.\n",
    "\n",
    "Key observations:\n",
    "- ROC AUC drops from ~0.794 to ~0.786. This is small and expected after aggressive feature pruning.\n",
    "- Recall increases from ~0.68 to ~0.69, aligning better with a recall-oriented objective.\n",
    "- Precision decreases slightly, consistent with the recall gain.\n",
    "- F1 score remains effectively unchanged.\n",
    "- Specificity decreases modestly, reflecting the same trade-off.\n",
    "\n",
    "Overall, predictive behavior remains stable and consistent between train and test, indicating no overfitting and good generalization.\n",
    "The reduced model uses only 6 predictors and achieves significantly faster inference and lower computational overhead compared to the full feature set. This makes the pruned model more suitable for deployment, especially in settings where latency or scalability matters.\n",
    "\n",
    "\n",
    "- Given the minimal performance loss, stable generalization, and substantial gains in simplicity and speed, the 6 - feature Elastic Net model represents the better deployment choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix among final predictors \n",
    "corr_xx_10_reduced = x_train_reduced_10_final.corr()\n",
    "\n",
    "# plot correlation between predictors \n",
    "plt.figure(figsize= (12, 10))\n",
    "\n",
    "# plot itself\n",
    "sns.heatmap(\n",
    "    corr_xx_10_reduced,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar=True,\n",
    "    annot=True,\n",
    "    fmt=\".1f\"\n",
    ")\n",
    "\n",
    "# title \n",
    "plt.title(\"Correlation Matrix of Independent variables\", \n",
    "          fontweight = 'bold', \n",
    "          fontsize = 18)\n",
    "\n",
    "# tickts \n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tick_params(size = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446a2ab",
   "metadata": {},
   "source": [
    "Since we still see high multicollinearity between predictors (>=|0.7|) after reducing the subset, we proceed with ElasticNet logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "# predict \n",
    "y_test_10_pred = pipelines_q10_logistic_reduced.predict(x_test_reduced_10_final)\n",
    "y_test_prob_10 = pipelines_q10_logistic_reduced.predict_proba(x_test_reduced_10_final)[:, 1]\n",
    "\n",
    "cm = confusion_matrix(y_test_10, y_test_10_pred)\n",
    "\n",
    "# figure size \n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# confusion matrix parameters \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "\n",
    "# title \n",
    "plt.title(\"Confusion Matrix\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "# xlabel\n",
    "plt.xlabel(\"Predicted\", fontsize=16)\n",
    "#ylabel\n",
    "plt.ylabel(\"Actual\", fontsize=16)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d77fb",
   "metadata": {},
   "source": [
    "The confusion matrix shows balanced and stable behavior on the held-out test set. The model correctly identifies a large portion of positive cases, with 1,087 true positives, while missing 489 positives, resulting in recall of approximately 0.69. This confirms that the model is operating in a recall-oriented regime.\n",
    "\n",
    "On the negative class, the model correctly classifies 1,535 instances, while producing 574 false positives, yielding a specificity of about 0.73. This level of false positives is expected given the emphasis on recall.\n",
    "\n",
    "Overall, the distribution of errors is consistent with cross-validation results and indicates no pathological behavior or overfitting. These results are obtained using the default classification threshold, without any additional threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure once\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# diagonal baseline once\n",
    "ax.plot([0, 1], [0, 1], linewidth=2)\n",
    "\n",
    "# add models using our function \n",
    "add_roc(ax, y_test_10, y_test_prob_10, \"ElasticNetCV | top-{6} features by |w|\")\n",
    "\n",
    "# aesthetics \n",
    "# title\n",
    "ax.set_title(\"AUROC Curve\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "# xlabel\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=16)\n",
    "# ylabel\n",
    "ax.set_ylabel(\"True Positive Rate\", fontsize=16)\n",
    "\n",
    "# tick_params\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "# legend\n",
    "ax.legend(frameon=True, loc=\"lower right\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e208f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities on train set\n",
    "y_train_prob_10 = pipelines_q10_logistic_reduced.predict_proba(x_train_reduced_10_final)[:, 1]\n",
    "\n",
    "# thresholds\n",
    "thresholds = np.linspace(0.01, 0.99, 50)\n",
    "rows_thresholds = []\n",
    "\n",
    "# loop over many thresholds\n",
    "for t in thresholds:\n",
    "    y_pred = (y_train_prob_10 >= t).astype(int)\n",
    "    rows_thresholds.append({\n",
    "        \"threshold\": t,\n",
    "        \"AUC\": roc_auc_score(y_train_10, y_train_prob_10),\n",
    "        \"recall\": recall_score(y_train_10, y_pred),\n",
    "        \"precision\": precision_score(y_train_10, y_pred),\n",
    "        \"f1\": f1_score(y_train_10, y_pred)\n",
    "    })\n",
    "\n",
    "# check \n",
    "probability_threshold_10_reduced = pd.DataFrame(rows_thresholds)\n",
    "probability_threshold_10_reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9abbba",
   "metadata": {},
   "source": [
    "The table shows the expected precision - recall trade-off as the classification threshold increases. Lower thresholds capture nearly all positives but generate many false positives, while higher thresholds sharply reduce recall in exchange for higher precision.\n",
    "\n",
    "Since we think that the better objective is to identify as many professors who receive a pepper as possible, recall is the primary metric. Precision is secondary, as false positives are less costly than missed positives in this setting.\n",
    "\n",
    "At a threshold of 0.27:\n",
    "- Recall is ~0.90\n",
    "- Precision is ~0.56\n",
    "- F1 score is near its maximum\n",
    "- Performance degradation relative to neighboring thresholds is minimal\n",
    "\n",
    "Beyond this point, recall drops more rapidly without providing a proportional gain in precision or overall model quality. Below this point, recall gains are marginal while precision deteriorates noticeably.\n",
    "\n",
    "We therefore select 0.27 as the operating threshold. This threshold maximizes recall while maintaining reasonable precision and overall balance, and aligns with the goal of identifying as many qualifying professors as possible without introducing excessive noise.\n",
    "\n",
    "This choice reflects an explicit, cost-aware decision rather than arbitrary thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply chosen threshold\n",
    "threshold = 0.27\n",
    "y_test_10_prob_threshold = pipelines_q10_logistic_reduced.predict_proba(x_test_reduced_10_final)[:, 1]\n",
    "y_test_10_pred_threshold = (y_test_10_prob_threshold >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "cm = confusion_matrix(y_test_10, y_test_10_pred_threshold)\n",
    "\n",
    "# figure size \n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# confusion matrix parameters \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "\n",
    "# title \n",
    "plt.title(\"Confusion Matrix\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "# xlabel\n",
    "plt.xlabel(\"Predicted\", fontsize=16)\n",
    "#ylabel\n",
    "plt.ylabel(\"Actual\", fontsize=16)\n",
    "\n",
    "# ticks\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89ad15",
   "metadata": {},
   "source": [
    "At the selected threshold of 0.27, the model strongly prioritizes recall. The number of true positives increases substantially to 1,431, while false negatives drop to 145, confirming that most professors who receive a pepper are correctly identified.\n",
    "\n",
    "This improvement in recall comes at the cost of additional false positives, with 1,087 negative cases incorrectly flagged as positive. As a result, specificity decreases, which is expected given the recall-first objective.\n",
    "\n",
    "Overall, this behavior is consistent with the threshold analysis. The model successfully shifts toward identifying as many positive cases as possible, aligning with the stated goal, without introducing instability or unexpected error patterns.\n",
    "\n",
    "Next step\n",
    "\n",
    "With the operating point fixed, we now examine the six retained predictors to understand which variables drive the model’s decisions and how they contribute to identifying professors who receive a pepper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights from the fitted model\n",
    "logistic_model_final = pipelines_q10_logistic_reduced.named_steps[\"model\"]\n",
    "\n",
    "# feature names after preprocessing\n",
    "feature_names = pipelines_q10_logistic_reduced.named_steps[\"imputer\"].get_feature_names_out(\n",
    "    x_train_reduced_10_final.columns\n",
    ")\n",
    "\n",
    "# coefficients (binary classification -> 1D)\n",
    "coef = logistic_model_final.coef_.ravel()\n",
    "\n",
    "# build dataframe of weights\n",
    "weights_df_q10_elastic_final = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_q10_elastic_final[\"abs_weight\"] = weights_df_q10_elastic_final[\"weight\"].abs()\n",
    "weights_df_q10_elastic_final = (\n",
    "    weights_df_q10_elastic_final\n",
    "      .sort_values(\"abs_weight\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check\n",
    "weights_df_q10_elastic_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a89fa",
   "metadata": {},
   "source": [
    "Final model features and interpretation: \n",
    "\n",
    "After feature selection and threshold tuning, the final model retains six predictors. All coefficients are standardized, so magnitudes are directly comparable.\n",
    "\n",
    "- `average_rating`\n",
    "This is the strongest predictor by a wide margin. Higher average ratings substantially increase the likelihood of receiving a pepper, indicating that overall student satisfaction is the dominant signal.\n",
    "\n",
    "- `number_of_ratings`\n",
    "A positive coefficient suggests that professors with more ratings are more likely to receive a pepper. This likely reflects visibility and confidence in the estimate rather than pure quality.\n",
    "\n",
    "- `inspirational`\n",
    "Professors described as inspirational are more likely to receive a pepper, capturing a qualitative teaching attribute beyond raw difficulty or grading.\n",
    "\n",
    "- `average_difficulty`\n",
    "Higher difficulty surprisingly slightly increases the probability of receiving a pepper. This suggests that challenge, when paired with quality, is not penalized and may even be valued.\n",
    "\n",
    "- `would_take_again`\n",
    "A positive effect indicates that students' willingness to retake a professor strongly aligns with receiving a pepper, reinforcing this variable as a proxy for overall teaching approval.\n",
    "\n",
    "- `lots_of_homework`\n",
    "This is the only negative coefficient. Professors perceived as assigning excessive homework are less likely to receive a pepper, even after controlling for overall rating and difficulty.\n",
    "\n",
    "The final model captures a coherent narrative: peppers are associated with highly rated, inspiring professors who are challenging but not overwhelming, widely reviewed, and perceived as worth retaking. The retained features are interpretable, stable, and align with intuitive expectations.\n",
    "\n",
    "This confirms that aggressive pruning preserved the core signal while removing redundant noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ff51b",
   "metadata": {},
   "source": [
    "### Extra: Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at train_data\n",
    "x_train_10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0294bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at correlations again \n",
    "x_train_10.corrwith(y_train_10).sort_values(ascending= False)\n",
    "# check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0f4bf",
   "metadata": {},
   "source": [
    "#### Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309909b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomials \n",
    "# copy x_train and x_test for safety \n",
    "# since the original dataset is pretty large by number of variables, we need to copy \n",
    "x_train_10_pol = x_train_10.copy()\n",
    "x_test_10_pol = x_test_10.copy()\n",
    "\n",
    "# average rating\n",
    "x_train_10_pol[\"average_rating^2\"] = np.square(x_train_10_pol[\"average_rating\"])\n",
    "x_test_10_pol[\"average_rating^2\"] = np.square(x_test_10_pol[\"average_rating\"])\n",
    "\n",
    "# average difficulty \n",
    "x_train_10_pol[\"average_difficulty^2\"] = np.square(x_train_10_pol[\"average_difficulty\"])\n",
    "x_test_10_pol[\"average_difficulty^2\"] = np.square(x_test_10_pol[\"average_difficulty\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at correlations again \n",
    "x_train_10_pol.corrwith(y_train_10).sort_values(ascending= False)\n",
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de03720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to drop redundant average_rating now \n",
    "# average rating \n",
    "x_train_10_pol.drop(\"average_rating\", \n",
    "                    axis = 1, \n",
    "                    inplace = True)\n",
    "\n",
    "x_test_10_pol.drop(\"average_rating\", \n",
    "                    axis = 1, \n",
    "                    inplace = True)\n",
    "\n",
    "# average difficulty \n",
    "x_train_10_pol.drop(\"average_difficulty\", \n",
    "                    axis = 1, \n",
    "                    inplace = True)\n",
    "\n",
    "x_test_10_pol.drop(\"average_difficulty\", \n",
    "                    axis = 1, \n",
    "                    inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13295783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=n_number\n",
    ")\n",
    "\n",
    "# pipelines for logistic models\n",
    "pipelines_q10_logistic_pol = Pipeline([\n",
    "    # imputer \n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()),\n",
    "\n",
    "    # model itself\n",
    "    (\"model\", LogisticRegressionCV(\n",
    "        cv=5,\n",
    "        l1_ratios=[0.1, 0.5, 0.9],\n",
    "        Cs = np.logspace(-3, 2, 6),\n",
    "        penalty='elasticnet',  \n",
    "        solver='saga',\n",
    "        scoring='roc_auc',\n",
    "        max_iter=10000,\n",
    "        random_state=n_number,\n",
    "        n_jobs= 1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# cross-validation\n",
    "results_features_pol = cross_validate(\n",
    "    pipelines_q10_logistic_pol,\n",
    "    x_train_10_pol,\n",
    "    y_train_10,\n",
    "    cv=cv,\n",
    "    scoring=['roc_auc', \n",
    "             'accuracy', \n",
    "             'precision', \n",
    "             'recall', \n",
    "             'f1'],\n",
    "    n_jobs=-1,  \n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# results \n",
    "models_comparison.append({\n",
    "        \"model\": \"Logistic Regression with ElasticNet penalty (polynomial features)\",\n",
    "        \"ROC_AUC_mean\": results_features_pol[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": results_features_pol[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": results_features_pol[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": results_features_pol[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": results_features_pol[\"test_f1\"].mean()\n",
    "    })\n",
    "\n",
    "# save to the dataset \n",
    "compare_models_q10_features_polynomials = (\n",
    "    pd.DataFrame(models_comparison)\n",
    "      .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check \n",
    "compare_models_q10_features_polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess polynomials on test data\n",
    "print(\"Elastic-Net Logistic Regression with polynomial features performance on test data:\")\n",
    "score_classifier(\n",
    "    pipelines_q10_logistic_pol,\n",
    "    x_train_10_pol,\n",
    "    x_test_10_pol,\n",
    "    y_train_10,\n",
    "    y_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07819c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare first baseline model to polynomial model \n",
    "print(\"Baseline Elastic-Net Logistic Regression performance on test data:\")\n",
    "score_classifier(\n",
    "    pipelines_q10_logistic,\n",
    "    x_train_10,\n",
    "    x_test_10,\n",
    "    y_train_10,\n",
    "    y_test_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dc367",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights from the fitted model\n",
    "logistic_model_pol = pipelines_q10_logistic_pol.named_steps[\"model\"]\n",
    "\n",
    "# feature names after preprocessing\n",
    "feature_names = pipelines_q10_logistic_pol.named_steps[\"imputer\"].get_feature_names_out(\n",
    "    x_train_10_pol.columns\n",
    ")\n",
    "\n",
    "# coefficients (binary classification -> 1D)\n",
    "coef = logistic_model_pol.coef_.ravel()\n",
    "\n",
    "# build dataframe of weights\n",
    "weights_df_q10_elastic_pol = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_q10_elastic_pol[\"abs_weight\"] = weights_df_q10_elastic_pol[\"weight\"].abs()\n",
    "weights_df_q10_elastic_pol = (\n",
    "    weights_df_q10_elastic_pol\n",
    "      .sort_values(\"abs_weight\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check\n",
    "weights_df_q10_elastic_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee45f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "elasticnetcv_q10_comparisons_pol = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features_pol = weights_df_q10_elastic_pol[\n",
    "    weights_df_q10_elastic_pol[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"feature\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_reduced_10_pol = x_train_10_pol[keep_features_pol]\n",
    "x_test_reduced_10_pol = x_test_10_pol[keep_features_pol]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features_pol), 0, -1):\n",
    "    keep_pool_pol = keep_features_pol[:k]\n",
    "\n",
    "    x_train_reduced_10_pol = x_train_10_pol[keep_pool_pol]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegressionCV(\n",
    "            cv=5,\n",
    "            l1_ratios=[0.1, 0.5, 0.9],\n",
    "            Cs=np.logspace(-3, 2, 6),\n",
    "            penalty=\"elasticnet\",\n",
    "            solver=\"saga\",\n",
    "            scoring=\"roc_auc\",\n",
    "            max_iter=10000,\n",
    "            random_state=n_number,\n",
    "            n_jobs=1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "    results_reduced_10_pol = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_10_pol,\n",
    "        y_train_10,\n",
    "        cv=cv,\n",
    "        scoring=[\"roc_auc\", \"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    elasticnetcv_q10_comparisons_pol.append({\n",
    "        \"model\": f\"ElasticNetCV | top-{k} by |w|\",\n",
    "        \"ROC_AUC_mean\": results_reduced_10_pol[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": results_reduced_10_pol[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": results_reduced_10_pol[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": results_reduced_10_pol[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": results_reduced_10_pol[\"test_f1\"].mean(),\n",
    "        \"num_features\": k\n",
    "    })\n",
    "\n",
    "# dataframe \n",
    "drop2_df = (pd.DataFrame(elasticnetcv_q10_comparisons_pol)\n",
    "            .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "            .reset_index(drop=True))\n",
    "drop2_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d64804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe with best models \n",
    "compare_models_q10_logistic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6c451",
   "metadata": {},
   "source": [
    "The ElasticNet logistic regression with polynomial features performs worse than the baseline ElasticNet model across all cross-validated metrics and on the held-out test set. This consistent degradation indicates that polynomial expansion does not add useful signal and instead increases noise and variance. As a result, we do not proceed with further feature selection or trimming on this specification and retain our best ElasticNetCV top-6 features model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00179a2f",
   "metadata": {},
   "source": [
    "#### Index Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad43cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy x_train_10 and x_test_10 again for safety \n",
    "# since we will be doing a lot of feature engineering, we want to keep the original data safe for reference and for any future use\n",
    "x_train_10_indexes = x_train_10.copy()\n",
    "x_test_10_indexes = x_test_10.copy()\n",
    "\n",
    "# check correlation first \n",
    "x_y_index_corr = x_train_10_indexes.corrwith(y_train_10).sort_values(ascending= False)\n",
    "x_y_index_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede37e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes\n",
    "# teaching quality group\n",
    "teaching_quality = [\n",
    "    \"inspirational\", \n",
    "    \"amazing_lectures\",\n",
    "    \"respected\", \n",
    "    \"good_feedback\", \n",
    "    \"caring\",\n",
    "    \"hilarious\", \n",
    "    \"accessible\", \n",
    "    \"tough_grader\"\n",
    "]\n",
    "\n",
    "# structure group \n",
    "structure_positive = [\n",
    "    \"clear_grading\", \n",
    "    \"extra_credit\"\n",
    "]\n",
    "\n",
    "structure_negative = [\n",
    "    \"pop_quizzes\", \n",
    "    \"group_projects\", \n",
    "    \"graded_by_few_things\"\n",
    "]\n",
    "\n",
    "# workload group\n",
    "workload = [\n",
    "    \"participation_matters\", \n",
    "    \"so_many_papers\", \n",
    "    \"test_heavy\", \n",
    "    \"lots_to_read\", \n",
    "    \"dont_skip_class_or_you_will_not_pass\", \n",
    "    \"lots_of_homework\", \n",
    "    \"lecture_heavy\"\n",
    "]\n",
    "\n",
    "# train data \n",
    "x_train_10_indexes[\"teaching_quality_index\"] = x_train_10_indexes[teaching_quality].mean(axis=1)\n",
    "x_train_10_indexes[\"workload_index\"] = x_train_10_indexes[workload].mean(axis=1)\n",
    "x_train_10_indexes[\"structure_index_positive\"] = x_train_10_indexes[structure_positive].mean(axis=1)\n",
    "x_train_10_indexes[\"structure_index_negative\"] = x_train_10_indexes[structure_negative].mean(axis=1)\n",
    "\n",
    "# test data \n",
    "x_test_10_indexes[\"teaching_quality_index\"] = x_test_10_indexes[teaching_quality].mean(axis=1)\n",
    "x_test_10_indexes[\"workload_index\"] = x_test_10_indexes[workload].mean(axis=1)\n",
    "x_test_10_indexes[\"structure_index_positive\"] = x_test_10_indexes[structure_positive].mean(axis=1)\n",
    "x_test_10_indexes[\"structure_index_negative\"] = x_test_10_indexes[structure_negative].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation first \n",
    "x_y_index_corr = x_train_10_indexes.corrwith(y_train_10).sort_values(ascending= False)\n",
    "x_y_index_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=n_number\n",
    ")\n",
    "\n",
    "# pipelines for logistic models\n",
    "pipelines_q10_logistic_index = Pipeline([\n",
    "    # imputer \n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()),\n",
    "\n",
    "    # model itself\n",
    "    (\"model\", LogisticRegressionCV(\n",
    "        cv=5,\n",
    "        l1_ratios=[0.1, 0.5, 0.9],\n",
    "        Cs = np.logspace(-3, 2, 6),\n",
    "        penalty='elasticnet',  \n",
    "        solver='saga',\n",
    "        scoring='roc_auc',\n",
    "        max_iter=10000,\n",
    "        random_state=n_number,\n",
    "        n_jobs= 1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# cross-validation\n",
    "results_index = cross_validate(\n",
    "    pipelines_q10_logistic_index,\n",
    "    x_train_10_indexes,\n",
    "    y_train_10,\n",
    "    cv=cv,\n",
    "    scoring=['roc_auc', \n",
    "             'accuracy', \n",
    "             'precision', \n",
    "             'recall', \n",
    "             'f1'],\n",
    "    n_jobs=-1,  \n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "models_comparison.append({\n",
    "        \"model\": \"Logistic Regression ElasticNet (indexes)\",\n",
    "        \"ROC_AUC_mean\": results_index[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": results_index[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": results_index[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": results_index[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": results_index[\"test_f1\"].mean()\n",
    "    })\n",
    "\n",
    "# save to the dataset \n",
    "compare_models_q10_logistic_df = (\n",
    "    pd.DataFrame(models_comparison)\n",
    "      .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check \n",
    "compare_models_q10_logistic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess model with indexes on test data\n",
    "print(\"ElasticNetCV with indexes:\")\n",
    "score_classifier(\n",
    "    pipelines_q10_logistic_index,\n",
    "    x_train_10_indexes,\n",
    "    x_test_10_indexes,\n",
    "    y_train_10,\n",
    "    y_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed02145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess model with 6 features on test data\n",
    "print(\"ElasticNetCV with 6 features:\")\n",
    "score_classifier(\n",
    "    pipelines_q10_logistic_reduced,\n",
    "    x_train_reduced_10_final,\n",
    "    x_test_reduced_10_final,\n",
    "    y_train_10,\n",
    "    y_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0256c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights from the fitted model\n",
    "logistic_model_indexes = pipelines_q10_logistic_index.named_steps[\"model\"]\n",
    "\n",
    "# feature names after preprocessing\n",
    "feature_names = pipelines_q10_logistic_index.named_steps[\"imputer\"].get_feature_names_out(\n",
    "    x_train_10_indexes.columns\n",
    ")\n",
    "\n",
    "# coefficients (binary classification -> 1D)\n",
    "coef = logistic_model_indexes.coef_.ravel()\n",
    "\n",
    "# build dataframe of weights\n",
    "weights_df_q10_elastic_indexes = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"weight\": coef\n",
    "})\n",
    "\n",
    "# sort by absolute weight\n",
    "weights_df_q10_elastic_indexes[\"abs_weight\"] = weights_df_q10_elastic_indexes[\"weight\"].abs()\n",
    "weights_df_q10_elastic_indexes = (\n",
    "    weights_df_q10_elastic_indexes\n",
    "      .sort_values(\"abs_weight\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# check\n",
    "weights_df_q10_elastic_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=n_number)\n",
    "\n",
    "# define threshold\n",
    "initial_threshold = [0.05]\n",
    "elasticnetcv_q10_comparisons_indexes = []\n",
    "\n",
    "# filter-out variables \n",
    "keep_features_indexes = weights_df_q10_elastic_indexes[\n",
    "    weights_df_q10_elastic_indexes[\"abs_weight\"] >= 0.05\n",
    "].sort_values(\"abs_weight\", ascending=False)[\"feature\"].tolist()\n",
    "\n",
    "# create train and test samples \n",
    "x_train_reduced_10_indexes = x_train_10_indexes[keep_features_indexes]\n",
    "x_test_reduced_10_indexes = x_test_10_indexes[keep_features_indexes]\n",
    "\n",
    "# loop to drop each variable 1 by one \n",
    "for k in range(len(keep_features_indexes), 0, -1):\n",
    "    keep_pool_indexes = keep_features_indexes[:k]\n",
    "\n",
    "    x_train_reduced_10_indexes= x_train_10_indexes[keep_pool_indexes]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegressionCV(\n",
    "            cv=5,\n",
    "            l1_ratios=[0.1, 0.5, 0.9],\n",
    "            Cs=np.logspace(-3, 2, 6),\n",
    "            penalty=\"elasticnet\",\n",
    "            solver=\"saga\",\n",
    "            scoring=\"roc_auc\",\n",
    "            max_iter=10000,\n",
    "            random_state=n_number,\n",
    "            n_jobs=1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "    results_reduced_10_indexes = cross_validate(\n",
    "        pipeline,\n",
    "        x_train_reduced_10_indexes,\n",
    "        y_train_10,\n",
    "        cv=cv,\n",
    "        scoring=[\"roc_auc\", \"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    elasticnetcv_q10_comparisons_indexes.append({\n",
    "        \"model\": f\"ElasticNetCV | top-{k} by |w|\",\n",
    "        \"ROC_AUC_mean\": results_reduced_10_indexes[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": results_reduced_10_indexes[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": results_reduced_10_indexes[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": results_reduced_10_indexes[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": results_reduced_10_indexes[\"test_f1\"].mean(),\n",
    "        \"num_features\": k\n",
    "    })\n",
    "\n",
    "# dataframe \n",
    "drop3_df = (pd.DataFrame(elasticnetcv_q10_comparisons_indexes)\n",
    "            .sort_values(\"ROC_AUC_mean\", ascending=False)\n",
    "            .reset_index(drop=True))\n",
    "drop3_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03131bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe with best models \n",
    "compare_models_q10_logistic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d72c2",
   "metadata": {},
   "source": [
    "This experiment evaluates whether replacing raw features with index-based representations improves model performance. The results show that index-based features do not provide a meaningful advantage over the original feature formulation.\n",
    "\n",
    "Performance across all index-based Elastic Net variants remains nearly identical to the baseline model, with ROC AUC around 0.79 and no consistent improvement in accuracy, precision, recall, or F1. Any observed differences are small and fall within cross-validation noise.\n",
    "\n",
    "Importantly, index-based models introduce additional abstraction without increasing predictive power. They neither improve separation nor enable a simpler model with better performance.\n",
    "\n",
    "Index-based feature representations do not add new signal in this setting. Since they increase complexity without improving performance or interpretability, they are rejected. The original feature formulation with Elastic Net regularization remains the preferred approach.\n",
    "\n",
    "Up to this point, model selection prioritized balance, interpretability, and deployment stability. In the final experiment, we change the objective from balanced performance to pure predictive performance maximization.\n",
    "\n",
    "Under this setting, interpretability and coefficient stability are no longer primary constraints. Instead, the goal is to assess whether more flexible, non-linear models can extract additional signal beyond what linear Elastic Net models can capture.\n",
    "\n",
    "To evaluate this upper bound on performance, we employ LightGBM and XGBoost, which are well-suited for handling non-linear interactions, feature hierarchies, and complex decision boundaries. These models are optimized directly for predictive performance and serve as a benchmark for the maximum achievable performance given the available features.\n",
    "\n",
    "This comparison allows us to determine whether the observed performance plateau is a limitation of linear modeling or a fundamental constraint imposed by the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1688c5",
   "metadata": {},
   "source": [
    "#### Extra: Prediction Accuracy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4833ef",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dd288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv \n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=n_number\n",
    ")\n",
    "\n",
    "# pipelines for logistic models\n",
    "# pipeline for LightGBM\n",
    "lgbm_pipe_q10 = Pipeline([\n",
    "    # scaler\n",
    "    (\"scaler\", StandardScaler()), \n",
    "\n",
    "    # model \n",
    "    (\"model\", LGBMClassifier(\n",
    "        objective = \"binary\",\n",
    "        random_state = n_number, \n",
    "        n_jobs = 1, \n",
    "        verbosity = -1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# objective for tuning \n",
    "def objective_lgbm_q10(trial):\n",
    "    params = {\n",
    "    \"model__n_estimators\": trial.suggest_int(\"model__n_estimators\", 300, 3000),\n",
    "    \"model__learning_rate\": trial.suggest_float(\"model__learning_rate\", 0.005, 0.15, log=True),\n",
    "\n",
    "    \"model__num_leaves\": trial.suggest_int(\"model__num_leaves\", 15, 255),\n",
    "    \"model__max_depth\": trial.suggest_int(\"model__max_depth\", -1, 16),\n",
    "\n",
    "    \"model__min_child_samples\": trial.suggest_int(\"model__min_child_samples\", 5, 200),\n",
    "    \"model__min_child_weight\": trial.suggest_float(\"model__min_child_weight\", 1e-3, 10.0, log=True),\n",
    "    \"model__min_split_gain\": trial.suggest_float(\"model__min_split_gain\", 0.0, 1.0),\n",
    "\n",
    "    \"model__subsample\": trial.suggest_float(\"model__subsample\", 0.6, 1.0),\n",
    "    \"model__colsample_bytree\": trial.suggest_float(\"model__colsample_bytree\", 0.6, 1.0),\n",
    "\n",
    "    \"model__reg_alpha\": trial.suggest_float(\"model__reg_alpha\", 1e-8, 10.0, log=True),\n",
    "    \"model__reg_lambda\": trial.suggest_float(\"model__reg_lambda\", 1e-8, 10.0, log=True),\n",
    "\n",
    "    \"model__max_bin\": trial.suggest_int(\"model__max_bin\", 63, 511),\n",
    "\n",
    "    \"model__scale_pos_weight\": trial.suggest_float(\"model__scale_pos_weight\", 0.5, 5.0),\n",
    "}\n",
    "\n",
    "    # copy the pipeline\n",
    "    pipe = clone(lgbm_pipe_q10)\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    # tune the score\n",
    "    scores_lgbm_q10 = cross_val_score(\n",
    "        pipe,\n",
    "        x_train_10,\n",
    "        y_train_10,\n",
    "        cv=cv,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    return scores_lgbm_q10.mean()\n",
    "\n",
    "# start optuna study \n",
    "study_lgbm_q10 = start_study_optuna(\n",
    "    objective=objective_lgbm_q10,\n",
    "    n_trials=50,\n",
    "    sampler_seed=n_number,\n",
    "    direction=\"maximize\"\n",
    ")\n",
    "\n",
    "# set the best parameters to the pipeline\n",
    "lgbm_pipe_q10.set_params(**study_lgbm_q10.best_params)\n",
    "\n",
    "cv_results_raw_lgbm_q10 = cross_validate(\n",
    "    lgbm_pipe_q10,\n",
    "    x_train_10,\n",
    "    y_train_10,\n",
    "    cv=cv,  # StratifiedKFold\n",
    "    scoring=(\"roc_auc\", \"accuracy\", \"precision\", \"recall\", \"f1\"),\n",
    "    return_train_score=False,\n",
    "    return_estimator=True\n",
    ")\n",
    "\n",
    "\n",
    "# results\n",
    "models_comparison.append({\n",
    "        \"model\": \"LightGBM (tuned)\",\n",
    "        \"ROC_AUC_mean\": cv_results_raw_lgbm_q10[\"test_roc_auc\"].mean(),\n",
    "        \"Accuracy_mean\": cv_results_raw_lgbm_q10[\"test_accuracy\"].mean(),\n",
    "        \"Precision_mean\": cv_results_raw_lgbm_q10[\"test_precision\"].mean(),\n",
    "        \"Recall_mean\": cv_results_raw_lgbm_q10[\"test_recall\"].mean(),\n",
    "        \"F1_mean\": cv_results_raw_lgbm_q10[\"test_f1\"].mean()\n",
    "    })\n",
    "\n",
    "# show as dataframe \n",
    "compare_models_q10_logistic_df = (\n",
    "    pd.DataFrame(models_comparison)\n",
    "    .reset_index(drop = True)\n",
    "    .sort_values(by = \"ROC_AUC_mean\", ascending = False))\n",
    "\n",
    "# check \n",
    "compare_models_q10_logistic_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e650c48",
   "metadata": {},
   "source": [
    "Looking at cross-validation first, the key result is that all models achieve almost identical ROC AUC around 0.79, which means they rank observations equally well and extract essentially the same signal. The main difference is not discrimination but decision behavior. Tuned LightGBM reaches extremely high recall ( ~ 91%) but with much lower precision (~ 55%), indicating an aggressive positive classification strategy that trades many false positives for sensitivity. In contrast, ElasticNet and logistic models operate at a more balanced point, with recall and precision both around 0.66-0.68 without any thresholding yet. Since AUC is the same across models, LightGBM is not learning a better representation of the problem, it is simply choosing a different operating point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess lightgbm on test data\n",
    "print(\"LightGBM (tuned) model performance on test data:\")\n",
    "score_classifier(lgbm_pipe_q10, \n",
    "                 x_train_10, \n",
    "                 x_test_10, \n",
    "                 y_train_10, \n",
    "                y_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess 6 feature model on test data\n",
    "print(\"ElasticNetCV with 6 features model performance on test data:\")\n",
    "score_classifier(pipelines_q10_logistic_reduced, \n",
    "                 x_train_reduced_10_final, \n",
    "                 x_test_reduced_10_final, \n",
    "                 y_train_10, \n",
    "                y_test_10, \n",
    "                threshold = 0.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "# predict \n",
    "y_test_10_pred_lgbm = lgbm_pipe_q10.predict(x_test_10)\n",
    "y_test_prob_10_lgbm = lgbm_pipe_q10.predict_proba(x_test_10)[:, 1]\n",
    "\n",
    "cm_lgbm = confusion_matrix(y_test_10, y_test_10_pred_lgbm)\n",
    "\n",
    "# confusion matrixes or both models \n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# ElasticNet CM\n",
    "ConfusionMatrixDisplay(cm).plot(\n",
    "    ax=axes[0],\n",
    "    cmap=\"Blues\",\n",
    "    values_format=\"d\"\n",
    ")\n",
    "# aesthetics \n",
    "axes[0].set_title(\"ElasticNet (6 features, threshold = 0.27)\", \n",
    "                  fontweight = \"bold\",\n",
    "                  fontsize = 18)\n",
    "axes[0].set_xlabel(\"Predicted\", \n",
    "                   fontsize = 16)\n",
    "axes[0].set_ylabel(\"Actual\", \n",
    "                   fontsize = 16)\n",
    "\n",
    "# LightGBM CM\n",
    "ConfusionMatrixDisplay(cm_lgbm).plot(\n",
    "    ax=axes[1],\n",
    "    cmap=\"Blues\",\n",
    "    values_format=\"d\"\n",
    ")\n",
    "# aesthetics \n",
    "axes[1].set_title(\"LightGBM (default threshold = 0.5)\", \n",
    "                  fontweight = \"bold\", \n",
    "                  fontsize = 18)\n",
    "axes[1].set_xlabel(\"Predicted\", \n",
    "                   fontsize = 16)\n",
    "axes[1].set_ylabel(\"Actual\", \n",
    "                   fontsize = 16)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02136f0c",
   "metadata": {},
   "source": [
    "On test data, both models converge to almost the same operating point. ROC AUC differs by about 0.01, which is within noise and does not indicate a meaningful ranking advantage for LightGBM. Accuracy, precision, recall, and F1 are nearly identical. LightGBM achieves slightly higher recall, but this comes with lower specificity and more false positives, as seen in the confusion matrix. ElasticNet trades a small amount of recall for better specificity, while matching LightGBM on F1 almost exactly. This confirms that both models are extracting the same signal and making similar errors. The boosted model is not discovering new structure, only shifting the decision boundary. In practical terms, this is convergence across model families, which signals that additional complexity is not buying real performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure once\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# diagonal baseline once\n",
    "ax.plot([0, 1], [0, 1], linewidth=2)\n",
    "\n",
    "# predictions on probability \n",
    "y_test_prob_10_lgbm = lgbm_pipe_q10.predict_proba(x_test_10)[:, 1]\n",
    "y_test_prob_10_elastic = pipelines_q10_logistic_reduced.predict_proba(x_test_reduced_10_final)[:, 1]\n",
    "\n",
    "# add models using our function \n",
    "add_roc(ax, y_test_10, y_test_prob_10_lgbm, \"LightGBM (tuned)\")\n",
    "add_roc(ax, y_test_10, y_test_prob_10_elastic, \"ElasticNetCV | top-{6} features by |w|\")\n",
    "# aesthetics \n",
    "# title\n",
    "ax.set_title(\"AUROC Curve\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "# xlabel\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=16)\n",
    "# ylabel\n",
    "ax.set_ylabel(\"True Positive Rate\", fontsize=16)\n",
    "\n",
    "# tick_params\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "# legend\n",
    "ax.legend(frameon=True, loc=\"lower right\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b3294",
   "metadata": {},
   "source": [
    "We evaluated whether switching from a parsimonious, interpretable Elastic Net model to a fully tuned LightGBM model yields a meaningful performance improvement.\n",
    "\n",
    "LightGBM achieves a slightly higher ROC AUC (≈ 0.796 vs ≈ 0.786). However, the improvement is modest and does not translate into a qualitatively different error profile. The ROC curves largely overlap, indicating that both models rank observations similarly across most operating points.\n",
    "\n",
    "From the confusion matrices, LightGBM identifies slightly more true positives and slightly fewer false negatives at its default threshold, but this comes with a comparable increase in false positives. When thresholds are adjusted to match recall levels, the practical difference between the two models further narrows.\n",
    "\n",
    "While LightGBM marginally improves ranking performance, it introduces:\n",
    "- Substantially higher model complexity\n",
    "- Reduced interpretability\n",
    "- Additional tuning and maintenance overhead\n",
    "\n",
    "Given that the performance gain is small and does not change downstream decisions, the added complexity is not justified for this task.\n",
    "\n",
    "The data appears to be signal-limited rather than model-limited. Even a highly flexible, tuned LightGBM model cannot extract substantially more information than the 6-feature Elastic Net model.\n",
    "\n",
    "As a result, we do not proceed to XGBoost. The expected gains would be similarly marginal while further increasing complexity. The 6-feature Elastic Net model remains the preferred final model due to its strong performance, interpretability, and operational simplicity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
